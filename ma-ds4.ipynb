{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 16:44:43,102 - INFO - Pull progress: pulling manifest\n",
      "2025-02-19 16:44:43,563 - INFO - Pull progress: pulling 2609048d349e\n",
      "2025-02-19 16:44:43,794 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:43,855 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:43,914 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:43,976 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,035 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,095 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,154 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,215 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,274 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,334 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,394 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,455 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,515 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,575 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,635 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,695 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:44,755 - INFO - Pull progress: pulling 8c17c2ebb0ea\n",
      "2025-02-19 16:44:45,026 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,086 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,146 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,207 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,266 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,326 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,387 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,455 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,507 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,566 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,628 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,686 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,746 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,806 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,866 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,926 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:45,986 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:46,046 - INFO - Pull progress: pulling 7c23fb36d801\n",
      "2025-02-19 16:44:46,268 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:46,328 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:46,388 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:46,448 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:46,508 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:46,568 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:46,628 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:46,688 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:46,748 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:46,808 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:46,868 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:46,928 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:46,988 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:47,048 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:47,108 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:47,169 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:47,228 - INFO - Pull progress: pulling 2e0493f67d0c\n",
      "2025-02-19 16:44:47,501 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:47,562 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:47,622 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:47,682 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:47,741 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:47,800 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:47,862 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:47,922 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:47,981 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:48,042 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:48,101 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:48,161 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:48,221 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:48,282 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:48,341 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:48,401 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:48,462 - INFO - Pull progress: pulling fa304d675061\n",
      "2025-02-19 16:44:48,729 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:48,790 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:48,849 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:48,909 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:48,969 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,030 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,089 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,151 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,209 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,269 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,329 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,389 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,449 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,509 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,575 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,629 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,689 - INFO - Pull progress: pulling be61bcdf308e\n",
      "2025-02-19 16:44:49,736 - INFO - Pull progress: verifying sha256 digest\n",
      "2025-02-19 16:44:54,945 - INFO - Pull progress: writing manifest\n",
      "2025-02-19 16:44:54,948 - INFO - Pull progress: success\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Core Imports and Base Classes\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TherapeuticResponse:\n",
    "    \"\"\"Enhanced response structure for therapeutic context\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "    error_details: str = \"\"\n",
    "    timeout: bool = False\n",
    "    empathy_score: float = 0.0\n",
    "    safety_checks: List[str] = None\n",
    "    ethical_considerations: List[str] = None\n",
    "    refinement_suggestions: List[str] = None\n",
    "    crisis_flag: bool = False\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Robust Ollama client with configurable timeouts\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = 5\n",
    "        self.request_timeout = 300\n",
    "        self._verify_model()\n",
    "\n",
    "    def _parse_json_safe(self, text: str):\n",
    "        \"\"\"Enhanced JSON parsing with fallback\"\"\"\n",
    "        clean_text = text.strip()\n",
    "        if not clean_text:\n",
    "            return {\"error\": \"Empty response\"}\n",
    "            \n",
    "        try:\n",
    "            return json.loads(clean_text)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                start = clean_text.find('{')\n",
    "                end = clean_text.rfind('}') + 1\n",
    "                return json.loads(clean_text[start:end])\n",
    "            except:\n",
    "                return {\"error\": f\"Invalid JSON format: {clean_text[:200]}...\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def _verify_model(self):\n",
    "        \"\"\"Model verification with status checks\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                resp = requests.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
    "                if resp.status_code == 200:\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    models = [m['name'] for m in data.get('models', [])]\n",
    "                    if any(self.model_name in m for m in models):\n",
    "                        return\n",
    "                    self._pull_model()\n",
    "                    return\n",
    "                logger.warning(f\"Model check failed (status {resp.status_code})\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Model check attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        raise ConnectionError(f\"Couldn't connect to Ollama after {self.max_retries} attempts\")\n",
    "\n",
    "    def _pull_model(self):\n",
    "        \"\"\"Model pulling with progress tracking\"\"\"\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                stream=True,\n",
    "                timeout=600\n",
    "            )\n",
    "            for line in resp.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        status = self._parse_json_safe(line).get('status', '')\n",
    "                        logger.info(f\"Pull progress: {status}\")\n",
    "                    except:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model pull failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Generation with configurable timeout and retries\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(\n",
    "                        requests.post,\n",
    "                        f\"{self.base_url}/api/generate\",\n",
    "                        json={\n",
    "                            \"model\": self.model_name,\n",
    "                            \"prompt\": prompt[:4000],\n",
    "                            \"stream\": False,\n",
    "                            \"options\": {\"temperature\": 0.5}\n",
    "                        },\n",
    "                        timeout=self.request_timeout\n",
    "                    )\n",
    "                    resp = future.result(timeout=self.request_timeout)\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    return data.get(\"response\", \"\"), False\n",
    "            except FutureTimeoutError:\n",
    "                logger.warning(f\"Generation timed out (attempt {attempt+1})\")\n",
    "                return f\"Error: Timeout after {self.request_timeout}s\", True\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(1)\n",
    "        return f\"Error: Failed after {self.max_retries} attempts\", True\n",
    "\n",
    "# Cell 2: Base Agent Framework\n",
    "class BaseAgent:\n",
    "    \"\"\"Timeout-aware base agent\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "        self.retry_count = 3\n",
    "        self.max_wait = 300\n",
    "        \n",
    "    def safe_generate(self, prompt: str) -> TherapeuticResponse:\n",
    "        \"\"\"Generation with time budget tracking\"\"\"\n",
    "        start_time = time.time()\n",
    "        error_state = False\n",
    "        timeout_occurred = False\n",
    "        \n",
    "        if not isinstance(prompt, str) or len(prompt.strip()) == 0:\n",
    "            return TherapeuticResponse(\n",
    "                text=\"Error: Invalid input prompt\",\n",
    "                timestamp=start_time,\n",
    "                error=True,\n",
    "                error_details=\"Empty or non-string prompt\",\n",
    "                processing_time=0.0\n",
    "            )\n",
    "            \n",
    "        for attempt in range(self.retry_count):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(self.client.generate, prompt)\n",
    "                    text, error = future.result(timeout=self.max_wait)\n",
    "                    \n",
    "                    return TherapeuticResponse(\n",
    "                        text=text,\n",
    "                        timestamp=start_time,\n",
    "                        error=error,\n",
    "                        processing_time=time.time() - start_time,\n",
    "                        error_details=text if error else \"\",\n",
    "                        timeout=timeout_occurred\n",
    "                    )\n",
    "            except FutureTimeoutError:\n",
    "                logger.error(f\"Generation timed out after {self.max_wait}s\")\n",
    "                timeout_occurred = True\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                logger.error(f\"Generation error: {e}\")\n",
    "                \n",
    "        return TherapeuticResponse(\n",
    "            text=f\"Final error: {error_msg}\" if 'error_msg' in locals() else \"Unknown error\",\n",
    "            timestamp=start_time,\n",
    "            error=True,\n",
    "            error_details=error_msg if 'error_msg' in locals() else \"\",\n",
    "            processing_time=time.time() - start_time,\n",
    "            timeout=timeout_occurred\n",
    "        )\n",
    "\n",
    "# Cell 3: Specialized Therapy Agents\n",
    "class PatientContextAnalyzer(BaseAgent):\n",
    "    \"\"\"Analyzes patient input and conversation history\"\"\"\n",
    "    def analyze_context(self, patient_input: str, history: List[str]) -> Tuple[Dict, float]:\n",
    "        safe_input = patient_input[:2000] if patient_input else \"Patient remains silent\"\n",
    "        history_context = \"\\n\".join(history[-3:])[:3000] if history else \"No history available\"\n",
    "        \n",
    "        prompt = f\"\"\"Analyze therapeutic context:\n",
    "        Patient Statement: {safe_input}\n",
    "        Conversation History: {history_context}\n",
    "        Identify:\n",
    "        - Primary emotions expressed\n",
    "        - Potential underlying issues\n",
    "        - Immediate needs\n",
    "        - Therapeutic approach suggestions\n",
    "        - Crisis indicators\n",
    "        \n",
    "        Output JSON with:\n",
    "        - emotions: List[str]\n",
    "        - key_themes: List[str]\n",
    "        - suggested_approaches: List[str]\n",
    "        - crisis_warnings: List[str]\n",
    "        - context_summary: str\"\"\"\n",
    "        \n",
    "        response = self.safe_generate(prompt)\n",
    "        return self.client._parse_json_safe(response.text), response.processing_time\n",
    "\n",
    "class TherapeuticResponseGenerator(BaseAgent):\n",
    "    \"\"\"Generates and evaluates therapist responses\"\"\"\n",
    "    def generate_response(self, context: Dict, history: List[str]) -> TherapeuticResponse:\n",
    "        prompt = f\"\"\"Generate therapeutic response:\n",
    "        Context: {json.dumps(context)[:3000]}\n",
    "        History: {\" | \".join(history[-3:])[:2000] if history else \"First session\"}\n",
    "        \n",
    "        Guidelines:\n",
    "        - Show empathy and validation\n",
    "        - Use open-ended questions\n",
    "        - Avoid medical advice\n",
    "        - Maintain professional boundaries\n",
    "        - Focus on patient's feelings\n",
    "        - Use CBT techniques when appropriate\n",
    "        \n",
    "        Response:\"\"\"\n",
    "        \n",
    "        response = self.safe_generate(prompt)\n",
    "        return self._enhance_response(response)\n",
    "\n",
    "    def _enhance_response(self, raw_response: TherapeuticResponse) -> TherapeuticResponse:\n",
    "        \"\"\"Add therapeutic metrics to response\"\"\"\n",
    "        empathy_score = min(1.0, raw_response.text.count(\"understand\") * 0.1)\n",
    "        return TherapeuticResponse(\n",
    "            **vars(raw_response),\n",
    "            empathy_score=empathy_score,\n",
    "            safety_checks=[\"Basic validation passed\"],\n",
    "            refinement_suggestions=[]\n",
    "        )\n",
    "\n",
    "class ClinicalSafetyChecker(BaseAgent):\n",
    "    \"\"\"Ensures responses meet safety and ethical standards\"\"\"\n",
    "    def evaluate_response(self, response: str, context: Dict) -> Dict:\n",
    "        prompt = f\"\"\"Evaluate therapeutic response:\n",
    "        Response: {response[:3000]}\n",
    "        Context: {json.dumps(context)[:2000]}\n",
    "        \n",
    "        Check for:\n",
    "        - Boundary violations\n",
    "        - Medical advice\n",
    "        - Crisis handling\n",
    "        - Cultural sensitivity\n",
    "        - Ethical compliance\n",
    "        \n",
    "        Output JSON with:\n",
    "        - safety_score: float (0-1)\n",
    "        - concerns: List[str]\n",
    "        - improvement_suggestions: List[str]\n",
    "        - crisis_handling: str\n",
    "        - ethical_violations: List[str]\"\"\"\n",
    "        \n",
    "        response = self.safe_generate(prompt)\n",
    "        return self.client._parse_json_safe(response.text)\n",
    "\n",
    "# Cell 4: Therapy Response System\n",
    "class TherapeuticResponseSystem:\n",
    "    \"\"\"End-to-end therapeutic response system\"\"\"\n",
    "    def __init__(self):\n",
    "        self.client = OllamaClient(model_name=\"llama2:13b\")\n",
    "        self.agents = {\n",
    "            'context': PatientContextAnalyzer(self.client),\n",
    "            'generator': TherapeuticResponseGenerator(self.client),\n",
    "            'safety': ClinicalSafetyChecker(self.client)\n",
    "        }\n",
    "        self.conversation_history = []\n",
    "        self.metrics = defaultdict(lambda: {'count': 0, 'errors': 0, 'timeouts': 0})\n",
    "        \n",
    "    def process_session(self, patient_input: str) -> Dict:\n",
    "        \"\"\"Main processing pipeline with safety checks\"\"\"\n",
    "        result = {\n",
    "            'response': '',\n",
    "            'context_analysis': {},\n",
    "            'safety_check': {},\n",
    "            'errors': [],\n",
    "            'warnings': [],\n",
    "            'timings': {},\n",
    "            'crisis_alert': False\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            processed_input = str(patient_input)[:5000] if patient_input else \"\"\n",
    "            \n",
    "            # Context analysis\n",
    "            ctx_start = time.time()\n",
    "            context, ctx_time = self.agents['context'].analyze_context(processed_input, self.conversation_history)\n",
    "            result['context_analysis'] = context\n",
    "            result['timings']['context'] = ctx_time\n",
    "            \n",
    "            # Crisis handling\n",
    "            crisis_warnings = context.get('crisis_warnings', [])\n",
    "            if crisis_warnings:\n",
    "                result['crisis_alert'] = True\n",
    "                result['response'] = self._handle_crisis_situation(crisis_warnings)\n",
    "                return self._compile_result(result)\n",
    "                \n",
    "            # Generate response\n",
    "            gen_start = time.time()\n",
    "            response = self.agents['generator'].generate_response(context, self.conversation_history)\n",
    "            result['timings']['generation'] = time.time() - gen_start\n",
    "            \n",
    "            if response.error:\n",
    "                result['errors'].append(\"Response generation failed\")\n",
    "            else:\n",
    "                result['response'] = response.text\n",
    "                \n",
    "                # Safety check\n",
    "                safety_start = time.time()\n",
    "                safety_check = self.agents['safety'].evaluate_response(response.text, context)\n",
    "                result['safety_check'] = safety_check\n",
    "                result['timings']['safety'] = time.time() - safety_start\n",
    "                \n",
    "                # Refinement\n",
    "                if safety_check.get('safety_score', 0) < 0.7:\n",
    "                    refined = self._refine_response(response.text, safety_check)\n",
    "                    result['response'] = refined\n",
    "                    result['timings']['refinement'] = time.time() - safety_start\n",
    "                \n",
    "            # Update history\n",
    "            self._update_history(processed_input, result['response'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            result['errors'].append(f\"Processing failed: {str(e)}\")\n",
    "        \n",
    "        return self._compile_result(result)\n",
    "        \n",
    "    def _handle_crisis_situation(self, crisis_warnings: List[str]) -> str:\n",
    "        \"\"\"Standard crisis response protocol\"\"\"\n",
    "        crisis_template = \"\"\"I hear that you're experiencing {issues}. That sounds incredibly difficult. \\\n",
    "        Please know you're not alone. I strongly recommend contacting {resource} immediately at {phone}. \\\n",
    "        Would you like me to help you connect with support?\"\"\"\n",
    "        \n",
    "        resources = {\n",
    "            'suicidal': ('the National Suicide Prevention Lifeline', '988'),\n",
    "            'abuse': ('the National Domestic Violence Hotline', '1-800-799-7233'),\n",
    "            'general': ('Crisis Text Line', 'Text HOME to 741741')\n",
    "        }\n",
    "        \n",
    "        crisis_type = next((ct for ct in resources if ct in str(crisis_warnings)), 'general')\n",
    "        resource, phone = resources[crisis_type]\n",
    "        \n",
    "        return crisis_template.format(\n",
    "            issues=\", \".join(crisis_warnings),\n",
    "            resource=resource,\n",
    "            phone=phone\n",
    "        )\n",
    "        \n",
    "    def _refine_response(self, response: str, safety_check: Dict) -> str:\n",
    "        \"\"\"Improve response based on safety check\"\"\"\n",
    "        prompt = f\"\"\"Refine therapist response:\n",
    "        Original Response: {response[:2000]}\n",
    "        Safety Concerns: {json.dumps(safety_check['concerns'])[:1000]}\n",
    "        \n",
    "        Create improved response that:\n",
    "        - Maintains therapeutic boundaries\n",
    "        - Removes any medical advice\n",
    "        - Enhances empathy\n",
    "        - Uses open-ended questions\n",
    "        \n",
    "        Revised Response:\"\"\"\n",
    "        \n",
    "        refined = self.agents['generator'].safe_generate(prompt)\n",
    "        return refined.text if not refined.error else response\n",
    "        \n",
    "    def _update_history(self, patient_input: str, therapist_response: str):\n",
    "        \"\"\"Maintain conversation history\"\"\"\n",
    "        self.conversation_history.extend([\n",
    "            f\"Patient: {patient_input[:500]}\",\n",
    "            f\"Therapist: {therapist_response[:500]}\"\n",
    "        ])\n",
    "        self.conversation_history = self.conversation_history[-12:]\n",
    "        \n",
    "    def _compile_result(self, raw_data: dict) -> dict:\n",
    "        \"\"\"Structure final output\"\"\"\n",
    "        return {\n",
    "            'response': raw_data.get('response', ''),\n",
    "            'context_analysis': raw_data.get('context_analysis', {}),\n",
    "            'safety_check': raw_data.get('safety_check', {}),\n",
    "            'crisis_alert': raw_data.get('crisis_alert', False),\n",
    "            'errors': raw_data.get('errors', []),\n",
    "            'warnings': raw_data.get('warnings', []),\n",
    "            'timings': raw_data.get('timings', {}),\n",
    "            'conversation_history': self.conversation_history[-4:]\n",
    "        }\n",
    "\n",
    "# Cell 5: Test Scenarios\n",
    "def test_empty_input():\n",
    "    \"\"\"Test handling of empty patient input\"\"\"\n",
    "    system = TherapeuticResponseSystem()\n",
    "    result = system.process_session(\"\")\n",
    "    print(\"\\nTest 1 - Empty Input:\")\n",
    "    print(f\"Response: {result['response'][:150]}\")\n",
    "    print(f\"Errors: {result['errors']}\")\n",
    "    print(f\"Crisis Alert: {result['crisis_alert']}\")\n",
    "\n",
    "def test_crisis_detection():\n",
    "    \"\"\"Test suicide risk detection\"\"\"\n",
    "    system = TherapeuticResponseSystem()\n",
    "    result = system.process_session(\"I can't take it anymore, I have a bottle of pills ready\")\n",
    "    print(\"\\nTest 2 - Crisis Detection:\")\n",
    "    print(f\"Response: {result['response'][:250]}\")\n",
    "    print(f\"Crisis Alert: {result['crisis_alert']}\")\n",
    "    print(f\"Safety Concerns: {result['safety_check'].get('concerns', [])[:2]}\")\n",
    "\n",
    "def test_invalid_input_type():\n",
    "    \"\"\"Test non-string input handling\"\"\"\n",
    "    system = TherapeuticResponseSystem()\n",
    "    result = system.process_session(12345)\n",
    "    print(\"\\nTest 3 - Invalid Input Type:\")\n",
    "    print(f\"Response: {result['response'][:150]}\")\n",
    "    print(f\"Errors: {result['errors']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_empty_input()\n",
    "    test_crisis_detection()\n",
    "    test_invalid_input_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 16:42:56,931 - INFO - Model hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. You can now run your main script.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "# from your_module import OllamaClient  # Replace your_module\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_and_verify_model(model_name: str, base_url: str):\n",
    "    try:\n",
    "        client = OllamaClient(model_name=model_name, base_url=base_url)  # This will load/verify\n",
    "        logger.info(f\"Model {model_name} loaded successfully.\")\n",
    "        return True # Indicate success\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\")\n",
    "        return False # Indicate failure\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_name = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\"\n",
    "    base_url = \"http://localhost:11434\"\n",
    "    success = load_and_verify_model(model_name, base_url)\n",
    "\n",
    "    if success:\n",
    "        print(\"Model loaded. You can now run your main script.\")\n",
    "    else:\n",
    "        print(\"Model loading failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
