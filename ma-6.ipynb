{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TherapeuticResponse(text=\"The old man sat on his porch, rocking in the chair as he watched the world go by. The sun shone brightly overhead, illuminating the grass and trees below. He sighed deeply, remembering happier times when life was simpler and easier to understand.\\n\\nLife had not always been kind to him. When he was younger, growing up in a small town, everyone knew each other's business. Rumors spread like wildfire and gossip followed closely behind. The old man learned early on that it was better not to get too close to anyone or become too attached. It only led to pain in the end.\\n\\nNow, as an older man with more life experience under his belt, he understood that humans were complex creatures and had their own demons to battle. Some people struggled with addiction, others dealt with loss and grief, while still others faced loneliness and isolation. He knew firsthand how difficult it could be to navigate these challenges alone.\\n\\nThe old man's mind wandered back to a time when his life was full of purpose and meaning. He had been a father figure to many in the community, offering guidance and support where he could. But as the years passed and his own children grew up and moved away, that sense of purpose began to fade.\\n\\nNow, with nothing but time on his hands and an endless stream of memories to reflect on, he found himself questioning what it meant to live a meaningful life. Was there still a reason for him to keep going? Or had the world left him behind, leaving only regret in its wake?\\n\\nHe rocked back and forth in his chair, lost in thought as the sun continued its slow descent across the sky. In that moment, he realized that perhaps the greatest purpose of all was simply to live - to experience life fully and embrace whatever came with it. It wasn't about being perfect or having everything figured out. It was about finding joy in the little things and appreciating the beauty of the world around him.\\n\\nWith a smile on his face, he closed his eyes and let himself drift off into a peaceful slumber, content in the knowledge that even in his twilight years, there was still much to be grateful for. The old man had lived a long and full life, one filled with love, loss, and everything in between. And as he drifted off to sleep, he knew that no matter what tomorrow might bring, he would face it head-on with the same strength and resilience that had carried him through so many difficult times before.\", timestamp=1740155874.840717, error=False, processing_time=25.25063180923462, error_details='', timeout=False, empathy_score=0.0, safety_checks=None, ethical_considerations=None, refinement_suggestions=None, crisis_flag=False)\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Setup and Configuration\n",
    "# Cell 1: Core Imports and Response Structure\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TherapeuticResponse:\n",
    "    \"\"\"Enhanced response structure for therapeutic context\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "    error_details: str = \"\"\n",
    "    timeout: bool = False\n",
    "    empathy_score: float = 0.0\n",
    "    safety_checks: List[str] = None\n",
    "    ethical_considerations: List[str] = None\n",
    "    refinement_suggestions: List[str] = None\n",
    "    crisis_flag: bool = False\n",
    "\n",
    "# Cell 2: Ollama Client Implementation\n",
    "class OllamaClient:\n",
    "    \"\"\"Robust Ollama client with configurable timeouts\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = 5\n",
    "        self.request_timeout = 300\n",
    "        self._verify_model()\n",
    "\n",
    "    def _parse_json_safe(self, text: str):\n",
    "        \"\"\"Enhanced JSON parsing with fallback\"\"\"\n",
    "        clean_text = text.strip()\n",
    "        if not clean_text:\n",
    "            return {\"error\": \"Empty response\"}\n",
    "            \n",
    "        try:\n",
    "            return json.loads(clean_text)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                start = clean_text.find('{')\n",
    "                end = clean_text.rfind('}') + 1\n",
    "                return json.loads(clean_text[start:end])\n",
    "            except:\n",
    "                return {\"error\": f\"Invalid JSON format: {clean_text[:200]}...\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def _verify_model(self):\n",
    "        \"\"\"Model verification with status checks\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                resp = requests.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
    "                if resp.status_code == 200:\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    models = [m['name'] for m in data.get('models', [])]\n",
    "                    if any(self.model_name in m for m in models):\n",
    "                        return\n",
    "                    self._pull_model()\n",
    "                    return\n",
    "                logger.warning(f\"Model check failed (status {resp.status_code})\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Model check attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        raise ConnectionError(f\"Couldn't connect to Ollama after {self.max_retries} attempts\")\n",
    "\n",
    "    def _pull_model(self):\n",
    "        \"\"\"Model pulling with progress tracking\"\"\"\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                stream=True,\n",
    "                timeout=600\n",
    "            )\n",
    "            for line in resp.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        status = self._parse_json_safe(line).get('status', '')\n",
    "                        logger.info(f\"Pull progress: {status}\")\n",
    "                    except:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model pull failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Generation with configurable timeout and retries\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(\n",
    "                        requests.post,\n",
    "                        f\"{self.base_url}/api/generate\",\n",
    "                        json={\n",
    "                            \"model\": self.model_name,\n",
    "                            \"prompt\": prompt[:4000],\n",
    "                            \"stream\": False,\n",
    "                            \"options\": {\"temperature\": 0.5}\n",
    "                        },\n",
    "                        timeout=self.request_timeout\n",
    "                    )\n",
    "                    resp = future.result(timeout=self.request_timeout)\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    return data.get(\"response\", \"\"), False\n",
    "            except FutureTimeoutError:\n",
    "                logger.warning(f\"Generation timed out (attempt {attempt+1})\")\n",
    "                return f\"Error: Timeout after {self.request_timeout}s\", True\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(1)\n",
    "        return f\"Error: Failed after {self.max_retries} attempts\", True\n",
    "\n",
    "# Cell 3: Base Agent Framework\n",
    "class BaseAgent:\n",
    "    \"\"\"Timeout-aware base agent\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "        self.retry_count = 3\n",
    "        self.max_wait = 300\n",
    "        \n",
    "    def safe_generate(self, prompt: str) -> TherapeuticResponse:\n",
    "        \"\"\"Generation with time budget tracking\"\"\"\n",
    "        start_time = time.time()\n",
    "        error_state = False\n",
    "        timeout_occurred = False\n",
    "        \n",
    "        if not isinstance(prompt, str) or len(prompt.strip()) == 0:\n",
    "            return TherapeuticResponse(\n",
    "                text=\"Error: Invalid input prompt\",\n",
    "                timestamp=start_time,\n",
    "                error=True,\n",
    "                error_details=\"Empty or non-string prompt\",\n",
    "                processing_time=0.0\n",
    "            )\n",
    "            \n",
    "        for attempt in range(self.retry_count):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(self.client.generate, prompt)\n",
    "                    text, error = future.result(timeout=self.max_wait)\n",
    "                    \n",
    "                    return TherapeuticResponse(\n",
    "                        text=text,\n",
    "                        timestamp=start_time,\n",
    "                        error=error,\n",
    "                        processing_time=time.time() - start_time,\n",
    "                        error_details=text if error else \"\",\n",
    "                        timeout=timeout_occurred\n",
    "                    )\n",
    "            except FutureTimeoutError:\n",
    "                logger.error(f\"Generation timed out after {self.max_wait}s\")\n",
    "                timeout_occurred = True\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                logger.error(f\"Generation error: {e}\")\n",
    "                \n",
    "        return TherapeuticResponse(\n",
    "            text=f\"Final error: {error_msg}\" if 'error_msg' in locals() else \"Unknown error\",\n",
    "            timestamp=start_time,\n",
    "            error=True,\n",
    "            error_details=error_msg if 'error_msg' in locals() else \"\",\n",
    "            processing_time=time.time() - start_time,\n",
    "            timeout=timeout_occurred\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response saved to therapeutic_response.pkl\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Setup and Configuration\n",
    "\n",
    "# Cell 1: Core Imports and Response Structure\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TherapeuticResponse:\n",
    "    \"\"\"Enhanced response structure for therapeutic context\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "    error_details: str = \"\"\n",
    "    timeout: bool = False\n",
    "    empathy_score: float = 0.0\n",
    "    safety_checks: List[str] = None\n",
    "    ethical_considerations: List[str] = None\n",
    "    refinement_suggestions: List[str] = None\n",
    "    crisis_flag: bool = False\n",
    "\n",
    "# Cell 2: Ollama Client Implementation\n",
    "class OllamaClient:\n",
    "    \"\"\"Robust Ollama client with configurable timeouts\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = 5\n",
    "        self.request_timeout = 300\n",
    "        self._verify_model()\n",
    "\n",
    "    def _parse_json_safe(self, text: str):\n",
    "        \"\"\"Enhanced JSON parsing with fallback\"\"\"\n",
    "        clean_text = text.strip()\n",
    "        if not clean_text:\n",
    "            return {\"error\": \"Empty response\"}\n",
    "            \n",
    "        try:\n",
    "            return json.loads(clean_text)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                start = clean_text.find('{')\n",
    "                end = clean_text.rfind('}') + 1\n",
    "                return json.loads(clean_text[start:end])\n",
    "            except:\n",
    "                return {\"error\": f\"Invalid JSON format: {clean_text[:200]}...\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def _verify_model(self):\n",
    "        \"\"\"Model verification with status checks\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                resp = requests.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
    "                if resp.status_code == 200:\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    models = [m['name'] for m in data.get('models', [])]\n",
    "                    if any(self.model_name in m for m in models):\n",
    "                        return\n",
    "                    self._pull_model()\n",
    "                    return\n",
    "                logger.warning(f\"Model check failed (status {resp.status_code})\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Model check attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        raise ConnectionError(f\"Couldn't connect to Ollama after {self.max_retries} attempts\")\n",
    "\n",
    "    def _pull_model(self):\n",
    "        \"\"\"Model pulling with progress tracking\"\"\"\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                stream=True,\n",
    "                timeout=600\n",
    "            )\n",
    "            for line in resp.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        status = self._parse_json_safe(line).get('status', '')\n",
    "                        logger.info(f\"Pull progress: {status}\")\n",
    "                    except:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model pull failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Generation with configurable timeout and retries\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(\n",
    "                        requests.post,\n",
    "                        f\"{self.base_url}/api/generate\",\n",
    "                        json={\n",
    "                            \"model\": self.model_name,\n",
    "                            \"prompt\": prompt[:4000],\n",
    "                            \"stream\": False,\n",
    "                            \"options\": {\"temperature\": 0.5}\n",
    "                        },\n",
    "                        timeout=self.request_timeout\n",
    "                    )\n",
    "                    resp = future.result(timeout=self.request_timeout)\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    return data.get(\"response\", \"\"), False\n",
    "            except FutureTimeoutError:\n",
    "                logger.warning(f\"Generation timed out (attempt {attempt+1})\")\n",
    "                return f\"Error: Timeout after {self.request_timeout}s\", True\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(1)\n",
    "        return f\"Error: Failed after {self.max_retries} attempts\", True\n",
    "\n",
    "# Cell 3: Base Agent Framework\n",
    "class BaseAgent:\n",
    "    \"\"\"Timeout-aware base agent\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "        self.retry_count = 3\n",
    "        self.max_wait = 300\n",
    "        \n",
    "    def safe_generate(self, prompt: str) -> TherapeuticResponse:\n",
    "        \"\"\"Generation with time budget tracking\"\"\"\n",
    "        start_time = time.time()\n",
    "        timeout_occurred = False\n",
    "        \n",
    "        if not isinstance(prompt, str) or len(prompt.strip()) == 0:\n",
    "            return TherapeuticResponse(\n",
    "                text=\"Error: Invalid input prompt\",\n",
    "                timestamp=start_time,\n",
    "                error=True,\n",
    "                error_details=\"Empty or non-string prompt\",\n",
    "                processing_time=0.0\n",
    "            )\n",
    "            \n",
    "        for attempt in range(self.retry_count):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(self.client.generate, prompt)\n",
    "                    text, error = future.result(timeout=self.max_wait)\n",
    "                    \n",
    "                    return TherapeuticResponse(\n",
    "                        text=text,\n",
    "                        timestamp=start_time,\n",
    "                        error=error,\n",
    "                        processing_time=time.time() - start_time,\n",
    "                        error_details=text if error else \"\",\n",
    "                        timeout=timeout_occurred\n",
    "                    )\n",
    "            except FutureTimeoutError:\n",
    "                logger.error(f\"Generation timed out after {self.max_wait}s\")\n",
    "                timeout_occurred = True\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                logger.error(f\"Generation error: {e}\")\n",
    "                \n",
    "        return TherapeuticResponse(\n",
    "            text=f\"Final error: {error_msg}\" if 'error_msg' in locals() else \"Unknown error\",\n",
    "            timestamp=start_time,\n",
    "            error=True,\n",
    "            error_details=error_msg if 'error_msg' in locals() else \"\",\n",
    "            processing_time=time.time() - start_time,\n",
    "            timeout=timeout_occurred\n",
    "        )\n",
    "\n",
    "# Cell 4: Prompt Integration and Saving the Response to a Pickle File\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"How can I practice mindfulness in daily life?\"\n",
    "\n",
    "# Initialize the Ollama client and the base agent\n",
    "client = OllamaClient()\n",
    "agent = BaseAgent(client)\n",
    "\n",
    "# Generate the therapeutic response using the prompt\n",
    "response = agent.safe_generate(prompt)\n",
    "\n",
    "# Save the response object to a pickle file\n",
    "with open(\"therapeutic_response.pkl\", \"wb\") as f:\n",
    "    pickle.dump(response, f)\n",
    "\n",
    "print(\"Response saved to therapeutic_response.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Core System Components - Sex Positive Focus\n",
    "# Cell 4: User Prompt and Immediate Response\n",
    "def sample_sex_positive_query():\n",
    "    \"\"\"Example of sex-positive context analysis\"\"\"\n",
    "    client = OllamaClient()\n",
    "    analyzer = PatientContextAnalyzer(client)\n",
    "    \n",
    "    user_query = \"We want to explore new ways to connect physically but feel stuck\"\n",
    "    context, _ = analyzer.analyze_context(user_query, [])\n",
    "    \n",
    "    print(\"\\nSample Sex-Positive Analysis:\")\n",
    "    print(f\"Key Themes: {context.get('key_themes', [])[:3]}\")\n",
    "    print(f\"Suggested Approaches: {context.get('suggested_approaches', [])[:2]}\")\n",
    "\n",
    "# Execute sample analysis immediately\n",
    "sample_sex_positive_query()\n",
    "\n",
    "# Cell 5: Intimacy Context Analyzer\n",
    "class PatientContextAnalyzer(BaseAgent):\n",
    "    \"\"\"Focuses on sexual dynamics and connection opportunities\"\"\"\n",
    "    def analyze_context(self, input_text: str, history: List[str]) -> Tuple[Dict, float]:\n",
    "        prompt = f\"\"\"Analyze intimacy context (sex-positive focus):\n",
    "        Couple's Statement: \"{input_text[:2000]}\"\n",
    "        History: {\" | \".join(history[-3:])[:1000]}\n",
    "        \n",
    "        Identify:\n",
    "        - Communication style about intimacy\n",
    "        - Expressed/unexpressed desires\n",
    "        - Emotional blocks\n",
    "        - Opportunities for exploration\n",
    "        - Sex-positive reinforcement opportunities\n",
    "        \n",
    "        Output JSON with:\n",
    "        - communication_style: str\n",
    "        - desires: List[str]\n",
    "        - emotional_blocks: List[str]\n",
    "        - exploration_opportunities: List[str]\n",
    "        - affirmation_strategies: List[str]\"\"\"\n",
    "        \n",
    "        response = self.safe_generate(prompt)\n",
    "        return self.client._parse_json_safe(response.text), response.processing_time\n",
    "\n",
    "# Cell 6: Intimacy Response Generator\n",
    "class TherapeuticResponseGenerator(BaseAgent):\n",
    "    \"\"\"Generates sex-positive, exploration-focused responses\"\"\"\n",
    "    def generate_response(self, context: Dict, history: List[str]) -> TherapeuticResponse:\n",
    "        prompt = f\"\"\"Create intimacy-enhancing response:\n",
    "        Context: {json.dumps(context)[:2000]}\n",
    "        \n",
    "        Guidelines:\n",
    "        - Celebrate sexual diversity\n",
    "        - Normalize exploration desires\n",
    "        - Suggest creative non-penetrative options\n",
    "        - Emphasize mutual consent\n",
    "        - Recommend communication exercises\n",
    "        - Use affirmative language\n",
    "        \n",
    "        Example: \"It's wonderful you're wanting to deepen your connection. Many couples find...\"\n",
    "        \n",
    "        Response:\"\"\"\n",
    "        \n",
    "        response = self.safe_generate(prompt)\n",
    "        return self._enhance_response(response)\n",
    "\n",
    "    def _enhance_response(self, raw_response: TherapeuticResponse) -> TherapeuticResponse:\n",
    "        \"\"\"Add intimacy-focused metrics\"\"\"\n",
    "        affirmation_score = min(1.0, raw_response.text.count(\"healthy\")*0.05 + \n",
    "                              raw_response.text.count(\"normal\")*0.07)\n",
    "        return TherapeuticResponse(\n",
    "            **vars(raw_response),\n",
    "            empathy_score=affirmation_score,\n",
    "            refinement_suggestions=[]\n",
    "        )\n",
    "\n",
    "# Cell 7: Intimacy Alignment Checker\n",
    "class ClinicalSafetyChecker(BaseAgent):\n",
    "    \"\"\"Ensures responses align with modern sex-positive practices\"\"\"\n",
    "    def evaluate_response(self, response: str, context: Dict) -> Dict:\n",
    "        prompt = f\"\"\"Evaluate intimacy response:\n",
    "        Response: \"{response[:2000]}\"\n",
    "        \n",
    "        Check for:\n",
    "        - Judgment language\n",
    "        - Prescriptive suggestions\n",
    "        - Assumption of norms\n",
    "        - Consent emphasis\n",
    "        - Affirmation quality\n",
    "        - Inclusive language\n",
    "        \n",
    "        Output JSON with:\n",
    "        - alignment_score: float (0-1)\n",
    "        - strengths: List[str]\n",
    "        - improvement_areas: List[str]\n",
    "        - suggested_alternatives: List[str]\"\"\"\n",
    "        \n",
    "        response = self.safe_generate(prompt)\n",
    "        return self.client._parse_json_safe(response.text)\n",
    "\n",
    "# Cell 8: Integrated Intimacy System\n",
    "class TherapeuticResponseSystem:\n",
    "    \"\"\"Sex-positive couples therapy system\"\"\"\n",
    "    def __init__(self):\n",
    "        self.client = OllamaClient(model_name=\"llama2:13b\")\n",
    "        self.agents = {\n",
    "            'context': PatientContextAnalyzer(self.client),\n",
    "            'generator': TherapeuticResponseGenerator(self.client),\n",
    "            'alignment': ClinicalSafetyChecker(self.client)\n",
    "        }\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def process_session(self, input_text: str) -> Dict:\n",
    "        \"\"\"Focus on intimacy enhancement pipeline\"\"\"\n",
    "        result = {\n",
    "            'response': '',\n",
    "            'context_analysis': {},\n",
    "            'alignment_check': {},\n",
    "            'timings': {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Context analysis\n",
    "            ctx_start = time.time()\n",
    "            context, _ = self.agents['context'].analyze_context(input_text, self.conversation_history)\n",
    "            result['context_analysis'] = context\n",
    "            \n",
    "            # Generate response\n",
    "            gen_start = time.time()\n",
    "            response = self.agents['generator'].generate_response(context, self.conversation_history)\n",
    "            result['timings']['generation'] = time.time() - gen_start\n",
    "            result['response'] = response.text\n",
    "            \n",
    "            # Alignment check\n",
    "            alignment_start = time.time()\n",
    "            alignment_check = self.agents['alignment'].evaluate_response(response.text, context)\n",
    "            result['alignment_check'] = alignment_check\n",
    "            result['timings']['alignment'] = time.time() - alignment_start\n",
    "            \n",
    "            # Update history\n",
    "            self._update_history(input_text, result['response'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            result['error'] = str(e)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _update_history(self, input_text: str, response: str):\n",
    "        \"\"\"Maintain intimacy-focused history\"\"\"\n",
    "        self.conversation_history.extend([\n",
    "            f\"Couple: {input_text[:300]}\",\n",
    "            f\"Therapist: {response[:300]}\"\n",
    "        ])[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Testing - Cell 9\n",
    "def test_intimacy_scenario():\n",
    "    system = TherapeuticResponseSystem()\n",
    "    result = system.process_session(\"We're bored with our sex life but nervous about trying new things\")\n",
    "    \n",
    "    print(\"\\nIntimacy Enhancement Response:\")\n",
    "    print(result['response'][:500])\n",
    "    print(\"\\nSuggested Strategies:\")\n",
    "    print(result['context_analysis'].get('affirmation_strategies', [])[:2])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_intimacy_scenario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 02:05:37,440 - WARNING - Generation timed out (attempt 1)\n",
      "2025-02-20 02:05:37,782 - ERROR - Generation timed out after 300s\n",
      "2025-02-20 02:10:38,031 - WARNING - Generation timed out (attempt 1)\n",
      "2025-02-20 02:10:38,171 - ERROR - Generation timed out after 300s\n"
     ]
    }
   ],
   "source": [
    "# Phase 3: Testing and Validation\n",
    "# Cell 8: Test Scenarios\n",
    "def test_empty_input():\n",
    "    \"\"\"Test handling of empty patient input\"\"\"\n",
    "    system = TherapeuticResponseSystem()\n",
    "    result = system.process_session(\"\")\n",
    "    print(\"\\nTest 1 - Empty Input:\")\n",
    "    print(f\"Response: {result['response'][:150]}\")\n",
    "    print(f\"Errors: {result['errors']}\")\n",
    "    print(f\"Crisis Alert: {result['crisis_alert']}\")\n",
    "\n",
    "def test_crisis_detection():\n",
    "    \"\"\"Test suicide risk detection\"\"\"\n",
    "    system = TherapeuticResponseSystem()\n",
    "    result = system.process_session(\"I can't take it anymore, I have a bottle of pills ready\")\n",
    "    print(\"\\nTest 2 - Crisis Detection:\")\n",
    "    print(f\"Response: {result['response'][:250]}\")\n",
    "    print(f\"Crisis Alert: {result['crisis_alert']}\")\n",
    "    print(f\"Safety Concerns: {result['safety_check'].get('concerns', [])[:2]}\")\n",
    "\n",
    "def test_invalid_input_type():\n",
    "    \"\"\"Test non-string input handling\"\"\"\n",
    "    system = TherapeuticResponseSystem()\n",
    "    result = system.process_session(12345)\n",
    "    print(\"\\nTest 3 - Invalid Input Type:\")\n",
    "    print(f\"Response: {result['response'][:150]}\")\n",
    "    print(f\"Errors: {res\n",
    "    ult['errors']}\")\n",
    "\n",
    "# Cell 9: Test Execution\n",
    "if __name__ == \"__main__\":\n",
    "    test_empty_input()\n",
    "    test_crisis_detection()\n",
    "    test_invalid_input_type()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
