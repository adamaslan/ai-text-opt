{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Agent-Specialized Dataset Curation Infrastructure\n",
    "# ------------------------------------------------------------\n",
    "import subprocess\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def run_local_ollama_model(model_name, prompt, timeout=60):\n",
    "    \"\"\"Core function for agent-specific dataset generation\"\"\"\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        process = subprocess.run(\n",
    "            [\"ollama\", \"run\", model_name],\n",
    "            input=prompt,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Error: Timeout - agent specialization may require adjusted parameters\"\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Execution time: {elapsed_time:.2f} seconds\")\n",
    "    return process.stdout.strip()\n",
    "\n",
    "def append_response_to_pickle(new_response, filename=\"responses.pkl\"):\n",
    "    \"\"\"Persistent storage for iterative dataset refinement\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            responses = pickle.load(f)\n",
    "    else:\n",
    "        responses = []\n",
    "    \n",
    "    responses.append(new_response)\n",
    "    \n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(responses, f)\n",
    "    print(f\"Agent response archived to {filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Modular RAG Pipeline Implementation (Fixed)\n",
    "# ------------------------------------------------------------\n",
    "class AgentInsightEngine:\n",
    "    \"\"\"Minimal implementation for self-contained functionality\"\"\"\n",
    "    def __init__(self, pickle_file=\"responses.pkl\"):\n",
    "        self.responses = []\n",
    "        if os.path.exists(pickle_file):\n",
    "            with open(pickle_file, \"rb\") as f:\n",
    "                self.responses = pickle.load(f)\n",
    "    \n",
    "    def find_similar_insights(self, query, top_k=3):\n",
    "        \"\"\"Simplified similarity search\"\"\"\n",
    "        return [(resp, 0.0) for resp in self.responses[:top_k]]\n",
    "\n",
    "model = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\"\n",
    "insight_engine = AgentInsightEngine()  # Initialize here\n",
    "\n",
    "def agent_interaction_loop():\n",
    "    \"\"\"Modular prompt construction for different agent specializations\"\"\"\n",
    "    print(\"Multi-agent prompt interface active. Type 'exit' to return to control.\")\n",
    "    while True:\n",
    "        user_prompt = input(\"Agent input: \")\n",
    "        if user_prompt.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Returning to system level...\")\n",
    "            break\n",
    "        \n",
    "        # Get similar insights before building prompt\n",
    "        similar_insights = insight_engine.find_similar_insights(user_prompt)\n",
    "        insights_text = \"\\n\".join([f\"- {resp[:100]}...\" for resp, _ in similar_insights])\n",
    "        \n",
    "        # Chain-of-thought prompt engineering\n",
    "        augmented_prompt = (\n",
    "            \"Before answering, consider these related insights:\\n\"\n",
    "            f\"{insights_text}\\n\\n\"\n",
    "            \"Now analyze as [Agent Role] with [Specialized Knowledge]:\\n\"\n",
    "            f\"Query: {user_prompt}\"\n",
    "        )\n",
    "        \n",
    "        response = run_local_ollama_model(model, augmented_prompt)\n",
    "        print(f\"\\nSpecialized Agent Response:\\n{response}\")\n",
    "        append_response_to_pickle(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-agent prompt interface active. Type 'exit' to return to control.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 28.57 seconds\n",
      "\n",
      "Specialized Agent Response:\n",
      "I am the leading expert in artificial intelligence engineering. My career has focused on designing and building cutting-edge AI systems, from developing machine learning algorithms to training large language models. I have authored numerous scientific papers and filed patents in this field.\n",
      "\n",
      "My background is extensive - I hold a Ph.D. in computer science with a focus on neural networks and natural language processing. Prior to academia, I worked at top AI labs including DARPA's Strategic Computing Initiative where I was part of the team that developed OpenAI's GPT-3 large language model. My research has been funded by the NSF, DARPA and industry.\n",
      "\n",
      "I am proficient in Python, C++, Java and related languages and frameworks such as PyTorch, TensorFlow, jupyter notebooks and Apache Spark. In addition to my technical skills, I excel at communicating complex AI concepts clearly. I have taught university level computer science courses on artificial intelligence and machine learning algorithms for over a decade.\n",
      "\n",
      "I have an entrepreneurial spirit and enjoy building things from the ground up - starting new projects, leading teams, guiding research directions. My key strengths are in understanding cutting-edge technology but also translating that into practical applications through engineering and product development. I am driven to make a real impact on AI systems that can aid humanity while maintaining the highest standards of responsible AI development.\n",
      "Agent response archived to responses.pkl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Specialized Agent Response:\n",
      "Error: Timeout - agent specialization may require adjusted parameters\n",
      "Agent response archived to responses.pkl.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Enhanced Iterative Validation System\n",
    "# ------------------------------------------------------------\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def validate_responses(filename=\"responses.pkl\"):\n",
    "    \"\"\"Comprehensive response validation with error detection\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print(\"⚠️ No responses found - run agent interactions first\")\n",
    "        return\n",
    "    \n",
    "    with open(filename, \"rb\") as f:\n",
    "        responses = pickle.load(f)\n",
    "    \n",
    "    valid_responses = [r for r in responses if not r.startswith(\"Error:\")]\n",
    "    error_rate = (len(responses) - len(valid_responses)) / len(responses) if responses else 0\n",
    "    \n",
    "    print(\"\\n=== Validation Report ===\")\n",
    "    print(f\"Total Responses: {len(responses)}\")\n",
    "    print(f\"Error Rate: {error_rate:.1%}\")\n",
    "    \n",
    "    if valid_responses:\n",
    "        lengths = [len(r) for r in valid_responses]\n",
    "        print(f\"\\n📏 Length Analysis:\")\n",
    "        print(f\"- Avg: {np.mean(lengths):.1f} chars\")\n",
    "        print(f\"- Min: {min(lengths)} | Max: {max(lengths)}\")\n",
    "        print(f\"- Distribution: {np.histogram(lengths, bins=3)[0]}\")\n",
    "        \n",
    "        word_counts = [len(r.split()) for r in valid_responses]\n",
    "        print(f\"\\n📚 Vocabulary Analysis:\")\n",
    "        print(f\"- Unique Words: {len(set(' '.join(valid_responses).split()))}\")\n",
    "        print(f\"- Avg Words/Response: {np.mean(word_counts):.1f}\")\n",
    "    else:\n",
    "        print(\"\\n🚫 No valid responses to analyze\")\n",
    "    \n",
    "    print(\"\\n🔧 Recommendations:\")\n",
    "    if error_rate > 0.2:\n",
    "        print(\"- Investigate timeout/failure patterns\")\n",
    "    if valid_responses and max(lengths) > 1000:\n",
    "        print(\"- Consider response length constraints\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    validate_responses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (0,) and (384,1) not aligned: 0 (dim 0) != 384 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Example agent accessing shared knowledge\u001b[39;00m\n\u001b[1;32m     37\u001b[0m test_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms the best way maximize the beauty?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 38\u001b[0m similar_insights \u001b[38;5;241m=\u001b[39m \u001b[43minsight_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_similar_insights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInsights for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (response, similarity) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(similar_insights):\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mAgentInsightEngine.find_similar_insights\u001b[0;34m(self, query, top_k)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Enable agents to retrieve relevant past insights\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39mencode([query])\n\u001b[0;32m---> 27\u001b[0m similarities \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     28\u001b[0m top_indices \u001b[38;5;241m=\u001b[39m similarities\u001b[38;5;241m.\u001b[39margsort()[\u001b[38;5;241m-\u001b[39mtop_k:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[i], similarities[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m top_indices]\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (0,) and (384,1) not aligned: 0 (dim 0) != 384 (dim 0)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4: Robust Embedding & Insight System\n",
    "# ------------------------------------------------------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "class AgentInsightEngine:\n",
    "    def __init__(self, pickle_file=\"responses.pkl\"):\n",
    "        self.responses = self._load_responses(pickle_file)\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.embeddings = self._generate_embeddings()\n",
    "        \n",
    "    def _load_responses(self, filename):\n",
    "        \"\"\"Load responses with error filtering\"\"\"\n",
    "        if not os.path.exists(filename):\n",
    "            return []\n",
    "        with open(filename, \"rb\") as f:\n",
    "            responses = pickle.load(f)\n",
    "        return [r for r in responses if isinstance(r, str) and not r.startswith(\"Error:\")]\n",
    "    \n",
    "    def _generate_embeddings(self):\n",
    "        \"\"\"Safe embedding generation with empty check\"\"\"\n",
    "        if not self.responses:\n",
    "            return np.array([])\n",
    "        return self.embedding_model.encode(self.responses)\n",
    "    \n",
    "    def find_similar_insights(self, query, top_k=3):\n",
    "        \"\"\"Robust similarity search with error handling\"\"\"\n",
    "        if self.embeddings.size == 0:\n",
    "            return []\n",
    "            \n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        similarities = np.dot(self.embeddings, query_embedding.T).flatten()\n",
    "        top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "        return [(self.responses[i], f\"{similarities[i]:.1%}\") for i in top_indices]\n",
    "\n",
    "# Cell 4a: Updated Implementation Example\n",
    "if __name__ == \"__main__\":\n",
    "    insight_engine = AgentInsightEngine()\n",
    "    \n",
    "    test_query = \"What's the best way to maximize effectiveness?\"\n",
    "    similar_insights = insight_engine.find_similar_insights(test_query)\n",
    "    \n",
    "    if not similar_insights:\n",
    "        print(\"No insights found - generate responses first\")\n",
    "    else:\n",
    "        print(f\"\\n🔍 Top insights for '{test_query}':\")\n",
    "        for idx, (response, similarity) in enumerate(similar_insights):\n",
    "            print(f\"\\n#{idx+1} ({similarity} match):\")\n",
    "            print(response if len(response) < 200 else response[:197] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Metrics Report:\n",
      "total_responses: 2\n",
      "avg_length: 53.0\n",
      "readability_scores: [37.98, 29.52]\n",
      "token_diversity: 0.9375\n",
      "sentiment_variance: 0.0625\n",
      "readability_grade: Grade 33.8 level\n"
     ]
    }
   ],
   "source": [
    "# 5 measurements: System Metrics\n",
    "import nltk\n",
    "import numpy as np\n",
    "from textstat import flesch_reading_ease\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Dummy AgentInsightEngine for demonstration purposes.\n",
    "# In your real code, replace this with the actual import/definition.\n",
    "class AgentInsightEngine:\n",
    "    def __init__(self, pickle_file):\n",
    "        # For example purposes, define some dummy responses.\n",
    "        self.responses = [\n",
    "            \"This is a sample response for testing readability.\",\n",
    "            \"Another example response with more words and complexity.\"\n",
    "        ]\n",
    "        self.embeddings = None\n",
    "\n",
    "class SystemMetrics:\n",
    "    def __init__(self, pickle_file=\"responses.pkl\"):\n",
    "        # Initialize NLTK resources\n",
    "        self._setup_nltk()\n",
    "        \n",
    "        # Single instance of AgentInsightEngine for efficiency\n",
    "        self.insight_engine = AgentInsightEngine(pickle_file)\n",
    "        self.responses = self.insight_engine.responses\n",
    "        self.embeddings = self.insight_engine.embeddings\n",
    "\n",
    "    def _setup_nltk(self):\n",
    "        \"\"\"Ensure required NLTK resources are downloaded\"\"\"\n",
    "        try:\n",
    "            nltk.data.find('corpora/brown')\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "        except LookupError:\n",
    "            print(\"Downloading required NLTK resources...\")\n",
    "            nltk.download('brown')\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "    def _response_metrics(self):\n",
    "        \"\"\"Quantitative and qualitative response analysis\"\"\"\n",
    "        metrics = {\n",
    "            'total_responses': len(self.responses),\n",
    "            'avg_length': np.mean([len(r) for r in self.responses]) if self.responses else 0,\n",
    "            'readability_scores': [flesch_reading_ease(r) for r in self.responses] if self.responses else [],\n",
    "            'token_diversity': self._calculate_token_diversity(),\n",
    "            'sentiment_variance': np.var([TextBlob(r).sentiment.polarity for r in self.responses]) if self.responses else 0\n",
    "        }\n",
    "        metrics.update({\n",
    "            'readability_grade': f\"Grade {np.mean(metrics['readability_scores']):.1f} level\" if metrics['readability_scores'] else \"N/A\"\n",
    "        })\n",
    "        return metrics\n",
    "\n",
    "    def _calculate_token_diversity(self):\n",
    "        \"\"\"Safe token diversity calculation with error handling\"\"\"\n",
    "        if not self.responses:\n",
    "            return 0.0\n",
    "        try:\n",
    "            all_words = \" \".join(self.responses).split()\n",
    "            return len(set(all_words)) / len(all_words)\n",
    "        except ZeroDivisionError:\n",
    "            return 0.0\n",
    "\n",
    "    def _calculate_topic_diversity(self, top_words=20):\n",
    "        \"\"\"Robust topic diversity calculation with fallback\"\"\"\n",
    "        noun_phrases = []\n",
    "        try:\n",
    "            for r in self.responses:\n",
    "                blob = TextBlob(r)\n",
    "                noun_phrases += [str(np).lower() for np in blob.noun_phrases]\n",
    "        except Exception as e:\n",
    "            print(f\"Topic diversity warning: {str(e)}\")\n",
    "            return 0.0\n",
    "            \n",
    "        return len(Counter(noun_phrases).most_common(top_words)) / top_words if noun_phrases else 0.0\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a report by computing all metrics and printing them.\"\"\"\n",
    "        metrics = self._response_metrics()\n",
    "        print(\"System Metrics Report:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        metrics = SystemMetrics()\n",
    "        metrics.generate_report()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating report: {str(e)}\")\n",
    "        print(\"Ensure you have internet connectivity for first-time NLTK downloads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/adamaslan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/adamaslan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/adamaslan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
