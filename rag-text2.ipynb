{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Base Configuration and Retrieval Setup\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Union\n",
    "import yaml\n",
    "import torch\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, config_path: str):\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.setup_logging()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.retriever = None\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def _load_config(self, config_path: str) -> Dict:\n",
    "        with open(config_path, 'r') as f:\n",
    "            return yaml.safe_load(f)\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(\n",
    "            level=self.config.get('logging_level', 'INFO'),\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[logging.FileHandler('rag.log'), logging.StreamHandler()]\n",
    "        )\n",
    "\n",
    "    def load_knowledge_base(self, file_path: str):\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                self.documents = [line.strip() for line in f if line.strip()]\n",
    "            \n",
    "            embed_model = SentenceTransformer(self.config['retriever_model'])\n",
    "            embeddings = embed_model.encode(self.documents)\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "            self.index.add(embeddings.astype('float32'))\n",
    "            self.retriever = embed_model\n",
    "            self.logger.info(f\"Loaded {len(self.documents)} documents\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Knowledge base error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
    "        try:\n",
    "            query_embed = self.retriever.encode([query])\n",
    "            distances, indices = self.index.search(query_embed.astype('float32'), k)\n",
    "            return [self.documents[i] for i in indices[0]]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Retrieval error: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Generation Component Setup\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "class RAGPipeline(RAGPipeline):\n",
    "    def setup_generator(self):\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config['model_name'], use_fast=True\n",
    "            )\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config['model_name'],\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto'\n",
    "            )\n",
    "            peft_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                r=8,\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"q_proj\", \"v_proj\"]\n",
    "            )\n",
    "            self.model = get_peft_model(base_model, peft_config)\n",
    "            self.logger.info(\"Generator ready\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Generator setup error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def rag_generate(self, query: str, max_length: int = 500) -> str:\n",
    "        try:\n",
    "            context = self.retrieve(query)\n",
    "            prompt = f\"Context: {' '.join(context)}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=self.config['max_length']\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=max_length,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "            \n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Generation error: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Ollama Integration\n",
    "import subprocess\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "HISTORY_FILE = \"rag_history.pkl\"\n",
    "\n",
    "class RAGPipeline(RAGPipeline):\n",
    "    def _run_ollama(self, prompt: str) -> str:\n",
    "        response_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model': self.config['ollama_model'],\n",
    "            'prompt': prompt,\n",
    "            'response': '',\n",
    "            'execution_time': 0,\n",
    "            'context_used': []\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            process = subprocess.run(\n",
    "                [\"ollama\", \"run\", self.config['ollama_model']],\n",
    "                input=prompt,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=self.config.get('timeout', 60)\n",
    "            )\n",
    "\n",
    "            response = process.stdout.strip()\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            response_data.update({\n",
    "                'response': response,\n",
    "                'execution_time': round(elapsed_time, 2),\n",
    "                'context_used': self.last_retrieved_context\n",
    "            })\n",
    "\n",
    "            self._save_history(response_data)\n",
    "            return response\n",
    "\n",
    "        except subprocess.TimeoutExpired:\n",
    "            response_data['response'] = \"Error: Response timed out.\"\n",
    "            self._save_history(response_data)\n",
    "            return response_data['response']\n",
    "\n",
    "    def _save_history(self, response_data: dict):\n",
    "        try:\n",
    "            history = {'interactions': []}\n",
    "            if os.path.exists(HISTORY_FILE):\n",
    "                with open(HISTORY_FILE, \"rb\") as f:\n",
    "                    history = pickle.load(f)\n",
    "            \n",
    "            history['interactions'].append(response_data)\n",
    "            \n",
    "            with open(HISTORY_FILE, \"wb\") as f:\n",
    "                pickle.dump(history, f)\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"History save error: {str(e)}\")\n",
    "\n",
    "    def show_history(self):\n",
    "        try:\n",
    "            with open(HISTORY_FILE, \"rb\") as f:\n",
    "                history = pickle.load(f)\n",
    "                print(f\"\\n{'='*40}\\nRAG Interaction History\")\n",
    "                for idx, interaction in enumerate(history['interactions'], 1):\n",
    "                    print(f\"\\nInteraction {idx} ({interaction['timestamp']}):\")\n",
    "                    print(f\"Model: {interaction['model']}\")\n",
    "                    print(f\"Context Used: {interaction['context_used'][:2]}...\")\n",
    "                    print(f\"Prompt: {interaction['prompt'][:100]}...\")\n",
    "                    print(f\"Response: {interaction['response'][:200]}...\")\n",
    "                    print(f\"Execution time: {interaction['execution_time']}s\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"No history available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Unified Interface and Final Integration\n",
    "class RAGPipeline(RAGPipeline):\n",
    "    def generate(self, query: str, use_ollama: bool = False) -> str:\n",
    "        context = self.retrieve(query)\n",
    "        self.last_retrieved_context = context  # Store for history\n",
    "        prompt = f\"Context: {' '.join(context)}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "        \n",
    "        if use_ollama:\n",
    "            return self._run_ollama(prompt)\n",
    "        return self.rag_generate(prompt)\n",
    "\n",
    "    def interactive_session(self):\n",
    "        print(\"RAG Interactive Session (type 'exit' to quit)\")\n",
    "        while True:\n",
    "            query = input(\"\\nUser: \")\n",
    "            if query.lower() in ['exit', 'quit']:\n",
    "                break\n",
    "                \n",
    "            use_ollama = input(\"Use Ollama? (y/n): \").lower() == 'y'\n",
    "            response = self.generate(query, use_ollama=use_ollama)\n",
    "            print(f\"\\nAssistant: {response}\")\n",
    "            \n",
    "        print(\"\\nSession history:\")\n",
    "        self.show_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
