{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/49/6ydqkbq172ngzt6p49xfm6b00000gn/T/ipykernel_59814/374682112.py:51: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "No sentence-transformers model found with name roberta-base. Creating a new one with mean pooling.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 150 entries\n",
      "GPT-Neo 125M model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/49/6ydqkbq172ngzt6p49xfm6b00000gn/T/ipykernel_59814/374682112.py:101: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://c879a45016083d0d20.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c879a45016083d0d20.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# works responses not great - better rag needed\n",
    "# Install required packages\n",
    "# pip install langchain huggingface_hub pandas faiss-cpu numpy transformers torch accelerate bitsandbytes gradio\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "def load_embeddings(csv_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Verify required columns exist\n",
    "        if 'Cleaned_Ideas' not in df.columns or 'Embeddings' not in df.columns:\n",
    "            raise ValueError(\"CSV must contain 'Cleaned_Ideas' and 'Embeddings' columns\")\n",
    "            \n",
    "        # Convert embeddings with proper handling\n",
    "        df['Embeddings'] = df['Embeddings'].apply(\n",
    "            lambda x: np.fromstring(\n",
    "                x.strip(\"[]\").replace(\"\\n\", \"\"),\n",
    "                sep=\", \",\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Validate embedding dimensions (RoBERTa-base has 768 dimensions)\n",
    "        expected_dim = 768\n",
    "        valid_embeddings = df['Embeddings'].apply(lambda x: len(x) == expected_dim)\n",
    "        if not valid_embeddings.all():\n",
    "            invalid_count = len(df) - valid_embeddings.sum()\n",
    "            raise ValueError(f\"{invalid_count} entries have invalid embedding dimensions\")\n",
    "            \n",
    "        return df['Cleaned_Ideas'].tolist(), np.array(df['Embeddings'].tolist())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embeddings: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# 1. Load texts and embeddings from CSV\n",
    "texts, embeddings = load_embeddings(\"ideas_with_embeddings.csv\")\n",
    "\n",
    "# 2. Create FAISS vector store using a Hugging Face embedding model (using roberta-base)\n",
    "try:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"roberta-base\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        # Removed \"show_progress_bar\" to avoid duplicate parameter issues.\n",
    "        encode_kwargs={'normalize_embeddings': False}\n",
    "    )\n",
    "    \n",
    "    # Create FAISS index; each entry is a tuple (text, embedding)\n",
    "    vector_store = FAISS.from_embeddings(\n",
    "        text_embeddings=list(zip(texts, embeddings)),\n",
    "        embedding=embedding_model,\n",
    "        normalize_L2=True  # Improves cosine similarity calculations\n",
    "    )\n",
    "    print(f\"FAISS index created with {vector_store.index.ntotal} entries\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Vector store creation failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# 3. Load a smaller Hugging Face model in safetensors format (EleutherAI/gpt-neo-125M)\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "try:\n",
    "    # For a smaller model, we typically use full precision\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"GPT-Neo 125M model loaded successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Model loading failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Create the text-generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=300,  # Increased from 256 to 300\n",
    "    min_new_tokens=100,   # Added minimum token requirement\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    do_sample=True,\n",
    "    return_full_text=False,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 4. Define a custom prompt template\n",
    "template = \"\"\"### Instruction:\n",
    "Analyze this philosophical concept using the provided context. \n",
    "If unsure, state \"I don't have sufficient information.\"\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template_format=\"f-string\"\n",
    ")\n",
    "\n",
    "# 5. Set up a Production-grade Retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5, \"score_threshold\": 0.4}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": prompt,\n",
    "        \"document_prompt\": PromptTemplate(\n",
    "            input_variables=[\"page_content\"],\n",
    "            template=\"{page_content}\"\n",
    "        )\n",
    "    },\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 6. Function to process queries for Gradio interface\n",
    "def process_query(query):\n",
    "    if not query.strip():\n",
    "        return \"Please enter a valid question\", \"\"\n",
    "    \n",
    "    try:\n",
    "        result = qa_chain({\"query\": query})\n",
    "        \n",
    "        # Process the response\n",
    "        response = result['result'].split(\"### Response:\")[-1].strip()\n",
    "        \n",
    "        # Format source information\n",
    "        sources_info = \"\\n\\n**Sources:**\\n\"\n",
    "        for i, doc in enumerate(result['source_documents'][:3], 1):\n",
    "            excerpt = doc.page_content[:150].replace(\"\\n\", \" \") + \"...\"\n",
    "            score = doc.metadata.get('score', 0)\n",
    "            sources_info += f\"{i}. {excerpt} (Score: {score:.2f})\\n\\n\"\n",
    "            \n",
    "        return response, sources_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing request: {str(e)}\", \"\"\n",
    "\n",
    "# 7. Create Gradio interface\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks(title=\"Philosophical Concepts RAG\") as demo:\n",
    "        gr.Markdown(\"# Adam's Fun RAG System\")\n",
    "        gr.Markdown(\"Ask questions about philosophical concepts and get answers based on the knowledge base.\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=4):\n",
    "                query_input = gr.Textbox(\n",
    "                    label=\"Your Question\",\n",
    "                    placeholder=\"Enter your philosophical question here...\",\n",
    "                    lines=2\n",
    "                )\n",
    "                submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                response_output = gr.Markdown(label=\"Response\")\n",
    "            with gr.Column(scale=2):\n",
    "                sources_output = gr.Markdown(label=\"Sources\")\n",
    "                \n",
    "        submit_btn.click(\n",
    "            fn=process_query,\n",
    "            inputs=[query_input],\n",
    "            outputs=[response_output, sources_output]\n",
    "        )\n",
    "        \n",
    "        gr.Markdown(\"## Examples\")\n",
    "        examples = gr.Examples(\n",
    "            examples=[\n",
    "                [\"What is the concept of dualism?\"],\n",
    "                [\"How do you maximize maximize the beauty?\"],\n",
    "                [\"What is full expression?\"]\n",
    "            ],\n",
    "            inputs=query_input\n",
    "        )\n",
    "        \n",
    "    return demo\n",
    "\n",
    "# 8. Run the chat interface with Gradio\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(share=True)  # share=True creates a public link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
