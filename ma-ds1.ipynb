{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 14:27:31,483 - WARNING - Model Gemmasutra-Mini-2B-v1-GGUF not found. Attempting to pull...\n",
      "2025-02-19 14:27:31,854 - INFO - Successfully pulled model hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing normal ===\n",
      "Test failed: Processing too slow\n",
      "\n",
      "=== Testing empty input ===\n",
      "Passed in 0.0s\n",
      "\n",
      "=== Testing factual ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 14:31:55,751 - ERROR - Ollama API error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test failed: Processing too slow\n",
      "\n",
      "=== Testing emotional ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 14:32:27,345 - ERROR - Ollama API error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=30)\n",
      "2025-02-19 14:32:59,640 - ERROR - Ollama API error: HTTPConnectionPool(host='localhost', port=11434): Read timed out. (read timeout=30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test failed: Processing too slow\n",
      "\n",
      "Final Metrics Report:\n",
      "=== System Metrics ===\n",
      "ContextManager: 3 calls, Avg: 23.26\n",
      "Generator: 3 calls, Avg: 1646.00\n",
      "Critic: 2 calls, Avg: 0.00\n",
      "Refiner: 2 calls, Avg: 0.00\n",
      "Validator: 2 calls, Avg: 16.70\n",
      "EmotionAnalyzer: 2 calls, Avg: 0.00\n",
      "Summarizer: 2 calls, Avg: 0.00\n",
      "\n",
      "Quality Metrics:\n",
      "Avg Emotion: 3.0\n",
      "Avg Compression: 32.3%\n",
      "Total Issues Found: 40\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Base Classes\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    \"\"\"Enhanced response structure with multi-agent processing metadata\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "    critique: Optional[str] = None\n",
    "    refinement: Optional[str] = None\n",
    "    iterations: int = 0\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Handles communication with Ollama API\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", \n",
    "                 base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self._check_model()\n",
    "        \n",
    "    def _check_model(self):\n",
    "        \"\"\"Verify model exists in Ollama\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/api/tags\")\n",
    "            response.raise_for_status()\n",
    "            available_models = [model['name'] for model in response.json().get('models', [])]\n",
    "            model_base = self.model_name.split('/')[-1].split(':')[0]\n",
    "            \n",
    "            if model_base not in available_models:\n",
    "                logger.warning(f\"Model {model_base} not found. Attempting to pull...\")\n",
    "                self._pull_model()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking model: {e}\")\n",
    "            raise\n",
    "                \n",
    "    def _pull_model(self):\n",
    "        \"\"\"Pull model from Hugging Face\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                timeout=600\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            logger.info(f\"Successfully pulled model {self.model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to pull model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate response from Ollama model\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.9,\n",
    "                        \"top_k\": 40\n",
    "                    }\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json().get(\"response\", \"Error: Empty response from API\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ollama API error: {e}\")\n",
    "            return f\"Error: Failed to generate response - {str(e)}\"\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Base class for all agents with common functionality\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "\n",
    "    def generate_response(self, prompt: str) -> Response:\n",
    "        \"\"\"Base method for generating responses\"\"\"\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            text = self.client.generate(prompt)\n",
    "            error = text.startswith(\"Error:\")\n",
    "        except Exception as e:\n",
    "            text = f\"Error: {e}\"\n",
    "            error = True\n",
    "        return Response(\n",
    "            text=text,\n",
    "            timestamp=start_time,\n",
    "            error=error,\n",
    "            processing_time=time.time() - start_time\n",
    "        )\n",
    "\n",
    "# Cell 2: Core Agents\n",
    "class GeneratorAgent(BaseAgent):\n",
    "    \"\"\"Generates initial responses with contextual awareness\"\"\"\n",
    "    def generate(self, base_prompt: str, context: str) -> Response:\n",
    "        prompt = f\"\"\"Generate a comprehensive response using context:\n",
    "        {context}\n",
    "        Query: {base_prompt}\n",
    "        Structured response:\"\"\"\n",
    "        return self.generate_response(prompt)\n",
    "\n",
    "class CriticAgent(BaseAgent):\n",
    "    \"\"\"Analyzes responses for quality and accuracy\"\"\"\n",
    "    def critique(self, prompt: str, response: str) -> Response:\n",
    "        critique_prompt = f\"\"\"Critique this response:\n",
    "        Query: {prompt}\n",
    "        Response: {response}\n",
    "        List strengths/weaknesses/suggestions:\"\"\"\n",
    "        return self.generate_response(critique_prompt)\n",
    "\n",
    "class RefinerAgent(BaseAgent):\n",
    "    \"\"\"Improves responses based on feedback\"\"\"\n",
    "    def refine(self, prompt: str, response: str, critique: str) -> Response:\n",
    "        refine_prompt = f\"\"\"Improve this response:\n",
    "        Original Query: {prompt}\n",
    "        Original Response: {response}\n",
    "        Critique: {critique}\n",
    "        Enhanced Response:\"\"\"\n",
    "        return self.generate_response(refine_prompt)\n",
    "\n",
    "# Cell 3: Enhanced Agents\n",
    "class ContextManagerAgent(BaseAgent):\n",
    "    \"\"\"Manages and enhances conversation context\"\"\"\n",
    "    def enhance_context(self, query: str, current_context: str) -> tuple:\n",
    "        start_time = time.time()\n",
    "        prompt = f\"\"\"Enhance context with relevant information:\n",
    "        Current Context: {current_context}\n",
    "        Query: {query}\n",
    "        Enhanced Context:\"\"\"\n",
    "        response = self.generate_response(prompt)\n",
    "        return response.text, time.time() - start_time\n",
    "\n",
    "class ValidatorAgent(BaseAgent):\n",
    "    \"\"\"Validates responses against knowledge base\"\"\"\n",
    "    def validate(self, response: str, knowledge_base: dict) -> tuple:\n",
    "        start_time = time.time()\n",
    "        prompt = f\"\"\"Validate response against knowledge:\n",
    "        Knowledge: {str(knowledge_base)[:1000]}\n",
    "        Response: {response}\n",
    "        List inaccuracies:\"\"\"\n",
    "        validation = self.generate_response(prompt)\n",
    "        return validation.text, time.time() - start_time\n",
    "\n",
    "class EmotionSummarizerAgent(BaseAgent):\n",
    "    \"\"\"Combined emotion analysis and summarization\"\"\"\n",
    "    def analyze_and_summarize(self, text: str) -> tuple:\n",
    "        start_time = time.time()\n",
    "        emotion_prompt = f\"\"\"Analyze emotional tone (1-5):\n",
    "        Text: {text}\n",
    "        Analysis:\"\"\"\n",
    "        emotion_analysis = self.generate_response(emotion_prompt)\n",
    "        \n",
    "        summary_prompt = f\"\"\"Summarize under 100 words:\n",
    "        {text}\n",
    "        Summary:\"\"\"\n",
    "        summary = self.generate_response(summary_prompt)\n",
    "        return emotion_analysis.text, summary.text, time.time() - start_time\n",
    "\n",
    "# Cell 4: System Implementation\n",
    "class EnhancedMultiAgentSystem:\n",
    "    \"\"\"Coordinated multi-agent system with quality control\"\"\"\n",
    "    def __init__(self, model_path: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\",\n",
    "                 pickle_file: str = \"responses.pkl\"):\n",
    "        self.pickle_file = pickle_file\n",
    "        self.responses: List[Response] = self._load_responses()\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        self.client = OllamaClient(model_path)\n",
    "        self.knowledge_base = {}\n",
    "        self.metrics = SystemMetrics()\n",
    "        \n",
    "        # Initialize agents\n",
    "        self.generator = GeneratorAgent(self.client)\n",
    "        self.critic = CriticAgent(self.client)\n",
    "        self.refiner = RefinerAgent(self.client)\n",
    "        self.context_manager = ContextManagerAgent(self.client)\n",
    "        self.validator = ValidatorAgent(self.client)\n",
    "        self.emotion_summarizer = EmotionSummarizerAgent(self.client)\n",
    "\n",
    "    def _load_responses(self) -> List[Response]:\n",
    "        \"\"\"Load responses from storage\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.pickle_file):\n",
    "                with open(self.pickle_file, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                    return data if isinstance(data, list) else []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading responses: {e}\")\n",
    "        return []\n",
    "\n",
    "    @lru_cache(maxsize=1000)\n",
    "    def find_similar_insights(self, query: str, top_k: int = 3) -> str:\n",
    "        \"\"\"Find similar historical responses\"\"\"\n",
    "        if not self.responses:\n",
    "            return \"\"\n",
    "            \n",
    "        query_words = set(query.lower().split())\n",
    "        scores = []\n",
    "        \n",
    "        for response in self.responses:\n",
    "            if response.error:\n",
    "                continue\n",
    "            response_words = set(response.text.lower().split())\n",
    "            union = query_words | response_words\n",
    "            similarity = len(query_words & response_words)/len(union) if union else 0\n",
    "            scores.append((response.text, similarity))\n",
    "            \n",
    "        return \"\\n\".join([f\"- {t[:200]}...\" for t,_ in sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]])\n",
    "\n",
    "    def execute_quality_pipeline(self, prompt: str) -> Response:\n",
    "        \"\"\"Full processing pipeline\"\"\"\n",
    "        if not prompt.strip():\n",
    "            return Response(text=\"Error: Empty input\", timestamp=time.time(), error=True)\n",
    "\n",
    "        # Context enhancement\n",
    "        base_context = self.find_similar_insights(prompt)\n",
    "        enhanced_context, ctx_time = self.context_manager.enhance_context(prompt, base_context)\n",
    "        self.metrics.update('ContextManager', time=ctx_time)\n",
    "        \n",
    "        # Generation\n",
    "        gen_response = self.generator.generate(prompt, enhanced_context)\n",
    "        self.metrics.update('Generator', length=len(gen_response.text))\n",
    "        if gen_response.error:\n",
    "            return gen_response\n",
    "        \n",
    "        # Critique & Refinement\n",
    "        critique = self.critic.critique(prompt, gen_response.text)\n",
    "        self.metrics.update('Critic', issues=len(critique.text.split('\\n')))\n",
    "        refined_response = self.refiner.refine(prompt, gen_response.text, critique.text)\n",
    "        self.metrics.update('Refiner', iterations=1)\n",
    "        \n",
    "        # Validation\n",
    "        validation, val_time = self.validator.validate(refined_response.text, self.knowledge_base)\n",
    "        self.metrics.update('Validator', time=val_time, issues=validation.count('inaccuracy'))\n",
    "        \n",
    "        # Secondary refinement\n",
    "        if \"serious inaccuracies\" in validation.lower():\n",
    "            refined_response = self.refiner.refine(prompt, refined_response.text, validation)\n",
    "            self.metrics.update('Refiner', iterations=1)\n",
    "        \n",
    "        # Emotion & Summary\n",
    "        emotion, summary, es_time = self.emotion_summarizer.analyze_and_summarize(refined_response.text)\n",
    "        self.metrics.update('EmotionAnalyzer', score=self._parse_emotion(emotion))\n",
    "        self.metrics.update('Summarizer', compression=len(summary)/len(refined_response.text))\n",
    "        \n",
    "        # Update knowledge base\n",
    "        self._update_knowledge(prompt, refined_response.text)\n",
    "        \n",
    "        return Response(\n",
    "            text=summary,\n",
    "            timestamp=time.time(),\n",
    "            processing_time=gen_response.processing_time + \n",
    "                          critique.processing_time + \n",
    "                          refined_response.processing_time +\n",
    "                          ctx_time + val_time + es_time,\n",
    "            critique=critique.text,\n",
    "            refinement=refined_response.text,\n",
    "            iterations=1\n",
    "        )\n",
    "\n",
    "    def _parse_emotion(self, text: str) -> float:\n",
    "        \"\"\"Extract emotion score from analysis\"\"\"\n",
    "        try:\n",
    "            return float(text.split(':')[-1].strip())\n",
    "        except:\n",
    "            return 3.0  # Default neutral\n",
    "\n",
    "    def _update_knowledge(self, query: str, response: str):\n",
    "        \"\"\"Update knowledge base\"\"\"\n",
    "        key = hash(query[:50])\n",
    "        self.knowledge_base[key] = {\n",
    "            'query': query,\n",
    "            'response': response,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "\n",
    "    def _save_response(self, response: Response):\n",
    "        \"\"\"Save validated responses\"\"\"\n",
    "        try:\n",
    "            self.responses.append(response)\n",
    "            with open(self.pickle_file, \"wb\") as f:\n",
    "                pickle.dump(self.responses, f)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving response: {e}\")\n",
    "\n",
    "class SystemMetrics:\n",
    "    \"\"\"Performance tracking system\"\"\"\n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(lambda: {'count': 0, 'total': 0.0})\n",
    "        self.quality = {\n",
    "            'emotion_scores': [],\n",
    "            'compression_rates': [],\n",
    "            'validation_issues': 0\n",
    "        }\n",
    "        \n",
    "    def update(self, agent: str, **kwargs):\n",
    "        self.metrics[agent]['count'] += 1\n",
    "        for key, value in kwargs.items():\n",
    "            if key in ['time', 'length']:\n",
    "                self.metrics[agent]['total'] += value\n",
    "            elif key == 'score':\n",
    "                self.quality['emotion_scores'].append(value)\n",
    "            elif key == 'compression':\n",
    "                self.quality['compression_rates'].append(value)\n",
    "            elif key == 'issues':\n",
    "                self.quality['validation_issues'] += value\n",
    "\n",
    "    def report(self):\n",
    "        \"\"\"Generate metrics report\"\"\"\n",
    "        print(\"=== System Metrics ===\")\n",
    "        for agent, data in self.metrics.items():\n",
    "            avg = data['total']/data['count'] if data['count'] else 0\n",
    "            print(f\"{agent}: {data['count']} calls, Avg: {avg:.2f}\")\n",
    "        \n",
    "        print(\"\\nQuality Metrics:\")\n",
    "        print(f\"Avg Emotion: {np.mean(self.quality['emotion_scores']) if self.quality['emotion_scores'] else 0:.1f}\")\n",
    "        print(f\"Avg Compression: {np.mean(self.quality['compression_rates']) if self.quality['compression_rates'] else 0:.1%}\")\n",
    "        print(f\"Total Issues Found: {self.quality['validation_issues']}\")\n",
    "\n",
    "# Cell 5: Testing Framework\n",
    "def test_pipeline():\n",
    "    \"\"\"Comprehensive test suite\"\"\"\n",
    "    system = EnhancedMultiAgentSystem()\n",
    "    tests = [\n",
    "        (\"Explain quantum computing basics\", \"normal\"),\n",
    "        (\"\", \"empty input\"),\n",
    "        (\"What's the capital of France?\", \"factual\"),\n",
    "        (\"I'm worried about AI safety\", \"emotional\")\n",
    "    ]\n",
    "\n",
    "    for query, test_type in tests:\n",
    "        print(f\"\\n=== Testing {test_type} ===\")\n",
    "        start = time.time()\n",
    "        try:\n",
    "            response = system.execute_quality_pipeline(query)\n",
    "            \n",
    "            assert isinstance(response, Response), \"Invalid response type\"\n",
    "            if test_type == \"empty input\":\n",
    "                assert response.error, \"Should handle empty input\"\n",
    "            else:\n",
    "                assert len(response.text) > 20, \"Response too short\"\n",
    "                assert response.processing_time < 15, \"Processing too slow\"\n",
    "            \n",
    "            print(f\"Passed in {time.time()-start:.1f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"Test failed: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nFinal Metrics Report:\")\n",
    "    system.metrics.report()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
