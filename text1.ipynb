{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning retraing parameters \n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Union\n",
    "import yaml\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "import psutil\n",
    "import json\n",
    "\n",
    "class LLMPipeline:\n",
    "    def __init__(self, config_path: str):\n",
    "        \"\"\"Initialize the LLM pipeline with configuration.\"\"\"\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.setup_logging()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def _load_config(self, config_path: str) -> Dict:\n",
    "        \"\"\"Load configuration from YAML file.\"\"\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            return yaml.safe_load(f)\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging with rotation and formatting.\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=self.config.get('logging_level', 'INFO'),\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('pipeline.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def preprocess_data(self, texts: List[str]) -> Dataset:\n",
    "        \"\"\"Preprocess and tokenize input texts.\"\"\"\n",
    "        try:\n",
    "            # Initialize tokenizer if not already done\n",
    "            if self.tokenizer is None:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    self.config['model_name'],\n",
    "                    use_fast=True\n",
    "                )\n",
    "\n",
    "            # Basic text cleaning\n",
    "            cleaned_texts = [\n",
    "                text.strip().replace('\\n', ' ').replace('\\r', ' ')\n",
    "                for text in texts\n",
    "            ]\n",
    "\n",
    "            # Create dataset\n",
    "            dataset = Dataset.from_dict({'text': cleaned_texts})\n",
    "\n",
    "            # Tokenization function\n",
    "            def tokenize_function(examples):\n",
    "                return self.tokenizer(\n",
    "                    examples['text'],\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=self.config['max_length']\n",
    "                )\n",
    "\n",
    "            # Apply tokenization\n",
    "            tokenized_dataset = dataset.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                num_proc=4,\n",
    "                remove_columns=['text']\n",
    "            )\n",
    "\n",
    "            return tokenized_dataset\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in preprocessing: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def setup_model(self):\n",
    "        \"\"\"Initialize model with PEFT configuration.\"\"\"\n",
    "        try:\n",
    "            # Load base model\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config['model_name'],\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto'\n",
    "            )\n",
    "\n",
    "            # Configure LoRA\n",
    "            lora_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                r=8,  # LoRA attention dimension\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"q_proj\", \"v_proj\"]\n",
    "            )\n",
    "\n",
    "            # Apply LoRA\n",
    "            self.model = get_peft_model(base_model, lora_config)\n",
    "            self.model.to(self.device)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in model setup: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train(self, dataset: Dataset):\n",
    "        \"\"\"Train the model using PEFT.\"\"\"\n",
    "        try:\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=\"./results\",\n",
    "                per_device_train_batch_size=4,\n",
    "                gradient_accumulation_steps=4,\n",
    "                learning_rate=2e-4,\n",
    "                num_train_epochs=3,\n",
    "                fp16=True,\n",
    "                logging_steps=100,\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=200,\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=200,\n",
    "                save_total_limit=3,\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset,\n",
    "                data_collator=DataCollatorForLanguageModeling(\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    mlm=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "            trainer.train()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during training: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def inference(self, text: str) -> str:\n",
    "        \"\"\"Perform inference with resource monitoring.\"\"\"\n",
    "        try:\n",
    "            # Monitor resource usage\n",
    "            cpu_percent = psutil.cpu_percent()\n",
    "            memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "            # Log resource usage\n",
    "            self.logger.info(f\"CPU Usage: {cpu_percent}%, Memory: {memory:.2f}MB\")\n",
    "\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.config['max_length']\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Generate with automatic mixed precision\n",
    "            with autocast():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_length=self.config['max_length'],\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during inference: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save the model and configuration.\"\"\"\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            save_path = Path(path)\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Save model\n",
    "            self.model.save_pretrained(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "\n",
    "            # Save configuration\n",
    "            with open(save_path / 'pipeline_config.json', 'w') as f:\n",
    "                json.dump(self.config, f)\n",
    "\n",
    "            self.logger.info(f\"Model saved successfully to {path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"Load a saved model and configuration.\"\"\"\n",
    "        try:\n",
    "            load_path = Path(path)\n",
    "            \n",
    "            # Load configuration\n",
    "            with open(load_path / 'pipeline_config.json', 'r') as f:\n",
    "                self.config = json.load(f)\n",
    "\n",
    "            # Load tokenizer and model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                load_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto'\n",
    "            )\n",
    "\n",
    "            self.logger.info(f\"Model loaded successfully from {path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading model: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(74097) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.21 seconds\n",
      "AI Response: \n"
     ]
    }
   ],
   "source": [
    "# didnt really work\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_local_ollama_model(model_name, prompt, max_tokens=100, timeout=60):\n",
    "    \"\"\"\n",
    "    Runs a local Ollama model with controlled response length.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Model name (e.g., \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\")\n",
    "        prompt (str): Input prompt for the model\n",
    "        max_tokens (int): Maximum number of tokens in response (default: 100)\n",
    "        timeout (int): Maximum time to wait (seconds)\n",
    "\n",
    "    Returns:\n",
    "        str: Model's output or error message\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Add concise instruction to the prompt\n",
    "        constrained_prompt = f\"{prompt} Please answer concisely and keep responses under {max_tokens} tokens.\"\n",
    "        \n",
    "        process = subprocess.run(\n",
    "            [\"ollama\", \"run\", model_name, \"--num-predict\", str(max_tokens)],\n",
    "            input=constrained_prompt,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Error: Response timed out.\"\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Execution time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return process.stdout.strip()\n",
    "\n",
    "# Example usage with length constraints\n",
    "model = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\"\n",
    "prompt_text = \"Explain the benefits of running models locally.\"\n",
    "\n",
    "# Get response with max 50 tokens\n",
    "response = run_local_ollama_model(model, prompt_text, max_tokens=50)\n",
    "print(\"AI Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(74294) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run the Ollama model and send the prompt\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m process \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mollama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuihui_ai/llama3.2-abliterate:1b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Print the response\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, process\u001b[38;5;241m.\u001b[39mstdout)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/subprocess.py:2115\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2109\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2110\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2115\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use llama \n",
    "import subprocess\n",
    "\n",
    "# Define the prompt for the AI\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Run the Ollama model and send the prompt\n",
    "process = subprocess.run(\n",
    "    [\"ollama\", \"run\", \"huihui_ai/llama3.2-abliterate:1b\"],\n",
    "    input=prompt,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(\"AI Response:\", process.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 33.51 seconds\n",
      "AI Response: Running models locally has several key benefits compared to running them in the cloud or on a remote server:\n",
      "\n",
      "1. Faster development and testing: Local machine setup allows you to quickly run your model on a real dataset, without waiting for resources to be provisioned. You can iterate and experiment faster by making changes to code/hyperparameters and re-running your model immediately. This enables more frequent development cycles.\n",
      "\n",
      "2. More control over environment: Running models locally gives you full control over the machine setup - OS, hardware, software versions, etc. You can fine-tune the environment to match requirements of your specific model. Cloud setups often have a default generic environment that may not always match your needs. \n",
      "\n",
      "3. Reduced latency for interactive use: With local deployment, there is no need to wait for cloud resources to be provisioned and booted up before running models interactively on them. You can quickly spin up an instance of the model in memory if needed, rather than waiting until a batch job finishes. This makes it useful for exploratory analysis where you want interactive performance without long delays.\n",
      "\n",
      "4. Better handling of large datasets: Running on local machine allows loading and processing larger datasets that may be too big to fit into cloud storage or RAM. You can load the whole dataset upfront into memory which is not possible with distributed cloud setup. Local machines also allow using more computing resources per instance, like multiple GPUs or CPUs, for parallel training/inference.\n",
      "\n",
      "5. Easier debugging: Debugging involves reproducing issues in a controlled environment and local setup allows you to do that without worrying about network latency and other remote variables. You can quickly modify code, re-run the model and analyze any changes with confidence it will have no side effects on cloud resources.\n",
      "\n",
      "Of course, running models locally also has some drawbacks - higher upfront hardware costs, longer boot times for new instances, potential data privacy/security concerns if your local environment shares a network boundary with others. There is often an optimal balance of benefits and tradeoffs between cloud vs local setups depending on the use case. But when used judiciously, running models locally can boost development speed and performance by empowering you to quickly test ideas in familiar environments without waiting for remote resources to be spun up.\n"
     ]
    }
   ],
   "source": [
    "# works no pickle\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_local_ollama_model(model_name, prompt, timeout=60):\n",
    "    \"\"\"\n",
    "    Runs a local Ollama model with a given prompt and returns its response.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The full name of the model (e.g., \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\").\n",
    "        prompt (str): The question or input prompt for the model.\n",
    "        timeout (int): Maximum time (in seconds) to wait for the response.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's output or an error message if it times out.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Execute the Ollama model command locally\n",
    "        process = subprocess.run(\n",
    "            [\"ollama\", \"run\", model_name],\n",
    "            input=prompt,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Error: The model took too long to respond. Please try again.\"\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Execution time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return process.stdout.strip()\n",
    "\n",
    "# --- Template Usage Example ---\n",
    "model = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\"\n",
    "prompt_text = \"Explain the benefits of running models locally.\"\n",
    "\n",
    "# Call the function and capture the response\n",
    "response = run_local_ollama_model(model, prompt_text)\n",
    "\n",
    "# Print the model's response\n",
    "print(\"AI Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works creates pickle\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_local_ollama_model(model_name, prompt, max_tokens=100, timeout=60):\n",
    "    \"\"\"\n",
    "    Runs a local Ollama model with controlled response length.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Model name (e.g., \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\")\n",
    "        prompt (str): Input prompt for the model\n",
    "        max_tokens (int): Maximum number of tokens in response (default: 100)\n",
    "        timeout (int): Maximum time to wait (seconds)\n",
    "\n",
    "    Returns:\n",
    "        str: Model's output or error message\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Add concise instruction to the prompt\n",
    "        constrained_prompt = f\"{prompt} Please answer concisely and keep responses under {max_tokens} tokens.\"\n",
    "        \n",
    "        process = subprocess.run(\n",
    "            [\"ollama\", \"run\", model_name, \"--num-predict\", str(max_tokens)],\n",
    "            input=constrained_prompt,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Error: Response timed out.\"\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Execution time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return process.stdout.strip()\n",
    "\n",
    "# Example usage with length constraints\n",
    "model = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\"\n",
    "prompt_text = \"Explain the benefits of running models locally.\"\n",
    "\n",
    "# Get response with max 50 tokens\n",
    "response = run_local_ollama_model(model, prompt_text, max_tokens=50)\n",
    "print(\"AI Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your prompts to interact with the model. Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(75524) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 58.97 seconds\n",
      "AI Response:\n",
      " Here is an outline with 3 main points and 4 deep sub-sections for each main point:\n",
      "\n",
      "Main Point 1: The Importance of Sleep  \n",
      "1) Sleep is essential for physical health. Lack of sleep can lead to obesity, diabetes, high blood pressure, heart disease and stroke.  \n",
      "2) Sleep helps the body repair and recover from the day's activities. Sufficient sleep allows tissues to regenerate and immune system to reset.\n",
      "3) Quality sleep promotes cognitive function. Proper rest supports memory formation and prevents mental decline with age.\n",
      "\n",
      "Deep Sub-Point 1a: The Stages of Sleep\n",
      "i) Non-rapid eye movement (NREM) sleep - Deep, regenerative stage that lasts about 20% of total sleep time. Body relaxes as breathing slows down.  \n",
      "ii) Rapid eye movement (REM) sleep - Vivid, dream-filled stage where the brain is more active than in waking state. Muscles are relaxed.\n",
      "iii) Alpha and theta waves - Hypnotic brainwaves associated with deep relaxation during NREM sleep. Key to calming effect of naps. \n",
      "iv) Delta waves - Dominant form of brain waves during deepest stages of NREM sleep, when we feel most vulnerable yet protected.\n",
      "\n",
      "Deep Sub-Point 1b: Sleep Apnea & Snoring\n",
      "i) Obstructive sleep apnea (OSA) is the most common type and occurs when throat muscles relax too much, causing obstruction to air flow as you breathe. Can lead to memory loss and high blood pressure.  \n",
      "ii) Loud snoring is often an indicator but not always diagnostic of OSA. Snoring can also be caused by alcohol use or cold medicine which relaxes throat muscles.\n",
      "iii) Treatments include continuous positive airway pressure (CPAP), lifestyle changes, oral devices like a mouthpiece that keeps airways open. Surgery may be needed for severe cases.  \n",
      "iv) Over 18 million Americans snore with the most common cause being tonsils in children. Treating underlying issues resolves many causes of snoring and sleep apnea.\n",
      "\n",
      "Main Point 2: The Impact of Sleep on Mental Health  \n",
      "1) Lack of sleep has been linked to increased risk for depression, anxiety disorders, bipolar disorder, and schizophrenia.  \n",
      "2) Poor quality or quantity of sleep can cause symptoms like irritability, poor mood, fatigue, forgetfulness, difficulty concentrating, and sadness which may resemble clinical depression. \n",
      "3) Sleep deprivation negatively impacts brain chemistry by interfering with the synthesis of serotonin (mood stabilizer) and dopamine (reward/motivation chemical). Chronic insomnia is especially harmful to mental health.\n",
      "\n",
      "Deep Sub-Point 2a: Seasonal Affective Disorder (SAD)\n",
      "i) SAD occurs in late fall or winter when daylight hours are reduced, leading to lower melatonin production which triggers a depressive episode.  \n",
      "ii) Symptoms include decreased energy and appetite, difficulty concentrating, feelings of hopelessness and withdrawal from social activities.  \n",
      "iii) Treatment involves light therapy, antidepressants, anti-anxiety medications and counseling to address negative thoughts. Lifestyle changes like regular exercise also help.\n",
      "\n",
      "Deep Sub-Point 2b: PTSD & Sleep Disorders\n",
      "i) PTSD sufferers often have trouble falling asleep or staying asleep due to constant mental replay of traumatic events during NREM sleep stages.  \n",
      "ii) Flashbacks and nightmares are very common which can trigger high levels of cortisol, adrenaline and other stress hormones that disrupt normal sleep patterns. \n",
      "iii) Medications like SSRIs help with the core PTSD symptoms while CBT therapy provides strategies for managing disturbing memories in a non-destructive way. Sleep hygiene practices also important.\n",
      "\n",
      "Main Point 3: The Importance of Restorative Naps  \n",
      "1) A nap allows you to \"catch up\" on lost sleep, reset your circadian rhythms and restore energy levels without losing an entire day's worth of work or rest time.  \n",
      "2) The ideal length is between 20-60 minutes for the restorative effects, but beware that oversleeping can cause groggy drowsiness rather than refreshed alertness.  \n",
      "3) Naps help prevent sleep inertia which impairs cognitive performance after wakefulness by resetting your brain's internal clock and preventing memory loss of recent events during sleep.\n",
      "\n",
      "Deep Sub-Point 3a: The Power Nap Myth\n",
      "i) Popular wisdom says napping is a waste of time, that it doesn't add up to full nights rest. However, naps are often the best way for busy people to get much needed recovery hours without losing days off work or precious free time.  \n",
      "ii) Research shows naps can be extremely beneficial in restoring cognitive function and energy levels if taken within a few hours of waking - around 20 minutes is optimal based on the power nap theory. Longer napping becomes less effective, as does oversleeping into drowsiness/drowsy sleep stages.\n",
      "\n",
      "Deep Sub-Point 3b: The Science of Nap Time\n",
      "i) Taking a quick power nap at midday can reduce fatigue and improve alertness by resetting internal clocks to align with circadian rhythm.  \n",
      "ii) Naps activate parasympathetic nervous system, slowing heart rate and breathing while promoting relaxation responses in brain stem areas associated with sleep and wakefulness regulation. \n",
      "iii) If you often fall asleep during the day from mental or physical fatigue, naps can help keep your mind focused on a task without losing important cognitive information due to interrupted sleep.\n",
      "\n",
      "In conclusion, quality restorative sleep is vital for overall health and well-being. Adequate amounts of high-quality sleep are essential as regular light sleep and NREM stages regenerate the body while REM dreaming allows consolidation of learning and memory formation. Restorative naps allow us to \"catch up\" on lost sleep without losing valuable work or leisure time, especially when short on total hours available each night. Mastering healthy sleep habits is key for maintaining optimal performance in both waking life and dreamland - our brains will thank you!\n",
      "\n",
      "I hope this detailed outline helps demonstrate the reasoning behind my answer while keeping it concise to match your request. Let me know if you have any other questions!\n",
      "Response appended to responses.pkl.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(76399) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 20.14 seconds\n",
      "AI Response:\n",
      " To find the nth Fibonacci number, we can use a recursive formula with base cases of 0 and 1. The recursive formula is:\n",
      "\n",
      "Fn = Fn-1 + Fn-2 for n > 2 \n",
      "       = 0 if n=0\n",
      "       = 1 if n=1\n",
      "\n",
      "Proof: Consider the closed form expression for the nth Fibonacci number as derived by Lucas (1890):\n",
      "\n",
      "Fn = Fib(n) = (φ^n - (1-φ)^n)/(φ-1), where φ = (1 + √5)/2 is the golden ratio. \n",
      "\n",
      "Now, using the Binomial Theorem expanded to 3rd degree:\n",
      "Fib(n) = Σ i=0 to n ((C(n,i)) * Fib(i-1) * (-1)^i) / i!  where C(n,i) = (n choose i) is the binomial coefficient.\n",
      "\n",
      "For large n (>10), this recursive formula matches the closed form expression almost exactly up to machine precision loss:\n",
      "| Fn - Fib(n) | < 5e-3 for all n > 10\n",
      "\n",
      "The proof shows these formulas generate the same results but through different logical processes, one a simple recurrence relation and the other an extended Binomial expansion. Both converge to Fibonacci numbers as expected.\n",
      "Response appended to responses.pkl.\n"
     ]
    }
   ],
   "source": [
    "# takes 2 mins initially\n",
    "import subprocess\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def run_local_ollama_model(model_name, prompt, timeout=60):\n",
    "    \"\"\"\n",
    "    Runs a local Ollama model with a given prompt and returns its response.\n",
    "    \n",
    "    The prompt is expected to include any chain-of-thought instructions.\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): The full name of the model (e.g., \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\").\n",
    "        prompt (str): The complete prompt for the model.\n",
    "        timeout (int): Maximum time (in seconds) to wait for the response.\n",
    "    \n",
    "    Returns:\n",
    "        str: The model's output or an error message if it times out.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        process = subprocess.run(\n",
    "            [\"ollama\", \"run\", model_name],\n",
    "            input=prompt,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Error: The model took too long to respond. Please try again.\"\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Execution time: {elapsed_time:.2f} seconds\")\n",
    "    return process.stdout.strip()\n",
    "\n",
    "def append_response_to_pickle(new_response, filename=\"responses.pkl\"):\n",
    "    \"\"\"\n",
    "    Appends a new response to a pickle file. Creates a new file if it doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        new_response (str): The model's response to store.\n",
    "        filename (str): The pickle file name.\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            responses = pickle.load(f)\n",
    "    else:\n",
    "        responses = []\n",
    "    \n",
    "    responses.append(new_response)\n",
    "    \n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(responses, f)\n",
    "    print(f\"Response appended to {filename}.\")\n",
    "\n",
    "# --- Interactive Template Usage Example ---\n",
    "model = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\"\n",
    "\n",
    "print(\"Enter your prompts to interact with the model. Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    user_prompt = input(\"Your prompt: \")\n",
    "    if user_prompt.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting interactive loop.\")\n",
    "        break\n",
    "    \n",
    "    # Construct a prompt that adds chain-of-thought instructions and instructs the model\n",
    "    # to produce a concise final answer (approximately half the length of a typical output).\n",
    "    augmented_prompt = (\n",
    "        \"Please provide a detailed chain-of-thought explanation for your reasoning, \"\n",
    "        \"then give your final answer. However, ensure that the final answer is concise—roughly half the length of a typical response.\\n\\n\"\n",
    "        f\"Question: {user_prompt}\"\n",
    "    )\n",
    "    \n",
    "    # Run the model with the augmented prompt\n",
    "    response = run_local_ollama_model(model, augmented_prompt)\n",
    "    print(\"AI Response:\\n\", response)\n",
    "    \n",
    "    # Append the response to the pickle file\n",
    "    append_response_to_pickle(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.10 seconds\n",
      "AI Response: \n",
      "Execution time: 0.02 seconds\n",
      "\n",
      "Second Response: \n",
      "\n",
      "=== Interaction History ===\n",
      "History created at: 2025-02-16T21:11:31.131597\n",
      "\n",
      "Interaction 1 (2025-02-16T21:11:31.002505):\n",
      "Model: hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\n",
      "Prompt: Explain the benefits of running models locally.\n",
      "Response: \n",
      "Execution time: 0.13s\n",
      "\n",
      "Interaction 2 (2025-02-16T21:11:31.133212):\n",
      "Model: hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\n",
      "Prompt: What is transfer learning?\n",
      "Response: \n",
      "Execution time: 0.01s\n",
      "\n",
      "Interaction 3 (2025-02-16T21:12:05.029014):\n",
      "Model: hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\n",
      "Prompt: Explain the benefits of running models locally.\n",
      "Response: \n",
      "Execution time: 0.1s\n",
      "\n",
      "Interaction 4 (2025-02-16T21:12:05.131621):\n",
      "Model: hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\n",
      "Prompt: What is transfer learning?\n",
      "Response: \n",
      "Execution time: 0.02s\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "HISTORY_FILE = \"ollama_history.pkl\"\n",
    "\n",
    "def save_to_pickle(response_data):\n",
    "    \"\"\"Save response data to pickle file with timestamp\"\"\"\n",
    "    try:\n",
    "        # Try to load existing history\n",
    "        with open(HISTORY_FILE, \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "    except (FileNotFoundError, EOFError):\n",
    "        # Create new history with creation timestamp\n",
    "        history = {\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'interactions': []\n",
    "        }\n",
    "    \n",
    "    # Append new interaction\n",
    "    history['interactions'].append(response_data)\n",
    "    \n",
    "    # Save updated history\n",
    "    with open(HISTORY_FILE, \"wb\") as f:\n",
    "        pickle.dump(history, f)\n",
    "\n",
    "def run_local_ollama_model(model_name, prompt, max_tokens=100, timeout=60):\n",
    "    \"\"\"\n",
    "    Runs a local Ollama model and saves responses to timestamped pickle file.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    response_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model': model_name,\n",
    "        'prompt': prompt,\n",
    "        'response': '',\n",
    "        'execution_time': 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        constrained_prompt = f\"{prompt} Please answer concisely and keep responses under {max_tokens} tokens.\"\n",
    "        \n",
    "        process = subprocess.run(\n",
    "            [\"ollama\", \"run\", model_name, \"--num-predict\", str(max_tokens)],\n",
    "            input=constrained_prompt,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        \n",
    "        response = process.stdout.strip()\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Update response data\n",
    "        response_data.update({\n",
    "            'response': response,\n",
    "            'execution_time': round(elapsed_time, 2)\n",
    "        })\n",
    "        \n",
    "        # Save to pickle\n",
    "        save_to_pickle(response_data)\n",
    "        \n",
    "        print(f\"Execution time: {elapsed_time:.2f} seconds\")\n",
    "        return response\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        response_data['response'] = \"Error: Response timed out.\"\n",
    "        save_to_pickle(response_data)\n",
    "        return response_data['response']\n",
    "\n",
    "def load_history():\n",
    "    \"\"\"Load and print interaction history\"\"\"\n",
    "    try:\n",
    "        with open(HISTORY_FILE, \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "            print(f\"History created at: {history['created_at']}\")\n",
    "            for idx, interaction in enumerate(history['interactions'], 1):\n",
    "                print(f\"\\nInteraction {idx} ({interaction['timestamp']}):\")\n",
    "                print(f\"Model: {interaction['model']}\")\n",
    "                print(f\"Prompt: {interaction['prompt']}\")\n",
    "                print(f\"Response: {interaction['response']}\")\n",
    "                print(f\"Execution time: {interaction['execution_time']}s\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No history found\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\"\n",
    "    prompt_text = \"Explain the benefits of running models locally.\"\n",
    "    \n",
    "    # Get response\n",
    "    response = run_local_ollama_model(model, prompt_text, max_tokens=50)\n",
    "    print(\"AI Response:\", response)\n",
    "    \n",
    "    # Get another response\n",
    "    response2 = run_local_ollama_model(model, \"What is transfer learning?\", max_tokens=30)\n",
    "    print(\"\\nSecond Response:\", response2)\n",
    "    \n",
    "    # View history\n",
    "    print(\"\\n=== Interaction History ===\")\n",
    "    load_history()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
