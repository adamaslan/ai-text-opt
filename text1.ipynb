{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Union\n",
    "import yaml\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "import psutil\n",
    "import json\n",
    "\n",
    "class LLMPipeline:\n",
    "    def __init__(self, config_path: str):\n",
    "        \"\"\"Initialize the LLM pipeline with configuration.\"\"\"\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.setup_logging()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def _load_config(self, config_path: str) -> Dict:\n",
    "        \"\"\"Load configuration from YAML file.\"\"\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            return yaml.safe_load(f)\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging with rotation and formatting.\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=self.config.get('logging_level', 'INFO'),\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('pipeline.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def preprocess_data(self, texts: List[str]) -> Dataset:\n",
    "        \"\"\"Preprocess and tokenize input texts.\"\"\"\n",
    "        try:\n",
    "            # Initialize tokenizer if not already done\n",
    "            if self.tokenizer is None:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    self.config['model_name'],\n",
    "                    use_fast=True\n",
    "                )\n",
    "\n",
    "            # Basic text cleaning\n",
    "            cleaned_texts = [\n",
    "                text.strip().replace('\\n', ' ').replace('\\r', ' ')\n",
    "                for text in texts\n",
    "            ]\n",
    "\n",
    "            # Create dataset\n",
    "            dataset = Dataset.from_dict({'text': cleaned_texts})\n",
    "\n",
    "            # Tokenization function\n",
    "            def tokenize_function(examples):\n",
    "                return self.tokenizer(\n",
    "                    examples['text'],\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=self.config['max_length']\n",
    "                )\n",
    "\n",
    "            # Apply tokenization\n",
    "            tokenized_dataset = dataset.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                num_proc=4,\n",
    "                remove_columns=['text']\n",
    "            )\n",
    "\n",
    "            return tokenized_dataset\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in preprocessing: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def setup_model(self):\n",
    "        \"\"\"Initialize model with PEFT configuration.\"\"\"\n",
    "        try:\n",
    "            # Load base model\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config['model_name'],\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto'\n",
    "            )\n",
    "\n",
    "            # Configure LoRA\n",
    "            lora_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                r=8,  # LoRA attention dimension\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"q_proj\", \"v_proj\"]\n",
    "            )\n",
    "\n",
    "            # Apply LoRA\n",
    "            self.model = get_peft_model(base_model, lora_config)\n",
    "            self.model.to(self.device)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in model setup: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train(self, dataset: Dataset):\n",
    "        \"\"\"Train the model using PEFT.\"\"\"\n",
    "        try:\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=\"./results\",\n",
    "                per_device_train_batch_size=4,\n",
    "                gradient_accumulation_steps=4,\n",
    "                learning_rate=2e-4,\n",
    "                num_train_epochs=3,\n",
    "                fp16=True,\n",
    "                logging_steps=100,\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=200,\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=200,\n",
    "                save_total_limit=3,\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset,\n",
    "                data_collator=DataCollatorForLanguageModeling(\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    mlm=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "            trainer.train()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during training: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def inference(self, text: str) -> str:\n",
    "        \"\"\"Perform inference with resource monitoring.\"\"\"\n",
    "        try:\n",
    "            # Monitor resource usage\n",
    "            cpu_percent = psutil.cpu_percent()\n",
    "            memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "            # Log resource usage\n",
    "            self.logger.info(f\"CPU Usage: {cpu_percent}%, Memory: {memory:.2f}MB\")\n",
    "\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.config['max_length']\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Generate with automatic mixed precision\n",
    "            with autocast():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_length=self.config['max_length'],\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during inference: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save the model and configuration.\"\"\"\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            save_path = Path(path)\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Save model\n",
    "            self.model.save_pretrained(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "\n",
    "            # Save configuration\n",
    "            with open(save_path / 'pipeline_config.json', 'w') as f:\n",
    "                json.dump(self.config, f)\n",
    "\n",
    "            self.logger.info(f\"Model saved successfully to {path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"Load a saved model and configuration.\"\"\"\n",
    "        try:\n",
    "            load_path = Path(path)\n",
    "            \n",
    "            # Load configuration\n",
    "            with open(load_path / 'pipeline_config.json', 'r') as f:\n",
    "                self.config = json.load(f)\n",
    "\n",
    "            # Load tokenizer and model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                load_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto'\n",
    "            )\n",
    "\n",
    "            self.logger.info(f\"Model loaded successfully from {path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading model: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of France?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run the Ollama model and send the prompt\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m process \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mollama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuihui_ai/llama3.2-abliterate:1b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Print the response\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, process\u001b[38;5;241m.\u001b[39mstdout)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/subprocess.py:2115\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2109\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2110\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2115\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the prompt for the AI\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# Run the Ollama model and send the prompt\n",
    "process = subprocess.run(\n",
    "    [\"ollama\", \"run\", \"huihui_ai/llama3.2-abliterate:1b\"],\n",
    "    input=prompt,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(\"AI Response:\", process.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 33.51 seconds\n",
      "AI Response: Running models locally has several key benefits compared to running them in the cloud or on a remote server:\n",
      "\n",
      "1. Faster development and testing: Local machine setup allows you to quickly run your model on a real dataset, without waiting for resources to be provisioned. You can iterate and experiment faster by making changes to code/hyperparameters and re-running your model immediately. This enables more frequent development cycles.\n",
      "\n",
      "2. More control over environment: Running models locally gives you full control over the machine setup - OS, hardware, software versions, etc. You can fine-tune the environment to match requirements of your specific model. Cloud setups often have a default generic environment that may not always match your needs. \n",
      "\n",
      "3. Reduced latency for interactive use: With local deployment, there is no need to wait for cloud resources to be provisioned and booted up before running models interactively on them. You can quickly spin up an instance of the model in memory if needed, rather than waiting until a batch job finishes. This makes it useful for exploratory analysis where you want interactive performance without long delays.\n",
      "\n",
      "4. Better handling of large datasets: Running on local machine allows loading and processing larger datasets that may be too big to fit into cloud storage or RAM. You can load the whole dataset upfront into memory which is not possible with distributed cloud setup. Local machines also allow using more computing resources per instance, like multiple GPUs or CPUs, for parallel training/inference.\n",
      "\n",
      "5. Easier debugging: Debugging involves reproducing issues in a controlled environment and local setup allows you to do that without worrying about network latency and other remote variables. You can quickly modify code, re-run the model and analyze any changes with confidence it will have no side effects on cloud resources.\n",
      "\n",
      "Of course, running models locally also has some drawbacks - higher upfront hardware costs, longer boot times for new instances, potential data privacy/security concerns if your local environment shares a network boundary with others. There is often an optimal balance of benefits and tradeoffs between cloud vs local setups depending on the use case. But when used judiciously, running models locally can boost development speed and performance by empowering you to quickly test ideas in familiar environments without waiting for remote resources to be spun up.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_local_ollama_model(model_name, prompt, timeout=60):\n",
    "    \"\"\"\n",
    "    Runs a local Ollama model with a given prompt and returns its response.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The full name of the model (e.g., \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\").\n",
    "        prompt (str): The question or input prompt for the model.\n",
    "        timeout (int): Maximum time (in seconds) to wait for the response.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's output or an error message if it times out.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Execute the Ollama model command locally\n",
    "        process = subprocess.run(\n",
    "            [\"ollama\", \"run\", model_name],\n",
    "            input=prompt,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Error: The model took too long to respond. Please try again.\"\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Execution time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return process.stdout.strip()\n",
    "\n",
    "# --- Template Usage Example ---\n",
    "model = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\"\n",
    "prompt_text = \"Explain the benefits of running models locally.\"\n",
    "\n",
    "# Call the function and capture the response\n",
    "response = run_local_ollama_model(model, prompt_text)\n",
    "\n",
    "# Print the model's response\n",
    "print(\"AI Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
