{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Union\n",
    "import yaml\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast\n",
    "import psutil\n",
    "import json\n",
    "\n",
    "class LLMPipeline:\n",
    "    def __init__(self, config_path: str):\n",
    "        \"\"\"Initialize the LLM pipeline with configuration.\"\"\"\n",
    "        self.config = self._load_config(config_path)\n",
    "        self.setup_logging()\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "    def _load_config(self, config_path: str) -> Dict:\n",
    "        \"\"\"Load configuration from YAML file.\"\"\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            return yaml.safe_load(f)\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Configure logging with rotation and formatting.\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=self.config.get('logging_level', 'INFO'),\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('pipeline.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def preprocess_data(self, texts: List[str]) -> Dataset:\n",
    "        \"\"\"Preprocess and tokenize input texts.\"\"\"\n",
    "        try:\n",
    "            # Initialize tokenizer if not already done\n",
    "            if self.tokenizer is None:\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    self.config['model_name'],\n",
    "                    use_fast=True\n",
    "                )\n",
    "\n",
    "            # Basic text cleaning\n",
    "            cleaned_texts = [\n",
    "                text.strip().replace('\\n', ' ').replace('\\r', ' ')\n",
    "                for text in texts\n",
    "            ]\n",
    "\n",
    "            # Create dataset\n",
    "            dataset = Dataset.from_dict({'text': cleaned_texts})\n",
    "\n",
    "            # Tokenization function\n",
    "            def tokenize_function(examples):\n",
    "                return self.tokenizer(\n",
    "                    examples['text'],\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    max_length=self.config['max_length']\n",
    "                )\n",
    "\n",
    "            # Apply tokenization\n",
    "            tokenized_dataset = dataset.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                num_proc=4,\n",
    "                remove_columns=['text']\n",
    "            )\n",
    "\n",
    "            return tokenized_dataset\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in preprocessing: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def setup_model(self):\n",
    "        \"\"\"Initialize model with PEFT configuration.\"\"\"\n",
    "        try:\n",
    "            # Load base model\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config['model_name'],\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto'\n",
    "            )\n",
    "\n",
    "            # Configure LoRA\n",
    "            lora_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                r=8,  # LoRA attention dimension\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"q_proj\", \"v_proj\"]\n",
    "            )\n",
    "\n",
    "            # Apply LoRA\n",
    "            self.model = get_peft_model(base_model, lora_config)\n",
    "            self.model.to(self.device)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in model setup: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def train(self, dataset: Dataset):\n",
    "        \"\"\"Train the model using PEFT.\"\"\"\n",
    "        try:\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=\"./results\",\n",
    "                per_device_train_batch_size=4,\n",
    "                gradient_accumulation_steps=4,\n",
    "                learning_rate=2e-4,\n",
    "                num_train_epochs=3,\n",
    "                fp16=True,\n",
    "                logging_steps=100,\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=200,\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=200,\n",
    "                save_total_limit=3,\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=training_args,\n",
    "                train_dataset=dataset,\n",
    "                data_collator=DataCollatorForLanguageModeling(\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    mlm=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "            trainer.train()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during training: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def inference(self, text: str) -> str:\n",
    "        \"\"\"Perform inference with resource monitoring.\"\"\"\n",
    "        try:\n",
    "            # Monitor resource usage\n",
    "            cpu_percent = psutil.cpu_percent()\n",
    "            memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "\n",
    "            # Log resource usage\n",
    "            self.logger.info(f\"CPU Usage: {cpu_percent}%, Memory: {memory:.2f}MB\")\n",
    "\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.config['max_length']\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Generate with automatic mixed precision\n",
    "            with autocast():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_length=self.config['max_length'],\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during inference: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save the model and configuration.\"\"\"\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            save_path = Path(path)\n",
    "            save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Save model\n",
    "            self.model.save_pretrained(save_path)\n",
    "            self.tokenizer.save_pretrained(save_path)\n",
    "\n",
    "            # Save configuration\n",
    "            with open(save_path / 'pipeline_config.json', 'w') as f:\n",
    "                json.dump(self.config, f)\n",
    "\n",
    "            self.logger.info(f\"Model saved successfully to {path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"Load a saved model and configuration.\"\"\"\n",
    "        try:\n",
    "            load_path = Path(path)\n",
    "            \n",
    "            # Load configuration\n",
    "            with open(load_path / 'pipeline_config.json', 'r') as f:\n",
    "                self.config = json.load(f)\n",
    "\n",
    "            # Load tokenizer and model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(load_path)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                load_path,\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto'\n",
    "            )\n",
    "\n",
    "            self.logger.info(f\"Model loaded successfully from {path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading model: {str(e)}\")\n",
    "            raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
