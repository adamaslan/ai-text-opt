{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "/var/folders/49/6ydqkbq172ngzt6p49xfm6b00000gn/T/ipykernel_84775/2723449637.py:98: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot=gr.Chatbot(height=500),\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/gradio/chat_interface.py:321: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# works but is weird\n",
    "!pip install gradio sentence-transformers pandas numpy torch transformers --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gradio as gr\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "from sentence_transformers import util\n",
    "\n",
    "# --- Data Loading & Preprocessing ---\n",
    "df = pd.read_csv('ideas_with_embeddings.csv')\n",
    "df['Embeddings'] = df['Embeddings'].apply(\n",
    "    lambda x: np.fromstring(x.strip(\"[]\"), sep=', ')\n",
    ")\n",
    "\n",
    "# --- Text Preprocessing ---\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# --- Embedding Generation Setup ---\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_roberta_embeddings(texts, batch_size=32):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,  # Explicit truncation\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# --- Similarity Search ---\n",
    "def find_most_similar(query_embedding, top_k=3):\n",
    "    similarities = []\n",
    "    for emb in df['Embeddings']:\n",
    "        sim = util.pytorch_cos_sim(\n",
    "            torch.tensor(query_embedding).float(),\n",
    "            torch.tensor(emb).float()\n",
    "        ).item()\n",
    "        similarities.append(sim)\n",
    "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    return df.iloc[top_indices]\n",
    "\n",
    "# --- Improved Response Generation ---\n",
    "generator = pipeline(\n",
    "    'text-generation', \n",
    "    model='sshleifer/tiny-gpt2',\n",
    "    pad_token_id=50256  # Explicitly set pad token\n",
    ")\n",
    "\n",
    "def generate_response(query, history):\n",
    "    # Process query\n",
    "    processed_query = preprocess_text(query)\n",
    "    query_embedding = get_roberta_embeddings([processed_query])[0]\n",
    "    \n",
    "    # Find similar ideas\n",
    "    similar_ideas = find_most_similar(query_embedding)\n",
    "    main_response = \"\\n\".join([f\"- {idea}\" for idea in similar_ideas['Cleaned_Ideas'].values[:3]])\n",
    "    \n",
    "    # Generate additional text with safe limits\n",
    "    prompt = f\"Based on these ideas: {main_response}\\nQuestion: {query}\\nAnswer:\"\n",
    "    generated_text = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=50,  # Generate up to 50 new tokens\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=40,\n",
    "        num_return_sequences=1\n",
    "    )[0]['generated_text'].split(\"Answer:\")[-1].strip()\n",
    "    \n",
    "    return f\"{main_response}\\n\\nAdditional Insights:\\n{generated_text}\"\n",
    "\n",
    "# --- Gradio Interface ---\n",
    "demo = gr.ChatInterface(\n",
    "    generate_response,\n",
    "    chatbot=gr.Chatbot(height=500),\n",
    "    textbox=gr.Textbox(placeholder=\"Ask about innovative ideas...\", container=False, scale=7),\n",
    "    title=\"Idea Generation Assistant\",\n",
    "    examples=[\"How can we improve urban transportation?\", \n",
    "             \"Suggest sustainable packaging ideas\"],\n",
    "    theme=\"soft\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
