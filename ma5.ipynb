{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response saved to therapeutic_response.pkl\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Setup and Configuration\n",
    "\n",
    "# Cell 1: Core Imports and Response Structure\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TherapeuticResponse:\n",
    "    \"\"\"Enhanced response structure for therapeutic context\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "    error_details: str = \"\"\n",
    "    timeout: bool = False\n",
    "    empathy_score: float = 0.0\n",
    "    safety_checks: List[str] = None\n",
    "    ethical_considerations: List[str] = None\n",
    "    refinement_suggestions: List[str] = None\n",
    "    crisis_flag: bool = False\n",
    "\n",
    "# Cell 2: Ollama Client Implementation\n",
    "class OllamaClient:\n",
    "    \"\"\"Robust Ollama client with configurable timeouts\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = 5\n",
    "        self.request_timeout = 300\n",
    "        self._verify_model()\n",
    "\n",
    "    def _parse_json_safe(self, text: str):\n",
    "        \"\"\"Enhanced JSON parsing with fallback\"\"\"\n",
    "        clean_text = text.strip()\n",
    "        if not clean_text:\n",
    "            return {\"error\": \"Empty response\"}\n",
    "            \n",
    "        try:\n",
    "            return json.loads(clean_text)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                start = clean_text.find('{')\n",
    "                end = clean_text.rfind('}') + 1\n",
    "                return json.loads(clean_text[start:end])\n",
    "            except:\n",
    "                return {\"error\": f\"Invalid JSON format: {clean_text[:200]}...\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def _verify_model(self):\n",
    "        \"\"\"Model verification with status checks\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                resp = requests.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
    "                if resp.status_code == 200:\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    models = [m['name'] for m in data.get('models', [])]\n",
    "                    if any(self.model_name in m for m in models):\n",
    "                        return\n",
    "                    self._pull_model()\n",
    "                    return\n",
    "                logger.warning(f\"Model check failed (status {resp.status_code})\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Model check attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        raise ConnectionError(f\"Couldn't connect to Ollama after {self.max_retries} attempts\")\n",
    "\n",
    "    def _pull_model(self):\n",
    "        \"\"\"Model pulling with progress tracking\"\"\"\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                stream=True,\n",
    "                timeout=600\n",
    "            )\n",
    "            for line in resp.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        status = self._parse_json_safe(line).get('status', '')\n",
    "                        logger.info(f\"Pull progress: {status}\")\n",
    "                    except:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model pull failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Generation with configurable timeout and retries\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(\n",
    "                        requests.post,\n",
    "                        f\"{self.base_url}/api/generate\",\n",
    "                        json={\n",
    "                            \"model\": self.model_name,\n",
    "                            \"prompt\": prompt[:4000],\n",
    "                            \"stream\": False,\n",
    "                            \"options\": {\"temperature\": 0.5}\n",
    "                        },\n",
    "                        timeout=self.request_timeout\n",
    "                    )\n",
    "                    resp = future.result(timeout=self.request_timeout)\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    return data.get(\"response\", \"\"), False\n",
    "            except FutureTimeoutError:\n",
    "                logger.warning(f\"Generation timed out (attempt {attempt+1})\")\n",
    "                return f\"Error: Timeout after {self.request_timeout}s\", True\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(1)\n",
    "        return f\"Error: Failed after {self.max_retries} attempts\", True\n",
    "\n",
    "# Cell 3: Base Agent Framework\n",
    "class BaseAgent:\n",
    "    \"\"\"Timeout-aware base agent\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "        self.retry_count = 3\n",
    "        self.max_wait = 300\n",
    "        \n",
    "    def safe_generate(self, prompt: str) -> TherapeuticResponse:\n",
    "        \"\"\"Generation with time budget tracking\"\"\"\n",
    "        start_time = time.time()\n",
    "        timeout_occurred = False\n",
    "        \n",
    "        if not isinstance(prompt, str) or len(prompt.strip()) == 0:\n",
    "            return TherapeuticResponse(\n",
    "                text=\"Error: Invalid input prompt\",\n",
    "                timestamp=start_time,\n",
    "                error=True,\n",
    "                error_details=\"Empty or non-string prompt\",\n",
    "                processing_time=0.0\n",
    "            )\n",
    "            \n",
    "        for attempt in range(self.retry_count):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(self.client.generate, prompt)\n",
    "                    text, error = future.result(timeout=self.max_wait)\n",
    "                    \n",
    "                    return TherapeuticResponse(\n",
    "                        text=text,\n",
    "                        timestamp=start_time,\n",
    "                        error=error,\n",
    "                        processing_time=time.time() - start_time,\n",
    "                        error_details=text if error else \"\",\n",
    "                        timeout=timeout_occurred\n",
    "                    )\n",
    "            except FutureTimeoutError:\n",
    "                logger.error(f\"Generation timed out after {self.max_wait}s\")\n",
    "                timeout_occurred = True\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                logger.error(f\"Generation error: {e}\")\n",
    "                \n",
    "        return TherapeuticResponse(\n",
    "            text=f\"Final error: {error_msg}\" if 'error_msg' in locals() else \"Unknown error\",\n",
    "            timestamp=start_time,\n",
    "            error=True,\n",
    "            error_details=error_msg if 'error_msg' in locals() else \"\",\n",
    "            processing_time=time.time() - start_time,\n",
    "            timeout=timeout_occurred\n",
    "        )\n",
    "\n",
    "# Cell 4: Prompt Integration and Saving the Response to a Pickle File\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"How can I as a masochistic male generate a sexually satisfying and enjoyable sexual experience for myself with my partner?\"\n",
    "\n",
    "# Initialize the Ollama client and the base agent\n",
    "client = OllamaClient()\n",
    "agent = BaseAgent(client)\n",
    "\n",
    "# Generate the therapeutic response using the prompt\n",
    "response = agent.safe_generate(prompt)\n",
    "\n",
    "# Save the response object to a pickle file\n",
    "with open(\"therapeutic_response.pkl\", \"wb\") as f:\n",
    "    pickle.dump(response, f)\n",
    "\n",
    "print(\"Response saved to therapeutic_response.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desire analysis saved to desire_analysis.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 14:24:27,665 - WARNING - Generation timed out (attempt 1)\n",
      "2025-02-20 14:24:27,767 - ERROR - Generation timed out after 300s\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Sex-Positive Processing Pipeline\n",
    "\n",
    "# Cell 5: Desire Analysis Agent\n",
    "class IntimacyContextAnalyzer(BaseAgent):\n",
    "    \"\"\"Analyzes intimacy needs and communication patterns\"\"\"\n",
    "    def analyze_desires(self, input_text: str) -> Dict:\n",
    "        prompt = f\"\"\"Analyze intimacy context (sex-positive focus):\n",
    "        User Statement: \"{input_text[:2000]}\"\n",
    "        \n",
    "        Identify:\n",
    "        - Expressed/unexpressed desires\n",
    "        - Communication style about intimacy\n",
    "        - Emotional blocks/opportunities\n",
    "        - Potential exploration pathways\n",
    "        - Consent awareness indicators\n",
    "        \n",
    "        Output JSON with:\n",
    "        - communication_style: str\n",
    "        - expressed_desires: List[str]\n",
    "        - potential_explorations: List[str]\n",
    "        - communication_improvements: List[str]\n",
    "        - affirmation_opportunities: List[str]\"\"\"\n",
    "        \n",
    "        response = self.safe_generate(prompt)\n",
    "        return self.client._parse_json_safe(response.text)\n",
    "\n",
    "def analyze_desires_from_pickle():\n",
    "    with open(\"therapeutic_response.pkl\", \"rb\") as f:\n",
    "        response = pickle.load(f)\n",
    "    \n",
    "    client = OllamaClient()\n",
    "    analyzer = IntimacyContextAnalyzer(client)\n",
    "    \n",
    "    analysis = analyzer.analyze_desires(response.text)\n",
    "    with open(\"desire_analysis.pkl\", \"wb\") as f:\n",
    "        pickle.dump(analysis, f)\n",
    "    print(\"Desire analysis saved to desire_analysis.pkl\")\n",
    "\n",
    "analyze_desires_from_pickle()\n",
    "\n",
    "# Cell 6: Exploration Generator\n",
    "class IntimacyActionGenerator(BaseAgent):\n",
    "    \"\"\"Generates personalized intimacy enhancement actions\"\"\"\n",
    "    def generate_actions(self, analysis: Dict) -> List[Dict]:\n",
    "        prompt = f\"\"\"Create sex-positive action plan:\n",
    "        Context: {json.dumps(analysis)[:3000]}\n",
    "        \n",
    "        Suggest 5-7 actions including:\n",
    "        - Communication exercises\n",
    "        - Phrases the partner should say\n",
    "        - Sensory exploration ideas\n",
    "        - Exciting Role-play scenarios\n",
    "        - Connection-building activities\n",
    "        \n",
    "        Format as JSON list with:\n",
    "        - action_type: str\n",
    "        - description: str\n",
    "        - purpose: str\n",
    "        - difficulty: str (beginner/intermediate/advanced)\"\"\"\n",
    "        \n",
    "        response = self.safe_generate(prompt)\n",
    "        return self.client._parse_json_safe(response.text)\n",
    "\n",
    "def generate_action_plan():\n",
    "    with open(\"desire_analysis.pkl\", \"rb\") as f:\n",
    "        analysis = pickle.load(f)\n",
    "    \n",
    "    client = OllamaClient()\n",
    "    generator = IntimacyActionGenerator(client)\n",
    "    \n",
    "    action_plan = generator.generate_actions(analysis)\n",
    "    with open(\"action_plan.pkl\", \"wb\") as f:\n",
    "        pickle.dump(action_plan, f)\n",
    "    print(\"Action plan saved to action_plan.pkl\")\n",
    "\n",
    "generate_action_plan()\n",
    "\n",
    "# Cell 7: Updated IntimacyCustomizer\n",
    "class IntimacyCustomizer(BaseAgent):\n",
    "    \"\"\"Tailors suggestions to individual preferences\"\"\"\n",
    "    def customize_actions(self, actions: List[Dict], analysis: Dict) -> Dict:\n",
    "        prompt = f\"\"\"Refine intimacy plan:\n",
    "        Initial Plan: {json.dumps(actions)[:3000]}\n",
    "        User Context: {json.dumps(analysis)[:2000]}\n",
    "        \n",
    "        Format response as JSON with:\n",
    "        {\n",
    "            \"plan_summary\": \"brief description\",\n",
    "            \"actions\": [\n",
    "                {\n",
    "                    \"action_type\": \"string\",\n",
    "                    \"description\": \"string\",\n",
    "                    \"preparation_steps\": [\"list\"],\n",
    "                    \"ideal_timing\": \"string\",\n",
    "                    \"consent_checkpoints\": [\"list\"]\n",
    "                }\n",
    "            ]\n",
    "        }\"\"\"\n",
    "        \n",
    "        response = self.safe_generate(prompt)\n",
    "        return self.client._parse_json_safe(response.text)\n",
    "def refine_action_plan():\n",
    "    with open(\"action_plan.pkl\", \"rb\") as f:\n",
    "        actions = pickle.load(f)\n",
    "    with open(\"desire_analysis.pkl\", \"rb\") as f:\n",
    "        analysis = pickle.load(f)\n",
    "    \n",
    "    client = OllamaClient()\n",
    "    customizer = IntimacyCustomizer(client)\n",
    "    \n",
    "    refined_plan = customizer.customize_actions(actions, analysis)\n",
    "    with open(\"refined_plan.pkl\", \"wb\") as f:\n",
    "        pickle.dump(refined_plan, f)\n",
    "    print(\"Refined plan saved to refined_plan.pkl\")\n",
    "\n",
    "refine_action_plan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Personalized Intimacy Plan:\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Robust Results Display\n",
    "def display_results():\n",
    "    try:\n",
    "        with open(\"refined_plan.pkl\", \"rb\") as f:\n",
    "            plan_data = pickle.load(f)\n",
    "        \n",
    "        print(\"\\nPersonalized Intimacy Plan:\")\n",
    "        \n",
    "        # Ensure we're working with a dictionary\n",
    "        if isinstance(plan_data, dict):\n",
    "            actions = plan_data.get('actions', [])\n",
    "        elif isinstance(plan_data, list):\n",
    "            actions = plan_data\n",
    "        else:\n",
    "            actions = []\n",
    "        \n",
    "        # Display first 3 actions safely\n",
    "        for idx, action in enumerate(actions[:3], 1):\n",
    "            print(f\"\\nAction {idx}:\")\n",
    "            print(f\"Type: {action.get('action_type', 'Connection Activity')}\")\n",
    "            print(f\"Purpose: {action.get('purpose', 'Enhancing intimacy through mutual exploration')}\")\n",
    "            print(f\"Steps: {', '.join(action.get('preparation_steps', ['Create comfortable environment']))}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying results: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    display_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
