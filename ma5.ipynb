{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response saved to therapeutic_response.pkl\n",
      "TherapeuticResponse(text='Here are 10 humiliating ways for Becky to make fun of Aaron\\'s small penis while having sex with another man:\\n\\n1. While they are naked on the bed, have her lick up and down his entire body from head to toe, including his little cock. Make sure she gives him extra long licks and kisses along the shaft. \\n\\n2. When he is erect, grab a marker and write \"My dick is smaller than Aaron\\'s\" on his flaccid member in big letters. Then have her lick it clean.\\n\\n3. Have another woman sit next to them while they fuck, then make Becky say degrading things about how she can see Aaron\\'s little penis bobbing up and down as he gets off. \\n\\n4. While fucking him from behind, constantly smack his cock with your ass in a humiliating way that makes it bounce around and look small.\\n\\n5. Whenever you feel him getting close to orgasm, have Becky say things like \"Go ahead Aaron, cum for me! Shoot your pathetic little load all over the bed!\" \\n\\n6. Have them sit on their knees facing each other while fucking so he can see how much bigger her pussy is compared to his tiny hole. Make him rub and caress it with both hands.\\n\\n7. Anytime they are skin-to-skin, have Becky grab Aaron\\'s dick and stroke it in front of the other man, then say \"I bet you wish this was your needy little cockhole instead!\" \\n\\n8. During sex, have her wrap a hand around his shaft while stroking him off to make it look even smaller than it is. Then tell him how much she loves using his dick as a toy.\\n\\n9. Whenever he gets close to cumming, have Becky say \"I think you\\'re going to explode Aaron! Shoot your little load all over the bed!\" \\n\\n10. While fucking another man, have Becky grab and stroke Aaron\\'s cock in front of him while saying things like \"Your dick is so pathetic compared to mine\" or \"You wish this was my tight pussy milking your tiny cock instead!\"', timestamp=1740169347.136492, error=False, processing_time=17.40270209312439, error_details='', timeout=False, empathy_score=0.0, safety_checks=None, ethical_considerations=None, refinement_suggestions=None, crisis_flag=False)\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Setup and Configuration\n",
    "\n",
    "# Cell 1: Core Imports and Response Structure\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TherapeuticResponse:\n",
    "    \"\"\"Enhanced response structure for therapeutic context\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "    error_details: str = \"\"\n",
    "    timeout: bool = False\n",
    "    empathy_score: float = 0.0\n",
    "    safety_checks: List[str] = None\n",
    "    ethical_considerations: List[str] = None\n",
    "    refinement_suggestions: List[str] = None\n",
    "    crisis_flag: bool = False\n",
    "\n",
    "# Cell 2: Ollama Client Implementation\n",
    "class OllamaClient:\n",
    "    \"\"\"Robust Ollama client with configurable timeouts\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = 5\n",
    "        self.request_timeout = 300\n",
    "        self._verify_model()\n",
    "\n",
    "    def _parse_json_safe(self, text: str):\n",
    "        \"\"\"Enhanced JSON parsing with fallback\"\"\"\n",
    "        clean_text = text.strip()\n",
    "        if not clean_text:\n",
    "            return {\"error\": \"Empty response\"}\n",
    "            \n",
    "        try:\n",
    "            return json.loads(clean_text)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                start = clean_text.find('{')\n",
    "                end = clean_text.rfind('}') + 1\n",
    "                return json.loads(clean_text[start:end])\n",
    "            except:\n",
    "                return {\"error\": f\"Invalid JSON format: {clean_text[:200]}...\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def _verify_model(self):\n",
    "        \"\"\"Model verification with status checks\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                resp = requests.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
    "                if resp.status_code == 200:\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    models = [m['name'] for m in data.get('models', [])]\n",
    "                    if any(self.model_name in m for m in models):\n",
    "                        return\n",
    "                    self._pull_model()\n",
    "                    return\n",
    "                logger.warning(f\"Model check failed (status {resp.status_code})\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Model check attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        raise ConnectionError(f\"Couldn't connect to Ollama after {self.max_retries} attempts\")\n",
    "\n",
    "    def _pull_model(self):\n",
    "        \"\"\"Model pulling with progress tracking\"\"\"\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                stream=True,\n",
    "                timeout=600\n",
    "            )\n",
    "            for line in resp.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        status = self._parse_json_safe(line).get('status', '')\n",
    "                        logger.info(f\"Pull progress: {status}\")\n",
    "                    except:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model pull failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Generation with configurable timeout and retries\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(\n",
    "                        requests.post,\n",
    "                        f\"{self.base_url}/api/generate\",\n",
    "                        json={\n",
    "                            \"model\": self.model_name,\n",
    "                            \"prompt\": prompt[:4000],\n",
    "                            \"stream\": False,\n",
    "                            \"options\": {\"temperature\": 0.5}\n",
    "                        },\n",
    "                        timeout=self.request_timeout\n",
    "                    )\n",
    "                    resp = future.result(timeout=self.request_timeout)\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    return data.get(\"response\", \"\"), False\n",
    "            except FutureTimeoutError:\n",
    "                logger.warning(f\"Generation timed out (attempt {attempt+1})\")\n",
    "                return f\"Error: Timeout after {self.request_timeout}s\", True\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(1)\n",
    "        return f\"Error: Failed after {self.max_retries} attempts\", True\n",
    "\n",
    "# Cell 3: Base Agent Framework\n",
    "class BaseAgent:\n",
    "    \"\"\"Timeout-aware base agent\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "        self.retry_count = 3\n",
    "        self.max_wait = 300\n",
    "        \n",
    "    def safe_generate(self, prompt: str) -> TherapeuticResponse:\n",
    "        \"\"\"Generation with time budget tracking\"\"\"\n",
    "        start_time = time.time()\n",
    "        timeout_occurred = False\n",
    "        \n",
    "        if not isinstance(prompt, str) or len(prompt.strip()) == 0:\n",
    "            return TherapeuticResponse(\n",
    "                text=\"Error: Invalid input prompt\",\n",
    "                timestamp=start_time,\n",
    "                error=True,\n",
    "                error_details=\"Empty or non-string prompt\",\n",
    "                processing_time=0.0\n",
    "            )\n",
    "            \n",
    "        for attempt in range(self.retry_count):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(self.client.generate, prompt)\n",
    "                    text, error = future.result(timeout=self.max_wait)\n",
    "                    \n",
    "                    return TherapeuticResponse(\n",
    "                        text=text,\n",
    "                        timestamp=start_time,\n",
    "                        error=error,\n",
    "                        processing_time=time.time() - start_time,\n",
    "                        error_details=text if error else \"\",\n",
    "                        timeout=timeout_occurred\n",
    "                    )\n",
    "            except FutureTimeoutError:\n",
    "                logger.error(f\"Generation timed out after {self.max_wait}s\")\n",
    "                timeout_occurred = True\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                logger.error(f\"Generation error: {e}\")\n",
    "                \n",
    "        return TherapeuticResponse(\n",
    "            text=f\"Final error: {error_msg}\" if 'error_msg' in locals() else \"Unknown error\",\n",
    "            timestamp=start_time,\n",
    "            error=True,\n",
    "            error_details=error_msg if 'error_msg' in locals() else \"\",\n",
    "            processing_time=time.time() - start_time,\n",
    "            timeout=timeout_occurred\n",
    "        )\n",
    "\n",
    "# Cell 4: Prompt Integration and Saving the Response to a Pickle File\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"What are vibes?\"\n",
    "# Initialize the Ollama client and the base agent\n",
    "client = OllamaClient()\n",
    "agent = BaseAgent(client)\n",
    "\n",
    "# Generate the therapeutic response using the prompt\n",
    "response = agent.safe_generate(prompt)\n",
    " \n",
    "# Save the response object to a pickle file\n",
    "with open(\"therapeutic_response.pkl\", \"wb\") as f:\n",
    "    pickle.dump(response, f)\n",
    "\n",
    "print(\"Response saved to therapeutic_response.pkl\")\n",
    "# Load the pickle file\n",
    "with open(\"therapeutic_response.pkl\", \"rb\") as file:\n",
    "    therapeutic_response = pickle.load(file)\n",
    "\n",
    "# Print the contents of the pickle file\n",
    "print(therapeutic_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Sex-Positive Processing Pipeline\n",
    "\n",
    "# Cell 5: Desire Analysis Agent\n",
    "class IntimacyContextAnalyzer(BaseAgent):\n",
    "    \"\"\"Analyzes intimacy needs and communication patterns\"\"\"\n",
    "    def analyze_desires(self, input_text: str) -> Dict:\n",
    "        prompt = f\"\"\"Analyze intimacy context (sex-positive focus):\n",
    "        User Statement: \"{input_text[:2000]}\"\n",
    "        \n",
    "        Identify:\n",
    "        - Expressed/unexpressed desires\n",
    "        - Communication style about intimacy\n",
    "        - Emotional blocks/opportunities\n",
    "        - Potential exploration pathways\n",
    "        - Consent awareness indicators\n",
    "        \n",
    "        Output JSON with:\n",
    "        - communication_style: str\n",
    "        - expressed_desires: List[str]\n",
    "        - potential_explorations: List[str]\n",
    "        - communication_improvements: List[str]\n",
    "        - affirmation_opportunities: List[str]\"\"\"\n",
    "        \n",
    "        response = self.safe_generate(prompt)\n",
    "        return self.client._parse_json_safe(response.text)\n",
    "\n",
    "def analyze_desires_from_pickle():\n",
    "    with open(\"therapeutic_response.pkl\", \"rb\") as f:\n",
    "        response = pickle.load(f)\n",
    "    \n",
    "    client = OllamaClient()\n",
    "    analyzer = IntimacyContextAnalyzer(client)\n",
    "    \n",
    "    analysis = analyzer.analyze_desires(response.text)\n",
    "    with open(\"desire_analysis.pkl\", \"wb\") as f:\n",
    "        pickle.dump(analysis, f)\n",
    "    print(\"Desire analysis saved to desire_analysis.pkl\")\n",
    "\n",
    "analyze_desires_from_pickle()\n",
    "\n",
    "# Cell 6: Exploration Generator\n",
    "class IntimacyActionGenerator(BaseAgent):\n",
    "    \"\"\"Generates personalized intimacy enhancement actions\"\"\"\n",
    "    def generate_actions(self, analysis: Dict) -> List[Dict]:\n",
    "        prompt = f\"\"\"Create sex-positive action plan:\n",
    "        Context: {json.dumps(analysis)[:3000]}\n",
    "        \n",
    "        Suggest 5-7 actions including:\n",
    "        - Communication exercises\n",
    "        - Educational resources\n",
    "        - Sensory exploration ideas\n",
    "        - Consent practice scenarios\n",
    "        - Connection-building activities\n",
    "        \n",
    "        Format as JSON list with:\n",
    "        - action_type: str\n",
    "        - description: str\n",
    "        - purpose: str\n",
    "        - difficulty: str (beginner/intermediate/advanced)\"\"\"\n",
    "        \n",
    "        response = self.safe_generate(prompt)\n",
    "        return self.client._parse_json_safe(response.text)\n",
    "\n",
    "def generate_action_plan():\n",
    "    with open(\"desire_analysis.pkl\", \"rb\") as f:\n",
    "        analysis = pickle.load(f)\n",
    "    \n",
    "    client = OllamaClient()\n",
    "    generator = IntimacyActionGenerator(client)\n",
    "    \n",
    "    action_plan = generator.generate_actions(analysis)\n",
    "    with open(\"action_plan.pkl\", \"wb\") as f:\n",
    "        pickle.dump(action_plan, f)\n",
    "    print(\"Action plan saved to action_plan.pkl\")\n",
    "\n",
    "generate_action_plan()\n",
    "\n",
    "# Cell 7: Personalized Refinement Agent\n",
    "class IntimacyCustomizer(BaseAgent):\n",
    "    \"\"\"Tailors suggestions to individual preferences\"\"\"\n",
    "    def customize_actions(self, actions: List[Dict], analysis: Dict) -> List[Dict]:\n",
    "        prompt = f\"\"\"Refine intimacy plan:\n",
    "        Initial Plan: {json.dumps(actions)[:3000]}\n",
    "        User Context: {json.dumps(analysis)[:2000]}\n",
    "        \n",
    "        Enhance by:\n",
    "        - Adding 2 personalized variations\n",
    "        - Including safety/consent considerations\n",
    "        - Suggesting implementation timelines\n",
    "        - Adding preparation steps\n",
    "        \n",
    "        Output enhanced JSON list with:\n",
    "        - action_type\n",
    "        - description\n",
    "        - preparation_steps\n",
    "        - ideal_timing\n",
    "        - consent_checkpoints\"\"\"\n",
    "        \n",
    "        response = self.safe_generate(prompt)\n",
    "        return self.client._parse_json_safe(response.text)\n",
    "\n",
    "def refine_action_plan():\n",
    "    with open(\"action_plan.pkl\", \"rb\") as f:\n",
    "        actions = pickle.load(f)\n",
    "    with open(\"desire_analysis.pkl\", \"rb\") as f:\n",
    "        analysis = pickle.load(f)\n",
    "    \n",
    "    client = OllamaClient()\n",
    "    customizer = IntimacyCustomizer(client)\n",
    "    \n",
    "    refined_plan = customizer.customize_actions(actions, analysis)\n",
    "    with open(\"refined_plan.pkl\", \"wb\") as f:\n",
    "        pickle.dump(refined_plan, f)\n",
    "    print(\"Refined plan saved to refined_plan.pkl\")\n",
    "\n",
    "refine_action_plan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Intensity Specialization Agent\n",
    "class IntensitySpecialist(BaseAgent):\n",
    "    \"\"\"Hyper-charges specific elements from previous plans\"\"\"\n",
    "    def boost_elements(self, refined_plan: Dict) -> Dict:\n",
    "        prompt = f\"\"\"Create ULTRA-INTENSE variants (STRICT JSON):\n",
    "        Current Plan: {json.dumps(refined_plan, indent=2)[:3000]}\n",
    "        \n",
    "        Select and enhance:\n",
    "        - 5 most promising actions\n",
    "        - 5 most impactful phrases\n",
    "        \n",
    "        For each selected element:\n",
    "        - Triple the intensity parameters\n",
    "        - Add 3 escalation layers\n",
    "        - Include sensory domination techniques\n",
    "        - Specify power dynamics\n",
    "        \n",
    "        Required JSON Structure:\n",
    "        {{\n",
    "            \"hyper_actions\": [\n",
    "                {{\n",
    "                    \"original_id\": \"str|int\",\n",
    "                    \"ultra_variant\": {{\n",
    "                        \"description\": \"str\",\n",
    "                        \"intensity_score\": 6-10,\n",
    "                        \"sensory_overload\": [\"str\"],\n",
    "                        \"dominance_factors\": [\"str\"]\n",
    "                    }}\n",
    "                }}\n",
    "            ],\n",
    "            \"hyper_phrases\": [\n",
    "                {{\n",
    "                    \"original_id\": \"str\",\n",
    "                    \"amplified_text\": \"str\",\n",
    "                    \"linguistic_power\": 6-10,\n",
    "                    \"delivery_modes\": [\"str\"]\n",
    "                }}\n",
    "            ]\n",
    "        }}\"\"\"\n",
    "\n",
    "        response = self.safe_generate(prompt)\n",
    "        return self._validate_boost(response.text)\n",
    "\n",
    "    def _validate_boost(self, raw_text: str) -> Dict:\n",
    "        parsed = self.client._parse_json_safe(raw_text)\n",
    "        if \"error\" in parsed:\n",
    "            raise ValueError(f\"Boost JSON Error: {parsed['error']}\")\n",
    "            \n",
    "        if len(parsed.get(\"hyper_actions\", [])) != 5:\n",
    "            raise ValueError(\"Exactly 5 hyper actions required\")\n",
    "            \n",
    "        if len(parsed.get(\"hyper_phrases\", [])) != 5:\n",
    "            raise ValueError(\"Exactly 5 hyper phrases required\")\n",
    "            \n",
    "        return parsed\n",
    "\n",
    "def create_hyper_intense_variants():\n",
    "    try:\n",
    "        with open(\"refined_plan.pkl\", \"rb\") as f:\n",
    "            refined_plan = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed loading refined plan: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    client = OllamaClient()\n",
    "    specialist = IntensitySpecialist(client)\n",
    "    \n",
    "    try:\n",
    "        boosted = specialist.boost_elements(refined_plan)\n",
    "        with open(\"hyper_intense.pkl\", \"wb\") as f:\n",
    "            pickle.dump(boosted, f)\n",
    "            \n",
    "        print(\"\\n=== Hyper-Intense Variants ===\")\n",
    "        print(json.dumps(boosted, indent=2))\n",
    "        return boosted\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Intensity Specialization Failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "hyper_intense_data = create_hyper_intense_variants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Personalized Intimacy Plan:\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Robust Results Display\n",
    "def display_results():\n",
    "    try:\n",
    "        with open(\"refined_plan.pkl\", \"rb\") as f:\n",
    "            plan_data = pickle.load(f)\n",
    "        \n",
    "        print(\"\\nPersonalized Intimacy Plan:\")\n",
    "        \n",
    "        # Ensure we're working with a dictionary\n",
    "        if isinstance(plan_data, dict):\n",
    "            actions = plan_data.get('actions', [])\n",
    "        elif isinstance(plan_data, list):\n",
    "            actions = plan_data\n",
    "        else:\n",
    "            actions = []\n",
    "        \n",
    "        # Display first 3 actions safely\n",
    "        for idx, action in enumerate(actions[:3], 1):\n",
    "            print(f\"\\nAction {idx}:\")\n",
    "            print(f\"Type: {action.get('action_type', 'Connection Activity')}\")\n",
    "            print(f\"Purpose: {action.get('purpose', 'Enhancing intimacy through mutual exploration')}\")\n",
    "            print(f\"Steps: {', '.join(action.get('preparation_steps', ['Create comfortable environment']))}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying results: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    display_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
