{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 150 entries\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acecfcebd12d40ba95e91ea24f537dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   4%|4         | 231M/5.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install required packages\n",
    "# pip install langchain huggingface_hub pandas faiss-cpu numpy transformers torch accelerate bitsandbytes gradio\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "def load_embeddings(csv_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        if 'Cleaned_Ideas' not in df.columns or 'Embeddings' not in df.columns:\n",
    "            raise ValueError(\"CSV must contain 'Cleaned_Ideas' and 'Embeddings' columns\")\n",
    "            \n",
    "        df['Embeddings'] = df['Embeddings'].apply(\n",
    "            lambda x: np.fromstring(\n",
    "                x.strip(\"[]\").replace(\"\\n\", \"\"),\n",
    "                sep=\", \",\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        expected_dim = 768\n",
    "        valid_embeddings = df['Embeddings'].apply(lambda x: len(x) == expected_dim)\n",
    "        if not valid_embeddings.all():\n",
    "            invalid_count = len(df) - valid_embeddings.sum()\n",
    "            raise ValueError(f\"{invalid_count} entries have invalid embedding dimensions\")\n",
    "            \n",
    "        return df['Cleaned_Ideas'].tolist(), np.array(df['Embeddings'].tolist())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embeddings: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# 1. Load texts and embeddings from CSV\n",
    "texts, embeddings = load_embeddings(\"ideas_with_embeddings.csv\")\n",
    "\n",
    "# 2. Improved embedding model and FAISS configuration\n",
    "try:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    vector_store = FAISS.from_embeddings(\n",
    "        text_embeddings=list(zip(texts, embeddings)),\n",
    "        embedding=embedding_model,\n",
    "        normalize_L2=True\n",
    "    )\n",
    "    print(f\"FAISS index created with {vector_store.index.ntotal} entries\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Vector store creation failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# 3. Using an openly available model (GPT-Neo 1.3B)\n",
    "model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"GPT-Neo 1.3B model loaded successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Model loading failed: {str(e)}\")\n",
    "    print(\"You may need to accept the model license at: https://huggingface.co/EleutherAI/gpt-neo-1.3B\")\n",
    "    raise\n",
    "\n",
    "# Text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=250,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    do_sample=True,\n",
    "    return_full_text=False,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 4. Enhanced prompt template\n",
    "template = \"\"\"<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Analyze the philosophical concept using ONLY the provided context. Follow these steps:\n",
    "1. Identify key elements from the context\n",
    "2. Explain relationships between concepts\n",
    "3. Highlight any mentioned philosophical implications\n",
    "\n",
    "If context is insufficient, state: \"This requires deeper analysis beyond current knowledge.\" \n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template_format=\"f-string\"\n",
    ")\n",
    "\n",
    "# 5. Optimized retrieval configuration\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\n",
    "            \"k\": 6,\n",
    "            \"score_threshold\": 0.55,\n",
    "            \"include_metadata\": True\n",
    "        }\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": prompt,\n",
    "        \"document_prompt\": PromptTemplate(\n",
    "            input_variables=[\"page_content\"],\n",
    "            template=\"{page_content}\"\n",
    "        )\n",
    "    },\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 6. Query processing function\n",
    "def process_query(query):\n",
    "    if not query.strip():\n",
    "        return \"Please enter a valid question\", \"\"\n",
    "    \n",
    "    try:\n",
    "        result = qa_chain({\"query\": query})\n",
    "        response = result['result'].split(\"Answer:\")[-1].strip()\n",
    "        \n",
    "        sources_info = \"\\n\\n**Sources:**\\n\"\n",
    "        for i, doc in enumerate(result['source_documents'][:3], 1):\n",
    "            excerpt = doc.page_content[:200].replace(\"\\n\", \" \") + \"...\"\n",
    "            score = doc.metadata.get('score', 0)\n",
    "            sources_info += f\"{i}. {excerpt} (Relevance: {score:.2f})\\n\\n\"\n",
    "            \n",
    "        return response, sources_info\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing request: {str(e)}\", \"\"\n",
    "\n",
    "# 7. Gradio interface\n",
    "def create_gradio_interface():\n",
    "    with gr.Blocks(title=\"Philosophical Concepts RAG v2\") as demo:\n",
    "        gr.Markdown(\"# Enhanced Philosophical RAG System\")\n",
    "        gr.Markdown(\"Now with improved context understanding and source verification\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=4):\n",
    "                query_input = gr.Textbox(\n",
    "                    label=\"Your Question\",\n",
    "                    placeholder=\"Ask about philosophical concepts...\",\n",
    "                    lines=3\n",
    "                )\n",
    "                submit_btn = gr.Button(\"Analyze\", variant=\"primary\")\n",
    "            \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                response_output = gr.Markdown(label=\"Analysis\")\n",
    "            with gr.Column(scale=2):\n",
    "                sources_output = gr.Markdown(label=\"Supporting Context\")\n",
    "                \n",
    "        submit_btn.click(\n",
    "            fn=process_query,\n",
    "            inputs=[query_input],\n",
    "            outputs=[response_output, sources_output]\n",
    "        )\n",
    "        \n",
    "        gr.Markdown(\"## Example Queries\")\n",
    "        examples = gr.Examples(\n",
    "            examples=[\n",
    "                [\"Explain the relationship between consciousness and reality using the provided context\"],\n",
    "                [\"How does the concept of 'being' differ across philosophical traditions?\"],\n",
    "                [\"Analyze the arguments for and against epistemological dualism\"]\n",
    "            ],\n",
    "            inputs=query_input\n",
    "        )\n",
    "        \n",
    "    return demo\n",
    "\n",
    "# 8. Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
