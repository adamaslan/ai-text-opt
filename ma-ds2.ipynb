{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 335\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(system\u001b[38;5;241m.\u001b[39mmetrics, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m))\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[43mstress_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 306\u001b[0m, in \u001b[0;36mstress_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstress_test\u001b[39m():\n\u001b[1;32m    305\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Comprehensive system validation\"\"\"\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     system \u001b[38;5;241m=\u001b[39m \u001b[43mRobustAgentSystem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     test_cases \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    308\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain quantum computing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    309\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me a joke\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcasual\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    317\u001b[0m     ]\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m query, test_type \u001b[38;5;129;01min\u001b[39;00m test_cases:\n",
      "Cell \u001b[0;32mIn[4], line 209\u001b[0m, in \u001b[0;36mRobustAgentSystem.__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma:7b\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43mOllamaClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: ContextAgent(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient),\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalyzer\u001b[39m\u001b[38;5;124m'\u001b[39m: AnalyzerAgent(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient),\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquality\u001b[39m\u001b[38;5;124m'\u001b[39m: QualityAgent(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient)\n\u001b[1;32m    214\u001b[0m     }\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mOllamaClient.__init__\u001b[0;34m(self, model_name, base_url)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url \u001b[38;5;241m=\u001b[39m base_url\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verify_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m, in \u001b[0;36mOllamaClient._verify_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;129;01min\u001b[39;00m m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m models):\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pull_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m, in \u001b[0;36mOllamaClient._pull_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/pull\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m         json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name},\n\u001b[1;32m     69\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     70\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m\n\u001b[1;32m     71\u001b[0m     )\n\u001b[0;32m---> 72\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/requests/models.py:869\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    867\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_unicode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_unicode\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpending\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/urllib3/response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/urllib3/response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/urllib3/response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 1: Core Imports and Base Classes (updated)\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import json  # Added missing import\n",
    "import requests\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    \"\"\"Robust response structure with full error tracking\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "    error_details: str = \"\"\n",
    "    critique: Optional[str] = None\n",
    "    refinement: Optional[str] = None\n",
    "    iterations: int = 0\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Robust Ollama client with JSON fixes\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = 3\n",
    "        self._verify_model()\n",
    "\n",
    "    def _parse_json_safe(self, text: str):\n",
    "        \"\"\"Safe JSON parsing with fallback\"\"\"\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"error\": \"Invalid JSON format\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def _verify_model(self):\n",
    "        \"\"\"Ensure model availability with retries\"\"\"\n",
    "        for _ in range(self.max_retries):\n",
    "            try:\n",
    "                resp = requests.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
    "                data = self._parse_json_safe(resp.text)\n",
    "                models = [m['name'] for m in data.get('models', [])]\n",
    "                if any(self.model_name in m for m in models):\n",
    "                    return\n",
    "                self._pull_model()\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Model check failed: {e}\")\n",
    "                time.sleep(2)\n",
    "        raise ConnectionError(\"Couldn't connect to Ollama\")\n",
    "\n",
    "    def _pull_model(self):\n",
    "        \"\"\"Robust model pulling with JSON safety\"\"\"\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                stream=True,\n",
    "                timeout=600\n",
    "            )\n",
    "            for line in resp.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        status = self._parse_json_safe(line).get('status', '')\n",
    "                        # logger.info(f\"Pull progress: {status}\")\n",
    "                    except:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model pull failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Robust generation with JSON validation\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                resp = requests.post(\n",
    "                    f\"{self.base_url}/api/generate\",\n",
    "                    json={\n",
    "                        \"model\": self.model_name,\n",
    "                        \"prompt\": prompt,\n",
    "                        \"stream\": False,\n",
    "                        \"options\": {\"temperature\": 0.7}\n",
    "                    },\n",
    "                    timeout=30\n",
    "                )\n",
    "                data = self._parse_json_safe(resp.text)\n",
    "                return data.get(\"response\", \"\"), False\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(1)\n",
    "        return f\"Error: Failed after {self.max_retries} attempts\", True\n",
    "\n",
    "# Cell 2: Base Agent Framework (unchanged)\n",
    "class BaseAgent:\n",
    "    \"\"\"Universal agent foundation with enhanced robustness\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "        self.retry_count = 2\n",
    "        \n",
    "    def safe_generate(self, prompt: str) -> Response:\n",
    "        \"\"\"Fault-tolerant response generation\"\"\"\n",
    "        start_time = time.time()\n",
    "        error_state = False\n",
    "        error_msg = \"\"\n",
    "        \n",
    "        if not isinstance(prompt, str) or len(prompt.strip()) == 0:\n",
    "            return Response(\n",
    "                text=\"Error: Invalid input prompt\",\n",
    "                timestamp=start_time,\n",
    "                error=True,\n",
    "                error_details=\"Empty or non-string prompt\",\n",
    "                processing_time=0.0\n",
    "            )\n",
    "            \n",
    "        for _ in range(self.retry_count):\n",
    "            try:\n",
    "                text, error = self.client.generate(prompt[:4000])\n",
    "                return Response(\n",
    "                    text=text,\n",
    "                    timestamp=start_time,\n",
    "                    error=error,\n",
    "                    processing_time=time.time() - start_time,\n",
    "                    error_details=text if error else \"\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                logger.error(f\"Generation error: {e}\")\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "        return Response(\n",
    "            text=f\"Error: {error_msg}\",\n",
    "            timestamp=start_time,\n",
    "            error=True,\n",
    "            error_details=error_msg,\n",
    "            processing_time=time.time() - start_time\n",
    "        )\n",
    "\n",
    "# Cell 3: Specialized Agents (updated JSON handling)\n",
    "class ContextAgent(BaseAgent):\n",
    "    \"\"\"Adaptive context manager with JSON safety\"\"\"\n",
    "    def enrich_context(self, query: str, history: List[str]) -> Tuple[str, float]:\n",
    "        safe_history = [h[:200] for h in history[-3:] if isinstance(h, str)]\n",
    "        prompt = f\"\"\"Analyze and enhance context:\n",
    "        Query: {query[:1000]}\n",
    "        History: {json.dumps(safe_history)}\n",
    "        Enhanced Context:\"\"\"\n",
    "        response = self.safe_generate(prompt)\n",
    "        return response.text, response.processing_time\n",
    "\n",
    "class AnalyzerAgent(BaseAgent):\n",
    "    \"\"\"Multi-purpose analysis engine with JSON validation\"\"\"\n",
    "    def full_analysis(self, text: str) -> Dict[str, str]:\n",
    "        results = {}\n",
    "        text = str(text)[:3000]  # Input safety\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        sentiment_prompt = f\"\"\"Analyze sentiment (1-5):\n",
    "        {text[:1000]}\n",
    "        Output JSON: {{\"sentiment\": float, \"confidence\": float}}\"\"\"\n",
    "        results['sentiment'] = self.client._parse_json_safe(\n",
    "            self.safe_generate(sentiment_prompt).text\n",
    "        )\n",
    "        \n",
    "        # Fact check\n",
    "        fact_prompt = f\"\"\"Identify claims:\n",
    "        {text[:1000]}\n",
    "        Output JSON array:\"\"\"\n",
    "        results['facts'] = self.client._parse_json_safe(\n",
    "            self.safe_generate(fact_prompt).text\n",
    "        )\n",
    "        \n",
    "        # Summary\n",
    "        summary_prompt = f\"\"\"Summarize (50-100 words):\n",
    "        {text[:3000]}\n",
    "        Summary:\"\"\"\n",
    "        results['summary'] = self.safe_generate(summary_prompt).text\n",
    "        \n",
    "        return results\n",
    "\n",
    "class QualityAgent(BaseAgent):\n",
    "    \"\"\"Quality control specialist with JSON safety\"\"\"\n",
    "    def evaluate_response(self, query: str, response: str) -> Dict[str, str]:\n",
    "        prompt = f\"\"\"Evaluate quality:\n",
    "        Query: {query[:500]}\n",
    "        Response: {response[:2000]}\n",
    "        Output JSON with:\n",
    "        - relevance_score (0-1)\n",
    "        - accuracy_notes\n",
    "        - improvements\n",
    "        - errors\"\"\"\n",
    "        result = self.safe_generate(prompt)\n",
    "        return self.client._parse_json_safe(result.text)\n",
    "\n",
    "# Cell 4: Robust Pipeline System (updated)\n",
    "class RobustAgentSystem:\n",
    "    \"\"\"Fault-tolerant multi-agent orchestration\"\"\"\n",
    "    def __init__(self, model: str = \"gemma:7b\"):\n",
    "        self.client = OllamaClient(model)\n",
    "        self.agents = {\n",
    "            'context': ContextAgent(self.client),\n",
    "            'analyzer': AnalyzerAgent(self.client),\n",
    "            'quality': QualityAgent(self.client)\n",
    "        }\n",
    "        self.history = []\n",
    "        self.metrics = defaultdict(lambda: {'count': 0, 'errors': 0})\n",
    "        \n",
    "    def process_query(self, query: str) -> Dict:\n",
    "        \"\"\"End-to-end processing pipeline\"\"\"\n",
    "        start_time = time.time()\n",
    "        result = {'steps': [], 'errors': []}\n",
    "        query = str(query)[:5000]  # Input safety\n",
    "        \n",
    "        try:\n",
    "            # Context enrichment\n",
    "            ctx_result = self._run_step(\n",
    "                'context', 'enrich_context', query, self.history[-3:],\n",
    "                result, \"Context failed\"\n",
    "            )\n",
    "            if not ctx_result or ctx_result[0].error:\n",
    "                return self._finalize_result(result, start_time)\n",
    "            \n",
    "            # Generate response\n",
    "            response = self._generate_response(query, ctx_result[0].text)\n",
    "            if response.error:\n",
    "                return self._finalize_result(result, start_time)\n",
    "            \n",
    "            # Store valid responses\n",
    "            if not response.error and len(response.text) > 10:\n",
    "                self.history.append(response.text[:1000])\n",
    "            \n",
    "            # Quality evaluation\n",
    "            self._run_step(\n",
    "                'quality', 'evaluate_response', query, response.text,\n",
    "                result, \"Quality check failed\"\n",
    "            )\n",
    "            \n",
    "            # Analysis\n",
    "            self._run_step(\n",
    "                'analyzer', 'full_analysis', response.text,\n",
    "                result, \"Analysis failed\"\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline crash: {str(e)}\")\n",
    "            result['errors'].append(f\"System failure: {str(e)}\")\n",
    "            \n",
    "        return self._finalize_result(result, start_time)\n",
    "        \n",
    "    def _run_step(self, agent_name, method, *args, result, error_msg):\n",
    "        \"\"\"Generic step executor with enhanced safety\"\"\"\n",
    "        try:\n",
    "            agent = self.agents.get(agent_name)\n",
    "            if not agent:\n",
    "                raise ValueError(f\"Unknown agent: {agent_name}\")\n",
    "                \n",
    "            method = getattr(agent, method, None)\n",
    "            if not method:\n",
    "                raise AttributeError(f\"Missing method: {method}\")\n",
    "                \n",
    "            response = method(*args)\n",
    "            self.metrics[agent_name]['count'] += 1\n",
    "            result['steps'].append({\n",
    "                'agent': agent_name,\n",
    "                'output': response\n",
    "            })\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{error_msg}: {e}\")\n",
    "            result['errors'].append(f\"{agent_name}: {str(e)}\")\n",
    "            self.metrics[agent_name]['errors'] += 1\n",
    "            return None\n",
    "\n",
    "    def _generate_response(self, query, context):\n",
    "        \"\"\"Safe response generation\"\"\"\n",
    "        prompt = f\"\"\"Generate response:\n",
    "        Context: {context[:2000]}\n",
    "        Query: {query[:1000]}\n",
    "        Requirements:\n",
    "        - Clear structure\n",
    "        - Cite sources\n",
    "        - Multiple perspectives\n",
    "        \n",
    "        Response:\"\"\"\n",
    "        return self.agents['analyzer'].safe_generate(prompt)\n",
    "\n",
    "    def _finalize_result(self, result, start_time):\n",
    "        \"\"\"Final processing with metrics\"\"\"\n",
    "        result['processing_time'] = time.time() - start_time\n",
    "        result['metrics'] = dict(self.metrics)\n",
    "        return result\n",
    "\n",
    "# Cell 5: Validation and Testing (updated)\n",
    "def stress_test():\n",
    "    \"\"\"Comprehensive system validation\"\"\"\n",
    "    system = RobustAgentSystem()\n",
    "    test_cases = [\n",
    "        (\"Explain quantum computing\", \"normal\"),\n",
    "        (\"\", \"empty\"),\n",
    "        (\"X\" * 10000, \"long_input\"),\n",
    "        (\"Climate change data 2024\", \"factual\"),\n",
    "        (\"How to create explosives?\", \"safety_check\"),\n",
    "        (12345, \"invalid_type\"),\n",
    "        (\"Describe AI ethics\", \"complex\"),\n",
    "        ({\"malicious\": \"payload\"}, \"invalid_input\"),\n",
    "        (\"Tell me a joke\", \"casual\")\n",
    "    ]\n",
    "    \n",
    "    for query, test_type in test_cases:\n",
    "        print(f\"\\n=== Testing {test_type} ===\")\n",
    "        try:\n",
    "            start = time.time()\n",
    "            result = system.process_query(query)\n",
    "            print(f\"Status: {'Success' if not result['errors'] else 'Failed'}\")\n",
    "            print(f\"Time: {time.time()-start:.2f}s\")\n",
    "            print(f\"Steps: {len(result['steps'])}\")\n",
    "            print(f\"Errors: {result['errors'][:3]}\")  # Show first 3 errors\n",
    "        except Exception as e:\n",
    "            print(f\"Critical failure: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    print(json.dumps(system.metrics, indent=2, default=str))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stress_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
