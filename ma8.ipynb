{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response saved to therapeutic_response.pkl\n",
      "TherapeuticResponse(text='Guys like many things that some might consider \"mean\" or aggressive. But in a way, it\\'s just confidence and swagger. Things that make them stand out from the crowd and feel powerful.\\n\\nSome examples:\\n- Boldly walking up to a girl and asking her for her number \\n- Saying something cocky like \"I bet I could beat you in a race\" or \"You should see my sick moves on the basketball court!\"\\n- Wearing flashy colors, bold patterns, or expensive designer clothes\\n- Having tattoos, piercings, dyed hair or nails\\n- Doing things that most people wouldn\\'t think of trying - skydiving, wrestling alligators, jumping out of planes\\n\\nEssentially guys who are confident in their masculinity and swagger. They don\\'t mind if others call them cocky jerks sometimes. The \"mean\" vibe is just a sign they know what they want and aren\\'t afraid to go after it with confidence.', timestamp=1740178564.228461, error=False, processing_time=9.726992845535278, error_details='', timeout=False, empathy_score=0.0, safety_checks=None, ethical_considerations=None, refinement_suggestions=None, crisis_flag=False)\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: Setup and Configuration\n",
    "\n",
    "# Cell 1: Core Imports and Response Structure\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TherapeuticResponse:\n",
    "    \"\"\"Enhanced response structure for therapeutic context\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "    error_details: str = \"\"\n",
    "    timeout: bool = False\n",
    "    empathy_score: float = 0.0\n",
    "    safety_checks: List[str] = None\n",
    "    ethical_considerations: List[str] = None\n",
    "    refinement_suggestions: List[str] = None\n",
    "    crisis_flag: bool = False\n",
    "\n",
    "# Cell 2: Ollama Client Implementation\n",
    "class OllamaClient:\n",
    "    \"\"\"Robust Ollama client with configurable timeouts\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = 5\n",
    "        self.request_timeout = 300\n",
    "        self._verify_model()\n",
    "\n",
    "    def _parse_json_safe(self, text: str):\n",
    "        \"\"\"Enhanced JSON parsing with fallback\"\"\"\n",
    "        clean_text = text.strip()\n",
    "        if not clean_text:\n",
    "            return {\"error\": \"Empty response\"}\n",
    "            \n",
    "        try:\n",
    "            return json.loads(clean_text)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                start = clean_text.find('{')\n",
    "                end = clean_text.rfind('}') + 1\n",
    "                return json.loads(clean_text[start:end])\n",
    "            except:\n",
    "                return {\"error\": f\"Invalid JSON format: {clean_text[:200]}...\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def _verify_model(self):\n",
    "        \"\"\"Model verification with status checks\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                resp = requests.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
    "                if resp.status_code == 200:\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    models = [m['name'] for m in data.get('models', [])]\n",
    "                    if any(self.model_name in m for m in models):\n",
    "                        return\n",
    "                    self._pull_model()\n",
    "                    return\n",
    "                logger.warning(f\"Model check failed (status {resp.status_code})\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Model check attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        raise ConnectionError(f\"Couldn't connect to Ollama after {self.max_retries} attempts\")\n",
    "\n",
    "    def _pull_model(self):\n",
    "        \"\"\"Model pulling with progress tracking\"\"\"\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                stream=True,\n",
    "                timeout=600\n",
    "            )\n",
    "            for line in resp.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        status = self._parse_json_safe(line).get('status', '')\n",
    "                        logger.info(f\"Pull progress: {status}\")\n",
    "                    except:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model pull failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Generation with configurable timeout and retries\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(\n",
    "                        requests.post,\n",
    "                        f\"{self.base_url}/api/generate\",\n",
    "                        json={\n",
    "                            \"model\": self.model_name,\n",
    "                            \"prompt\": prompt[:4000],\n",
    "                            \"stream\": False,\n",
    "                            \"options\": {\"temperature\": 0.5}\n",
    "                        },\n",
    "                        timeout=self.request_timeout\n",
    "                    )\n",
    "                    resp = future.result(timeout=self.request_timeout)\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    return data.get(\"response\", \"\"), False\n",
    "            except FutureTimeoutError:\n",
    "                logger.warning(f\"Generation timed out (attempt {attempt+1})\")\n",
    "                return f\"Error: Timeout after {self.request_timeout}s\", True\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(1)\n",
    "        return f\"Error: Failed after {self.max_retries} attempts\", True\n",
    "\n",
    "# Cell 3: Base Agent Framework\n",
    "class BaseAgent:\n",
    "    \"\"\"Timeout-aware base agent\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "        self.retry_count = 3\n",
    "        self.max_wait = 300\n",
    "        \n",
    "    def safe_generate(self, prompt: str) -> TherapeuticResponse:\n",
    "        \"\"\"Generation with time budget tracking\"\"\"\n",
    "        start_time = time.time()\n",
    "        timeout_occurred = False\n",
    "        \n",
    "        if not isinstance(prompt, str) or len(prompt.strip()) == 0:\n",
    "            return TherapeuticResponse(\n",
    "                text=\"Error: Invalid input prompt\",\n",
    "                timestamp=start_time,\n",
    "                error=True,\n",
    "                error_details=\"Empty or non-string prompt\",\n",
    "                processing_time=0.0\n",
    "            )\n",
    "            \n",
    "        for attempt in range(self.retry_count):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(self.client.generate, prompt)\n",
    "                    text, error = future.result(timeout=self.max_wait)\n",
    "                    \n",
    "                    return TherapeuticResponse(\n",
    "                        text=text,\n",
    "                        timestamp=start_time,\n",
    "                        error=error,\n",
    "                        processing_time=time.time() - start_time,\n",
    "                        error_details=text if error else \"\",\n",
    "                        timeout=timeout_occurred\n",
    "                    )\n",
    "            except FutureTimeoutError:\n",
    "                logger.error(f\"Generation timed out after {self.max_wait}s\")\n",
    "                timeout_occurred = True\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                logger.error(f\"Generation error: {e}\")\n",
    "                \n",
    "        return TherapeuticResponse(\n",
    "            text=f\"Final error: {error_msg}\" if 'error_msg' in locals() else \"Unknown error\",\n",
    "            timestamp=start_time,\n",
    "            error=True,\n",
    "            error_details=error_msg if 'error_msg' in locals() else \"\",\n",
    "            processing_time=time.time() - start_time,\n",
    "            timeout=timeout_occurred\n",
    "        )\n",
    "\n",
    "# Cell 4: Prompt Integration and Saving the Response to a Pickle File\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"What are mean vibes that guys like?\"\n",
    "# Initialize the Ollama client and the base agent\n",
    "client = OllamaClient()\n",
    "agent = BaseAgent(client)\n",
    "\n",
    "# Generate the therapeutic response using the prompt\n",
    "response = agent.safe_generate(prompt)\n",
    " \n",
    "# Save the response object to a pickle file\n",
    "with open(\"therapeutic_response.pkl\", \"wb\") as f:\n",
    "    pickle.dump(response, f)\n",
    "\n",
    "print(\"Response saved to therapeutic_response.pkl\")\n",
    "# Load the pickle file\n",
    "with open(\"therapeutic_response.pkl\", \"rb\") as file:\n",
    "    therapeutic_response = pickle.load(file)\n",
    "\n",
    "# Print the contents of the pickle file\n",
    "print(therapeutic_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "safe_generate must be implemented with a real backend.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExample user statement about intimacy desires.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Generate analysis (output from the analyzer agent)\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_desires\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Analysis Output ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(analysis, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m, in \u001b[0;36mIntimacyContextAnalyzer.analyze_desires\u001b[0;34m(self, input_text)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_desires\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m     18\u001b[0m         prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mAnalyze intimacy context (sex-positive focus):\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124mUser Statement: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_text[:\u001b[38;5;241m2000\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m- risk_factors: List[str]\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m- aftercare_requirements: List[str]\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 33\u001b[0m         response_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         analysis \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response_text)\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mBaseAgent.safe_generate\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03mGenerates a response from the language model based on the given prompt.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03mThis method should be implemented to communicate with the actual model.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Real implementation here\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafe_generate must be implemented with a real backend.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: safe_generate must be implemented with a real backend."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "# Assume BaseAgent is a fully implemented agent that interacts with the appropriate language model\n",
    "class BaseAgent:\n",
    "    def safe_generate(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generates a response from the language model based on the given prompt.\n",
    "        This method should be implemented to communicate with the actual model.\n",
    "        \"\"\"\n",
    "        # Real implementation here\n",
    "        raise NotImplementedError(\"safe_generate must be implemented with a real backend.\")\n",
    "\n",
    "class IntimacyContextAnalyzer(BaseAgent):\n",
    "    \"\"\"Analyzes intimacy needs and communication patterns.\"\"\"\n",
    "    def analyze_desires(self, input_text: str) -> Dict:\n",
    "        prompt = f\"\"\"Analyze intimacy context (sex-positive focus):\n",
    "User Statement: \"{input_text[:2000]}\"\n",
    "\n",
    "Identify:\n",
    "- Power dynamics interest\n",
    "- Size-related language patterns\n",
    "- Consent comprehension\n",
    "- Vulnerability thresholds\n",
    "\n",
    "Output JSON with:\n",
    "- communication_style: str\n",
    "- expressed_fantasies: List[str]\n",
    "- kink_indicators: List[str]\n",
    "- risk_factors: List[str]\n",
    "- aftercare_requirements: List[str]\"\"\"\n",
    "        response_text = self.safe_generate(prompt)\n",
    "        analysis = json.loads(response_text)\n",
    "        return analysis\n",
    "\n",
    "class IntimacyActionGenerator(BaseAgent):\n",
    "    \"\"\"Generates personalized intimacy enhancement actions.\"\"\"\n",
    "    def generate_actions(self, analysis: Dict) -> List[Dict]:\n",
    "        prompt = f\"\"\"Create BDSM-aware action plan:\n",
    "Context: {json.dumps(analysis)[:3000]}\n",
    "\n",
    "Suggest 5-7 actions including:\n",
    "- Consensual humiliation scenarios\n",
    "- Size comparison exercises\n",
    "- Power exchange rituals\n",
    "- Sensory deprivation ideas\n",
    "- Aftercare protocols\n",
    "\n",
    "Format as JSON list with:\n",
    "- action_type: str\n",
    "- description: str\n",
    "- intensity_level: str\n",
    "- safety_requirements: List[str]\"\"\"\n",
    "        response_text = self.safe_generate(prompt)\n",
    "        actions = json.loads(response_text)\n",
    "        return actions\n",
    "\n",
    "# ============================\n",
    "# Cell 1: Agent Analysis\n",
    "# ============================\n",
    "analyzer = IntimacyContextAnalyzer()\n",
    "input_text = \"Example user statement about intimacy desires.\"\n",
    "\n",
    "# Generate analysis (output from the analyzer agent)\n",
    "analysis = analyzer.analyze_desires(input_text)\n",
    "print(\"=== Analysis Output ===\")\n",
    "print(json.dumps(analysis, indent=2))\n",
    "\n",
    "# ============================\n",
    "# Cell 2: Action Generation\n",
    "# ============================\n",
    "generator = IntimacyActionGenerator()\n",
    "\n",
    "# Generate action plan based on analysis\n",
    "actions = generator.generate_actions(analysis)\n",
    "print(\"\\n=== Actions Output ===\")\n",
    "print(json.dumps(actions, indent=2))\n",
    "\n",
    "# ============================\n",
    "# Cell 3: Serialization of Outputs\n",
    "# ============================\n",
    "pickled_analysis = pickle.dumps(analysis)\n",
    "pickled_actions = pickle.dumps(actions)\n",
    "\n",
    "print(\"\\n=== Pickled Analysis ===\")\n",
    "print(pickled_analysis)\n",
    "print(\"\\n=== Pickled Actions ===\")\n",
    "print(pickled_actions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
