{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis Output ===\n",
      "{\n",
      "  \"error\": \"Failed to decode JSON: Expecting value: line 1 column 1 (char 0)\"\n",
      "}\n",
      "\n",
      "=== Actions Output ===\n",
      "[\n",
      "  {\n",
      "    \"error\": \"Failed to decode JSON: Extra data: line 10 column 1 (char 305)\"\n",
      "  }\n",
      "]\n",
      "\n",
      "=== Pickled Analysis ===\n",
      "b'\\x80\\x04\\x95O\\x00\\x00\\x00\\x00\\x00\\x00\\x00}\\x94\\x8c\\x05error\\x94\\x8c@Failed to decode JSON: Expecting value: line 1 column 1 (char 0)\\x94s.'\n",
      "\n",
      "=== Pickled Actions ===\n",
      "b'\\x80\\x04\\x95P\\x00\\x00\\x00\\x00\\x00\\x00\\x00]\\x94}\\x94\\x8c\\x05error\\x94\\x8c>Failed to decode JSON: Extra data: line 10 column 1 (char 305)\\x94sa.'\n",
      "\n",
      "Response saved to therapeutic_response.pkl\n",
      "\n",
      "=== Therapeutic Response ===\n",
      "TherapeuticResponse(text='Guys like a variety of things that can be considered \"mean vibes\", but some common examples include:\\n\\n- Guys who are confident and assertive \\n- Guys with good looks and style\\n- Guys who are intelligent and educated\\n- Guys who are witty, sarcastic or have a dry sense of humor\\n- Guys who are strong, athletic, muscular (not in an overtly sexual way)\\n- Guys who are down-to-earth and relatable\\n- Guys who are respectful, polite and treat women right\\n- Guys who have good taste in music/movies/etc. and can hold an interesting conversation\\n- Guys who are spontaneous and adventurous \\n\\nThe key is to be confident, authentic, and show off your unique qualities and interests. Guys like hanging around guys who \"get them\" - share their hobbies, jokes, and life experiences. Being a little mysterious adds intrigue too.\\n\\nOf course, the best vibes come from being yourself honestly and naturally. Don\\'t try to be someone you\\'re not. And always treat others with respect and kindness!', timestamp=1740182127.433919, error=False, processing_time=7.8978190422058105, error_details='', timeout=False, empathy_score=0.0, safety_checks=None, ethical_considerations=None, refinement_suggestions=None, crisis_flag=False)\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cell 1: Core Imports and Response Structure\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TherapeuticResponse:\n",
    "    \"\"\"Enhanced response structure for therapeutic context\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "    error_details: str = \"\"\n",
    "    timeout: bool = False\n",
    "    empathy_score: float = 0.0\n",
    "    safety_checks: List[str] = None\n",
    "    ethical_considerations: List[str] = None\n",
    "    refinement_suggestions: List[str] = None\n",
    "    crisis_flag: bool = False\n",
    "\n",
    "# ============================\n",
    "# Cell 2: Ollama Client Implementation\n",
    "# ============================\n",
    "class OllamaClient:\n",
    "    \"\"\"Robust Ollama client with configurable timeouts\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = 5\n",
    "        self.request_timeout = 300\n",
    "        self._verify_model()\n",
    "\n",
    "    def _parse_json_safe(self, text: str):\n",
    "        clean_text = text.strip()\n",
    "        if not clean_text:\n",
    "            return {\"error\": \"Empty response\"}\n",
    "        try:\n",
    "            return json.loads(clean_text)\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                start = clean_text.find('{')\n",
    "                end = clean_text.rfind('}') + 1\n",
    "                return json.loads(clean_text[start:end])\n",
    "            except Exception:\n",
    "                return {\"error\": f\"Invalid JSON format: {clean_text[:200]}...\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def _verify_model(self):\n",
    "        \"\"\"Verify that the model is available; pull it if necessary.\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                resp = requests.get(f\"{self.base_url}/api/tags\", timeout=10)\n",
    "                if resp.status_code == 200:\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    models = [m['name'] for m in data.get('models', [])]\n",
    "                    if any(self.model_name in m for m in models):\n",
    "                        return\n",
    "                    self._pull_model()\n",
    "                    return\n",
    "                logger.warning(f\"Model check failed (status {resp.status_code})\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Model check attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(2 ** attempt)\n",
    "        raise ConnectionError(f\"Couldn't connect to Ollama after {self.max_retries} attempts\")\n",
    "\n",
    "    def _pull_model(self):\n",
    "        \"\"\"Pull model with progress tracking.\"\"\"\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                stream=True,\n",
    "                timeout=600\n",
    "            )\n",
    "            for line in resp.iter_lines():\n",
    "                if line:\n",
    "                    try:\n",
    "                        status = self._parse_json_safe(line).get('status', '')\n",
    "                        logger.info(f\"Pull progress: {status}\")\n",
    "                    except Exception:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model pull failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Generate a response from the language model with retries.\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(\n",
    "                        requests.post,\n",
    "                        f\"{self.base_url}/api/generate\",\n",
    "                        json={\n",
    "                            \"model\": self.model_name,\n",
    "                            \"prompt\": prompt[:4000],\n",
    "                            \"stream\": False,\n",
    "                            \"options\": {\"temperature\": 0.5}\n",
    "                        },\n",
    "                        timeout=self.request_timeout\n",
    "                    )\n",
    "                    resp = future.result(timeout=self.request_timeout)\n",
    "                    data = self._parse_json_safe(resp.text)\n",
    "                    return data.get(\"response\", \"\"), False\n",
    "            except FutureTimeoutError:\n",
    "                logger.warning(f\"Generation timed out (attempt {attempt+1})\")\n",
    "                return f\"Error: Timeout after {self.request_timeout}s\", True\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n",
    "                time.sleep(1)\n",
    "        return f\"Error: Failed after {self.max_retries} attempts\", True\n",
    "\n",
    "# ============================\n",
    "# Cell 3: Base Agent Framework\n",
    "# ============================\n",
    "class BaseAgent:\n",
    "    \"\"\"Timeout-aware base agent that utilizes the Ollama client.\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "        self.retry_count = 3\n",
    "        self.max_wait = 300\n",
    "        \n",
    "    def safe_generate(self, prompt: str) -> TherapeuticResponse:\n",
    "        start_time = time.time()\n",
    "        timeout_occurred = False\n",
    "        if not isinstance(prompt, str) or len(prompt.strip()) == 0:\n",
    "            return TherapeuticResponse(\n",
    "                text=\"Error: Invalid input prompt\",\n",
    "                timestamp=start_time,\n",
    "                error=True,\n",
    "                error_details=\"Empty or non-string prompt\",\n",
    "                processing_time=0.0\n",
    "            )\n",
    "        for attempt in range(self.retry_count):\n",
    "            try:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(self.client.generate, prompt)\n",
    "                    text, error = future.result(timeout=self.max_wait)\n",
    "                    return TherapeuticResponse(\n",
    "                        text=text,\n",
    "                        timestamp=start_time,\n",
    "                        error=error,\n",
    "                        processing_time=time.time() - start_time,\n",
    "                        error_details=text if error else \"\",\n",
    "                        timeout=timeout_occurred\n",
    "                    )\n",
    "            except FutureTimeoutError:\n",
    "                logger.error(f\"Generation timed out after {self.max_wait}s\")\n",
    "                timeout_occurred = True\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                logger.error(f\"Generation error: {e}\")\n",
    "        return TherapeuticResponse(\n",
    "            text=f\"Final error: {error_msg}\" if 'error_msg' in locals() else \"Unknown error\",\n",
    "            timestamp=start_time,\n",
    "            error=True,\n",
    "            error_details=error_msg if 'error_msg' in locals() else \"\",\n",
    "            processing_time=time.time() - start_time,\n",
    "            timeout=timeout_occurred\n",
    "        )\n",
    "\n",
    "# ============================\n",
    "# Cell 4: Intimacy Agents Implementation\n",
    "# ============================\n",
    "class IntimacyContextAnalyzer(BaseAgent):\n",
    "    \"\"\"Analyzes intimacy needs and communication patterns.\"\"\"\n",
    "    def analyze_desires(self, input_text: str) -> Dict:\n",
    "        prompt = f\"\"\"Analyze intimacy context (sex-positive focus):\n",
    "User Statement: \"{input_text[:2000]}\"\n",
    "\n",
    "Identify:\n",
    "- Power dynamics interest\n",
    "- Size-related language patterns\n",
    "- Consent comprehension\n",
    "- Vulnerability thresholds\n",
    "\n",
    "Output JSON with:\n",
    "- communication_style: str\n",
    "- expressed_fantasies: List[str]\n",
    "- kink_indicators: List[str]\n",
    "- risk_factors: List[str]\n",
    "- aftercare_requirements: List[str]\"\"\"\n",
    "        response = self.safe_generate(prompt)\n",
    "        try:\n",
    "            analysis = json.loads(response.text)\n",
    "        except json.JSONDecodeError as e:\n",
    "            analysis = {\"error\": f\"Failed to decode JSON: {e}\"}\n",
    "        return analysis\n",
    "\n",
    "class IntimacyActionGenerator(BaseAgent):\n",
    "    \"\"\"Generates personalized intimacy enhancement actions.\"\"\"\n",
    "    def generate_actions(self, analysis: Dict) -> List[Dict]:\n",
    "        prompt = f\"\"\"Create BDSM-aware action plan:\n",
    "Context: {json.dumps(analysis)[:3000]}\n",
    "\n",
    "Suggest 5-7 actions including:\n",
    "- Consensual humiliation scenarios\n",
    "- Size comparison exercises\n",
    "- Power exchange rituals\n",
    "- Sensory deprivation ideas\n",
    "- Aftercare protocols\n",
    "\n",
    "Format as JSON list with:\n",
    "- action_type: str\n",
    "- description: str\n",
    "- intensity_level: str\n",
    "- safety_requirements: List[str]\"\"\"\n",
    "        response = self.safe_generate(prompt)\n",
    "        try:\n",
    "            actions = json.loads(response.text)\n",
    "        except json.JSONDecodeError as e:\n",
    "            actions = [{\"error\": f\"Failed to decode JSON: {e}\"}]\n",
    "        return actions\n",
    "\n",
    "# ============================\n",
    "# Cell 5: Agent Execution and Serialization\n",
    "# ============================\n",
    "# Initialize the Ollama client and the specialized agents\n",
    "client = OllamaClient()\n",
    "analyzer = IntimacyContextAnalyzer(client)\n",
    "generator = IntimacyActionGenerator(client)\n",
    "\n",
    "input_text = \"Example user statement about intimacy desires.\"\n",
    "\n",
    "# Generate analysis output from the IntimacyContextAnalyzer\n",
    "analysis = analyzer.analyze_desires(input_text)\n",
    "print(\"=== Analysis Output ===\")\n",
    "print(json.dumps(analysis, indent=2))\n",
    "\n",
    "# Generate actions based on the analysis using IntimacyActionGenerator\n",
    "actions = generator.generate_actions(analysis)\n",
    "print(\"\\n=== Actions Output ===\")\n",
    "print(json.dumps(actions, indent=2))\n",
    "\n",
    "# Serialize the outputs to pickle objects\n",
    "pickled_analysis = pickle.dumps(analysis)\n",
    "pickled_actions = pickle.dumps(actions)\n",
    "\n",
    "print(\"\\n=== Pickled Analysis ===\")\n",
    "print(pickled_analysis)\n",
    "print(\"\\n=== Pickled Actions ===\")\n",
    "print(pickled_actions)\n",
    "\n",
    "# ============================\n",
    "# Cell 6: Therapeutic Response Test\n",
    "# ============================\n",
    "# Using BaseAgent directly for a therapeutic prompt\n",
    "prompt = \"What are mean vibes that guys like?\"\n",
    "agent = BaseAgent(client)\n",
    "\n",
    "# Generate the therapeutic response using the prompt\n",
    "response = agent.safe_generate(prompt)\n",
    "\n",
    "# Save the response object to a pickle file\n",
    "with open(\"therapeutic_response.pkl\", \"wb\") as f:\n",
    "    pickle.dump(response, f)\n",
    "\n",
    "print(\"\\nResponse saved to therapeutic_response.pkl\")\n",
    "\n",
    "# Load and print the contents of the pickle file\n",
    "with open(\"therapeutic_response.pkl\", \"rb\") as file:\n",
    "    therapeutic_response = pickle.load(file)\n",
    "\n",
    "print(\"\\n=== Therapeutic Response ===\")\n",
    "print(therapeutic_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 19:01:25,831 - ERROR - Generation failed: 'in <string>' requires string as left operand, not ellipsis\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid format specifier ' str,\n            'description': str,\n            'safety_requirements': list\n        ' for object of type 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 214\u001b[0m\n\u001b[1;32m    211\u001b[0m analysis \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39manalyze_desires(input_text)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Generate actions\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m actions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43manalysis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Serialize results\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[2], line 192\u001b[0m, in \u001b[0;36mIntimacyActionGenerator.generate_actions\u001b[0;34m(self, analysis)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, analysis: TherapeuticResponse) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TherapeuticResponse:\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate actions from analysis\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGenerate actions from analysis:\u001b[39m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalysis\u001b[38;5;241m.\u001b[39mtext[:\u001b[38;5;241m3000\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124m    Return Python-list structure with:\u001b[39m\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;124m    [\u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250;43m        \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m str,\u001b[39;49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: str,\u001b[39;49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msafety_requirements\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m: list\u001b[39;49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124m]\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mgenerate(prompt)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid format specifier ' str,\n            'description': str,\n            'safety_requirements': list\n        ' for object of type 'str'"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cell 1: Core Imports and Response Structure (Updated)\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import ast\n",
    "import requests\n",
    "import logging\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class TherapeuticResponse:\n",
    "    \"\"\"Enhanced response structure with pickle-native serialization\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "    error_details: str = \"\"\n",
    "    timeout: bool = False\n",
    "    empathy_score: float = 0.0\n",
    "    safety_checks: List[str] = None\n",
    "    ethical_considerations: List[str] = None\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # Custom serialization with safety checks\n",
    "        state = self.__dict__.copy()\n",
    "        if self.safety_checks:\n",
    "            state['safety_checks'] = [str(check) for check in self.safety_checks]\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        # Safe deserialization with validation\n",
    "        self.__dict__.update(state)\n",
    "        self.validate_content()\n",
    "\n",
    "    def validate_content(self):\n",
    "        \"\"\"Safety validation for deserialized content\"\"\"\n",
    "        if \"unsafe content\" in self.text.lower():\n",
    "            self.safety_checks = [\"Content validation failed\"]\n",
    "            self.text = \"Content restricted for safety reasons\"\n",
    "\n",
    "# ============================\n",
    "# Cell 2: Ollama Client Implementation (Pickle Version)\n",
    "# ============================\n",
    "class OllamaClient:\n",
    "    \"\"\"Pickle-native client with enhanced safety\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", \n",
    "                 base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.max_retries = 3\n",
    "        self.request_timeout = 120\n",
    "        self.safety_filter = SafetyFilter()\n",
    "\n",
    "    def _parse_response(self, text: str):\n",
    "        \"\"\"Safe parsing of Python native structures\"\"\"\n",
    "        try:\n",
    "            # Convert string representation to Python object\n",
    "            parsed = ast.literal_eval(text.strip())\n",
    "            if isinstance(parsed, (dict, list)):\n",
    "                return self.safety_filter.validate(parsed)\n",
    "            return {\"response\": parsed}\n",
    "        except (SyntaxError, ValueError) as e:\n",
    "            logger.error(f\"Parse error: {e}\")\n",
    "            return {\"error\": f\"Invalid Python structure: {text[:200]}\"}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Validation error: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def generate(self, prompt: str) -> TherapeuticResponse:\n",
    "        \"\"\"Pickle-native generation with end-to-end safety\"\"\"\n",
    "        start_time = time.time()\n",
    "        response = TherapeuticResponse(\n",
    "            text=\"\",\n",
    "            timestamp=start_time,\n",
    "            safety_checks=[]\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            resp = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                data=pickle.dumps({\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt\": self.safety_filter.sanitize_input(prompt[:4000]),\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\"temperature\": 0.5}\n",
    "                }),\n",
    "                timeout=self.request_timeout\n",
    "            )\n",
    "            \n",
    "            if resp.status_code == 200:\n",
    "                response_data = pickle.loads(resp.content)\n",
    "                validated = self.safety_filter.validate(response_data)\n",
    "                response.text = validated.get(\"response\", \"\")\n",
    "                response.safety_checks = validated.get(\"safety_checks\", [])\n",
    "            else:\n",
    "                response.error = True\n",
    "                response.error_details = f\"API error: {resp.status_code}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            response.error = True\n",
    "            response.error_details = str(e)\n",
    "            logger.error(f\"Generation failed: {e}\")\n",
    "\n",
    "        response.processing_time = time.time() - start_time\n",
    "        return response\n",
    "\n",
    "# ============================\n",
    "# Cell 3: Safety Infrastructure\n",
    "# ============================\n",
    "class SafetyFilter:\n",
    "    \"\"\"Content safety layer for pickle operations\"\"\"\n",
    "    UNSAFE_PATTERNS = {\n",
    "        'harmful_keywords': [...],  # Add specific patterns here\n",
    "        'max_nesting': 5,\n",
    "        'max_size': 10_000_000  # 10MB\n",
    "    }\n",
    "\n",
    "    def validate(self, obj):\n",
    "        \"\"\"Validate Python objects during deserialization\"\"\"\n",
    "        self._check_size(obj)\n",
    "        return self._recursive_filter(obj)\n",
    "\n",
    "    def _recursive_filter(self, obj, depth=0):\n",
    "        if depth > self.UNSAFE_PATTERNS['max_nesting']:\n",
    "            raise ValueError(\"Structure nesting too deep\")\n",
    "            \n",
    "        if isinstance(obj, dict):\n",
    "            return {k: self._recursive_filter(v, depth+1) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._recursive_filter(item, depth+1) for item in obj]\n",
    "        elif isinstance(obj, str):\n",
    "            return self._sanitize_string(obj)\n",
    "        return obj\n",
    "\n",
    "    def _check_size(self, obj):\n",
    "        size = len(pickle.dumps(obj))\n",
    "        if size > self.UNSAFE_PATTERNS['max_size']:\n",
    "            raise ValueError(f\"Object size {size} exceeds safety limit\")\n",
    "\n",
    "    def _sanitize_string(self, text: str) -> str:\n",
    "        for pattern in self.UNSAFE_PATTERNS['harmful_keywords']:\n",
    "            if pattern in text.lower():\n",
    "                text = text.replace(pattern, \"[REDACTED]\")\n",
    "        return text\n",
    "\n",
    "    def sanitize_input(self, prompt: str) -> str:\n",
    "        \"\"\"Clean input prompts before processing\"\"\"\n",
    "        return self._sanitize_string(prompt)[:4000]\n",
    "\n",
    "# ============================\n",
    "# Cell 4: Intimacy Agents (Pickle Version)\n",
    "# ============================\n",
    "class IntimacyContextAnalyzer:\n",
    "    \"\"\"Pickle-native context analysis\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "        self.executor = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "    def analyze_desires(self, input_text: str) -> TherapeuticResponse:\n",
    "        \"\"\"Full pickle workflow analysis\"\"\"\n",
    "        prompt = f\"\"\"Analyze intimacy context and return Python-dict structure:\n",
    "        User Input: {input_text[:2000]}\n",
    "        \n",
    "        Structure response as:\n",
    "        {{\n",
    "            'communication_style': str,\n",
    "            'expressed_fantasies': list,\n",
    "            'safety_checks': list,\n",
    "            'ethical_considerations': list\n",
    "        }}\n",
    "        \"\"\"\n",
    "        return self.client.generate(prompt)\n",
    "\n",
    "class IntimacyActionGenerator:\n",
    "    \"\"\"Pickle-native action generation\"\"\"\n",
    "    def __init__(self, client: OllamaClient):\n",
    "        self.client = client\n",
    "\n",
    "    def generate_actions(self, analysis: TherapeuticResponse) -> TherapeuticResponse:\n",
    "        \"\"\"Generate actions from analysis\"\"\"\n",
    "        prompt = f\"\"\"Generate actions from analysis:\n",
    "        {analysis.text[:3000]}\n",
    "        \n",
    "        Return Python-list structure with:\n",
    "        [{\n",
    "            'action_type': str,\n",
    "            'description': str,\n",
    "            'safety_requirements': list\n",
    "        }]\n",
    "        \"\"\"\n",
    "        return self.client.generate(prompt)\n",
    "\n",
    "# ============================\n",
    "# Cell 5: Usage Example\n",
    "# ============================\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize components\n",
    "    client = OllamaClient()\n",
    "    analyzer = IntimacyContextAnalyzer(client)\n",
    "    generator = IntimacyActionGenerator(client)\n",
    "\n",
    "    # Process input\n",
    "    input_text = \"Example user statement about intimacy desires.\"\n",
    "    analysis = analyzer.analyze_desires(input_text)\n",
    "    \n",
    "    # Generate actions\n",
    "    actions = generator.generate_actions(analysis)\n",
    "\n",
    "    # Serialize results\n",
    "    with open(\"analysis.pkl\", \"wb\") as f:\n",
    "        pickle.dump(analysis, f)\n",
    "    \n",
    "    with open(\"actions.pkl\", \"wb\") as f:\n",
    "        pickle.dump(actions, f)\n",
    "\n",
    "    print(\"Processing complete. Results pickled safely.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
