{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Text Embedding Generator\n",
    "\n",
    "This script generates and combines embeddings from multiple models (Ollama API and SentenceTransformer)\n",
    "for text data, optimized for modern Python environments in 2025.\n",
    "\n",
    "Features:\n",
    "- Asynchronous and parallel processing\n",
    "- GPU acceleration\n",
    "- Robust error handling and logging\n",
    "- Configurable parameters via environment variables or config files\n",
    "- Progress tracking\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "import asyncio\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "\n",
    "import httpx\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import yaml\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"embedding_generator.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"embedding_generator\")\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingConfig:\n",
    "    \"\"\"Configuration for embedding generation process.\"\"\"\n",
    "    input_file: Path\n",
    "    output_file: Path\n",
    "    ollama_model: str = \"distilroberta\"\n",
    "    ollama_url: str = \"http://localhost:11434/api/embeddings\"\n",
    "    sentence_transformer_model: str = \"all-MiniLM-L6-v2\"\n",
    "    batch_size: int = 32\n",
    "    max_retries: int = 3\n",
    "    retry_delay: float = 1.0\n",
    "    use_gpu: bool = True\n",
    "    num_workers: int = 4\n",
    "    timeout: int = 60  # HTTP request timeout in seconds\n",
    "    cache_dir: Optional[Path] = None\n",
    "    save_interval: int = 100  # Save progress every N items\n",
    "    \n",
    "    @classmethod\n",
    "    def from_yaml(cls, config_path: Union[str, Path]) -> \"EmbeddingConfig\":\n",
    "        \"\"\"Load configuration from YAML file.\"\"\"\n",
    "        config_path = Path(config_path)\n",
    "        if not config_path.exists():\n",
    "            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "        \n",
    "        with open(config_path, \"r\") as f:\n",
    "            config_data = yaml.safe_load(f)\n",
    "        \n",
    "        # Convert string paths to Path objects\n",
    "        for path_field in [\"input_file\", \"output_file\", \"cache_dir\"]:\n",
    "            if path_field in config_data and config_data[path_field]:\n",
    "                config_data[path_field] = Path(config_data[path_field])\n",
    "        \n",
    "        return cls(**config_data)\n",
    "\n",
    "    @classmethod\n",
    "    def from_env(cls) -> \"EmbeddingConfig\":\n",
    "        \"\"\"Load configuration from environment variables.\"\"\"\n",
    "        return cls(\n",
    "            input_file=Path(os.getenv(\"EMBEDDING_INPUT_FILE\", \"input.pkl\")),\n",
    "            output_file=Path(os.getenv(\"EMBEDDING_OUTPUT_FILE\", \"embeddings.pkl\")),\n",
    "            ollama_model=os.getenv(\"EMBEDDING_OLLAMA_MODEL\", \"distilroberta\"),\n",
    "            ollama_url=os.getenv(\"EMBEDDING_OLLAMA_URL\", \"http://localhost:11434/api/embeddings\"),\n",
    "            sentence_transformer_model=os.getenv(\"EMBEDDING_SENTENCE_TRANSFORMER_MODEL\", \"all-MiniLM-L6-v2\"),\n",
    "            batch_size=int(os.getenv(\"EMBEDDING_BATCH_SIZE\", \"32\")),\n",
    "            max_retries=int(os.getenv(\"EMBEDDING_MAX_RETRIES\", \"3\")),\n",
    "            retry_delay=float(os.getenv(\"EMBEDDING_RETRY_DELAY\", \"1.0\")),\n",
    "            use_gpu=os.getenv(\"EMBEDDING_USE_GPU\", \"true\").lower() in (\"true\", \"1\", \"yes\"),\n",
    "            num_workers=int(os.getenv(\"EMBEDDING_NUM_WORKERS\", \"4\")),\n",
    "            timeout=int(os.getenv(\"EMBEDDING_HTTP_TIMEOUT\", \"60\")),\n",
    "            cache_dir=Path(os.getenv(\"EMBEDDING_CACHE_DIR\")) if os.getenv(\"EMBEDDING_CACHE_DIR\") else None,\n",
    "            save_interval=int(os.getenv(\"EMBEDDING_SAVE_INTERVAL\", \"100\")),\n",
    "        )\n",
    "\n",
    "\n",
    "class EmbeddingCache:\n",
    "    \"\"\"Cache for storing and retrieving embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Optional[Path] = None):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.in_memory_cache = {}\n",
    "        \n",
    "        if cache_dir:\n",
    "            cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "            self.ollama_cache_path = cache_dir / \"ollama_cache.pkl\"\n",
    "            self.st_cache_path = cache_dir / \"sentence_transformer_cache.pkl\"\n",
    "            \n",
    "            # Load existing caches if they exist\n",
    "            self.ollama_cache = self._load_cache(self.ollama_cache_path)\n",
    "            self.st_cache = self._load_cache(self.st_cache_path)\n",
    "        else:\n",
    "            self.ollama_cache = {}\n",
    "            self.st_cache = {}\n",
    "    \n",
    "    def _load_cache(self, path: Path) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Load cache from disk if it exists.\"\"\"\n",
    "        if path.exists():\n",
    "            try:\n",
    "                with open(path, \"rb\") as f:\n",
    "                    return pickle.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load cache from {path}: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    def save_cache(self) -> None:\n",
    "        \"\"\"Save caches to disk.\"\"\"\n",
    "        if not self.cache_dir:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            with open(self.ollama_cache_path, \"wb\") as f:\n",
    "                pickle.dump(self.ollama_cache, f)\n",
    "            \n",
    "            with open(self.st_cache_path, \"wb\") as f:\n",
    "                pickle.dump(self.st_cache, f)\n",
    "                \n",
    "            logger.info(f\"Caches saved to {self.cache_dir}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save caches: {e}\")\n",
    "    \n",
    "    def get_ollama_embedding(self, text: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"Get Ollama embedding from cache.\"\"\"\n",
    "        return self.ollama_cache.get(text)\n",
    "    \n",
    "    def set_ollama_embedding(self, text: str, embedding: np.ndarray) -> None:\n",
    "        \"\"\"Set Ollama embedding in cache.\"\"\"\n",
    "        self.ollama_cache[text] = embedding\n",
    "    \n",
    "    def get_st_embedding(self, text: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"Get SentenceTransformer embedding from cache.\"\"\"\n",
    "        return self.st_cache.get(text)\n",
    "    \n",
    "    def set_st_embedding(self, text: str, embedding: np.ndarray) -> None:\n",
    "        \"\"\"Set SentenceTransformer embedding in cache.\"\"\"\n",
    "        self.st_cache[text] = embedding\n",
    "\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generates and combines embeddings from multiple models.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EmbeddingConfig):\n",
    "        self.config = config\n",
    "        self.cache = EmbeddingCache(config.cache_dir)\n",
    "        \n",
    "        # Initialize SentenceTransformer\n",
    "        device = \"cuda\" if config.use_gpu and torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Using device: {device} for SentenceTransformer\")\n",
    "        self.st_model = SentenceTransformer(config.sentence_transformer_model, device=device)\n",
    "        \n",
    "        # For batch processing with sentence-transformers\n",
    "        self.st_model.max_seq_length = 512  # Adjust as needed\n",
    "    \n",
    "    @asynccontextmanager\n",
    "    async def get_client(self):\n",
    "        \"\"\"Context manager for httpx client.\"\"\"\n",
    "        async with httpx.AsyncClient(timeout=self.config.timeout) as client:\n",
    "            yield client\n",
    "    \n",
    "    async def generate_ollama_embedding(self, text: str, client: httpx.AsyncClient) -> np.ndarray:\n",
    "        \"\"\"Generate embedding using Ollama API.\"\"\"\n",
    "        cached = self.cache.get_ollama_embedding(text)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "        \n",
    "        for attempt in range(self.config.max_retries):\n",
    "            try:\n",
    "                response = await client.post(\n",
    "                    self.config.ollama_url,\n",
    "                    json={\"model\": self.config.ollama_model, \"prompt\": text}\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                embedding = np.array(data.get(\"embedding\", []))\n",
    "                \n",
    "                if embedding.size == 0:\n",
    "                    raise ValueError(\"Empty embedding received from Ollama API\")\n",
    "                \n",
    "                self.cache.set_ollama_embedding(text, embedding)\n",
    "                return embedding\n",
    "            \n",
    "            except Exception as e:\n",
    "                if attempt < self.config.max_retries - 1:\n",
    "                    delay = self.config.retry_delay * (2 ** attempt)  # Exponential backoff\n",
    "                    logger.warning(f\"Ollama API error (attempt {attempt+1}/{self.config.max_retries}): {e}. Retrying in {delay:.2f}s\")\n",
    "                    await asyncio.sleep(delay)\n",
    "                else:\n",
    "                    logger.error(f\"Failed to generate Ollama embedding after {self.config.max_retries} attempts: {e}\")\n",
    "                    # Return zero vector of appropriate size on failure\n",
    "                    # Using a size of 768 as a default for many transformer-based models\n",
    "                    return np.zeros(768)\n",
    "    \n",
    "    def generate_sentence_transformer_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Generate embedding using SentenceTransformer.\"\"\"\n",
    "        cached = self.cache.get_st_embedding(text)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "        \n",
    "        try:\n",
    "            # SentenceTransformer automatically handles GPU acceleration if available\n",
    "            embedding = self.st_model.encode(text, convert_to_numpy=True)\n",
    "            self.cache.set_st_embedding(text, embedding)\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate SentenceTransformer embedding: {e}\")\n",
    "            # Return zero vector of appropriate size\n",
    "            return np.zeros(self.st_model.get_sentence_embedding_dimension())\n",
    "    \n",
    "    async def generate_combined_embedding(self, text: str, client: httpx.AsyncClient) -> np.ndarray:\n",
    "        \"\"\"Generate combined embedding from both models.\"\"\"\n",
    "        ollama_embedding = await self.generate_ollama_embedding(text, client)\n",
    "        st_embedding = self.generate_sentence_transformer_embedding(text)\n",
    "        \n",
    "        # Combine embeddings (simple concatenation, can be extended with other methods)\n",
    "        return np.concatenate([ollama_embedding, st_embedding])\n",
    "    \n",
    "    async def process_batch(self, texts: List[str], client: httpx.AsyncClient) -> List[np.ndarray]:\n",
    "        \"\"\"Process a batch of texts in parallel.\"\"\"\n",
    "        tasks = []\n",
    "        for text in texts:\n",
    "            task = self.generate_combined_embedding(text, client)\n",
    "            tasks.append(task)\n",
    "        \n",
    "        return await asyncio.gather(*tasks)\n",
    "    \n",
    "    async def process_data(self, data: Dict[str, str]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Process all text data and generate embeddings.\"\"\"\n",
    "        result = {}\n",
    "        keys = list(data.keys())\n",
    "        total_batches = (len(keys) + self.config.batch_size - 1) // self.config.batch_size\n",
    "        \n",
    "        async with self.get_client() as client:\n",
    "            for i in range(0, len(keys), self.config.batch_size):\n",
    "                batch_keys = keys[i:i + self.config.batch_size]\n",
    "                batch_texts = [data[k] for k in batch_keys]\n",
    "                \n",
    "                batch_result = await self.process_batch(batch_texts, client)\n",
    "                \n",
    "                for key, embedding in zip(batch_keys, batch_result):\n",
    "                    result[key] = embedding\n",
    "                \n",
    "                # Save progress at intervals\n",
    "                if i > 0 and i % self.config.save_interval == 0:\n",
    "                    self._save_checkpoint(result)\n",
    "                    self.cache.save_cache()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _save_checkpoint(self, current_results: Dict[str, np.ndarray]) -> None:\n",
    "        \"\"\"Save current progress to a checkpoint file.\"\"\"\n",
    "        checkpoint_path = self.config.output_file.with_suffix(\".checkpoint.pkl\")\n",
    "        try:\n",
    "            with open(checkpoint_path, \"wb\") as f:\n",
    "                pickle.dump(current_results, f)\n",
    "            logger.info(f\"Checkpoint saved: {len(current_results)} items processed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to run the embedding generator.\"\"\"\n",
    "    # Load configuration (prioritize config file, fall back to env vars)\n",
    "    config_path = Path(\"embedding_config.yaml\")\n",
    "    if config_path.exists():\n",
    "        config = EmbeddingConfig.from_yaml(config_path)\n",
    "        logger.info(f\"Loaded configuration from {config_path}\")\n",
    "    else:\n",
    "        config = EmbeddingConfig.from_env()\n",
    "        logger.info(\"Loaded configuration from environment variables\")\n",
    "    \n",
    "    logger.info(f\"Input file: {config.input_file}\")\n",
    "    logger.info(f\"Output file: {config.output_file}\")\n",
    "    \n",
    "    # Check if input file exists\n",
    "    if not config.input_file.exists():\n",
    "        logger.error(f\"Input file not found: {config.input_file}\")\n",
    "        return 1\n",
    "    \n",
    "    # Load input data\n",
    "    try:\n",
    "        with open(config.input_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        logger.info(f\"Loaded {len(data)} items from {config.input_file}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load input file: {e}\")\n",
    "        return 1\n",
    "    \n",
    "    # Check for checkpoint\n",
    "    checkpoint_path = config.output_file.with_suffix(\".checkpoint.pkl\")\n",
    "    result = {}\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            with open(checkpoint_path, \"rb\") as f:\n",
    "                result = pickle.load(f)\n",
    "            logger.info(f\"Resuming from checkpoint with {len(result)} items already processed\")\n",
    "            \n",
    "            # Filter data to process only remaining items\n",
    "            data = {k: v for k, v in data.items() if k not in result}\n",
    "            logger.info(f\"{len(data)} items remaining to process\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load checkpoint, starting from scratch: {e}\")\n",
    "    \n",
    "    # Initialize and run the embedding generator\n",
    "    start_time = time.time()\n",
    "    generator = EmbeddingGenerator(config)\n",
    "    \n",
    "    if not data:\n",
    "        logger.info(\"No new data to process\")\n",
    "    else:\n",
    "        logger.info(f\"Starting embedding generation for {len(data)} items\")\n",
    "        \n",
    "        # Process data\n",
    "        new_results = await generator.process_data(data)\n",
    "        result.update(new_results)\n",
    "        \n",
    "        # Save cache for future runs\n",
    "        generator.cache.save_cache()\n",
    "    \n",
    "    # Save final results\n",
    "    try:\n",
    "        with open(config.output_file, \"wb\") as f:\n",
    "            pickle.dump(result, f)\n",
    "        \n",
    "        # Remove checkpoint file if exists\n",
    "        if checkpoint_path.exists():\n",
    "            checkpoint_path.unlink()\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Completed successfully. Processed {len(result)} items in {elapsed_time:.2f} seconds\")\n",
    "        logger.info(f\"Results saved to {config.output_file}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save results: {e}\")\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        exit_code = asyncio.run(main())\n",
    "        exit(exit_code)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Process interrupted by user\")\n",
    "        exit(130)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Unhandled exception: {e}\", exc_info=True)\n",
    "        exit(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
