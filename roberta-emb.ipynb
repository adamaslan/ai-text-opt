{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Text Embedding Generator\n",
    "\n",
    "This script generates and combines embeddings from multiple models (Ollama API and SentenceTransformer)\n",
    "for text data, optimized for modern Python environments in 2025.\n",
    "\n",
    "Features:\n",
    "- Asynchronous and parallel processing\n",
    "- GPU acceleration\n",
    "- Robust error handling and logging\n",
    "- Configurable parameters via environment variables or config files\n",
    "- Progress tracking\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import logging\n",
    "import asyncio\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "\n",
    "import httpx\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import yaml\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"embedding_generator.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"embedding_generator\")\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingConfig:\n",
    "    \"\"\"Configuration for embedding generation process.\"\"\"\n",
    "    input_file: Path\n",
    "    output_file: Path\n",
    "    ollama_model: str = \"distilroberta\"\n",
    "    ollama_url: str = \"http://localhost:11434/api/embeddings\"\n",
    "    sentence_transformer_model: str = \"all-MiniLM-L6-v2\"\n",
    "    batch_size: int = 32\n",
    "    max_retries: int = 3\n",
    "    retry_delay: float = 1.0\n",
    "    use_gpu: bool = True\n",
    "    num_workers: int = 4\n",
    "    timeout: int = 60  # HTTP request timeout in seconds\n",
    "    cache_dir: Optional[Path] = None\n",
    "    save_interval: int = 100  # Save progress every N items\n",
    "    \n",
    "    @classmethod\n",
    "    def from_yaml(cls, config_path: Union[str, Path]) -> \"EmbeddingConfig\":\n",
    "        \"\"\"Load configuration from YAML file.\"\"\"\n",
    "        config_path = Path(config_path)\n",
    "        if not config_path.exists():\n",
    "            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "        \n",
    "        with open(config_path, \"r\") as f:\n",
    "            config_data = yaml.safe_load(f)\n",
    "        \n",
    "        # Convert string paths to Path objects\n",
    "        for path_field in [\"input_file\", \"output_file\", \"cache_dir\"]:\n",
    "            if path_field in config_data and config_data[path_field]:\n",
    "                config_data[path_field] = Path(config_data[path_field])\n",
    "        \n",
    "        return cls(**config_data)\n",
    "\n",
    "    @classmethod\n",
    "    def from_env(cls) -> \"EmbeddingConfig\":\n",
    "        \"\"\"Load configuration from environment variables.\"\"\"\n",
    "        return cls(\n",
    "            input_file=Path(os.getenv(\"EMBEDDING_INPUT_FILE\", \"input.pkl\")),\n",
    "            output_file=Path(os.getenv(\"EMBEDDING_OUTPUT_FILE\", \"embeddings.pkl\")),\n",
    "            ollama_model=os.getenv(\"EMBEDDING_OLLAMA_MODEL\", \"distilroberta\"),\n",
    "            ollama_url=os.getenv(\"EMBEDDING_OLLAMA_URL\", \"http://localhost:11434/api/embeddings\"),\n",
    "            sentence_transformer_model=os.getenv(\"EMBEDDING_SENTENCE_TRANSFORMER_MODEL\", \"all-MiniLM-L6-v2\"),\n",
    "            batch_size=int(os.getenv(\"EMBEDDING_BATCH_SIZE\", \"32\")),\n",
    "            max_retries=int(os.getenv(\"EMBEDDING_MAX_RETRIES\", \"3\")),\n",
    "            retry_delay=float(os.getenv(\"EMBEDDING_RETRY_DELAY\", \"1.0\")),\n",
    "            use_gpu=os.getenv(\"EMBEDDING_USE_GPU\", \"true\").lower() in (\"true\", \"1\", \"yes\"),\n",
    "            num_workers=int(os.getenv(\"EMBEDDING_NUM_WORKERS\", \"4\")),\n",
    "            timeout=int(os.getenv(\"EMBEDDING_HTTP_TIMEOUT\", \"60\")),\n",
    "            cache_dir=Path(os.getenv(\"EMBEDDING_CACHE_DIR\")) if os.getenv(\"EMBEDDING_CACHE_DIR\") else None,\n",
    "            save_interval=int(os.getenv(\"EMBEDDING_SAVE_INTERVAL\", \"100\")),\n",
    "        )\n",
    "\n",
    "\n",
    "class EmbeddingCache:\n",
    "    \"\"\"Cache for storing and retrieving embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Optional[Path] = None):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.in_memory_cache = {}\n",
    "        \n",
    "        if cache_dir:\n",
    "            cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "            self.ollama_cache_path = cache_dir / \"ollama_cache.pkl\"\n",
    "            self.st_cache_path = cache_dir / \"sentence_transformer_cache.pkl\"\n",
    "            \n",
    "            # Load existing caches if they exist\n",
    "            self.ollama_cache = self._load_cache(self.ollama_cache_path)\n",
    "            self.st_cache = self._load_cache(self.st_cache_path)\n",
    "        else:\n",
    "            self.ollama_cache = {}\n",
    "            self.st_cache = {}\n",
    "    \n",
    "    def _load_cache(self, path: Path) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Load cache from disk if it exists.\"\"\"\n",
    "        if path.exists():\n",
    "            try:\n",
    "                with open(path, \"rb\") as f:\n",
    "                    return pickle.load(f)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load cache from {path}: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    def save_cache(self) -> None:\n",
    "        \"\"\"Save caches to disk.\"\"\"\n",
    "        if not self.cache_dir:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            with open(self.ollama_cache_path, \"wb\") as f:\n",
    "                pickle.dump(self.ollama_cache, f)\n",
    "            \n",
    "            with open(self.st_cache_path, \"wb\") as f:\n",
    "                pickle.dump(self.st_cache, f)\n",
    "                \n",
    "            logger.info(f\"Caches saved to {self.cache_dir}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save caches: {e}\")\n",
    "    \n",
    "    def get_ollama_embedding(self, text: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"Get Ollama embedding from cache.\"\"\"\n",
    "        return self.ollama_cache.get(text)\n",
    "    \n",
    "    def set_ollama_embedding(self, text: str, embedding: np.ndarray) -> None:\n",
    "        \"\"\"Set Ollama embedding in cache.\"\"\"\n",
    "        self.ollama_cache[text] = embedding\n",
    "    \n",
    "    def get_st_embedding(self, text: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"Get SentenceTransformer embedding from cache.\"\"\"\n",
    "        return self.st_cache.get(text)\n",
    "    \n",
    "    def set_st_embedding(self, text: str, embedding: np.ndarray) -> None:\n",
    "        \"\"\"Set SentenceTransformer embedding in cache.\"\"\"\n",
    "        self.st_cache[text] = embedding\n",
    "\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generates and combines embeddings from multiple models.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EmbeddingConfig):\n",
    "        self.config = config\n",
    "        self.cache = EmbeddingCache(config.cache_dir)\n",
    "        \n",
    "        # Initialize SentenceTransformer\n",
    "        device = \"cuda\" if config.use_gpu and torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Using device: {device} for SentenceTransformer\")\n",
    "        self.st_model = SentenceTransformer(config.sentence_transformer_model, device=device)\n",
    "        \n",
    "        # For batch processing with sentence-transformers\n",
    "        self.st_model.max_seq_length = 512  # Adjust as needed\n",
    "    \n",
    "    @asynccontextmanager\n",
    "    async def get_client(self):\n",
    "        \"\"\"Context manager for httpx client.\"\"\"\n",
    "        async with httpx.AsyncClient(timeout=self.config.timeout) as client:\n",
    "            yield client\n",
    "    \n",
    "    async def generate_ollama_embedding(self, text: str, client: httpx.AsyncClient) -> np.ndarray:\n",
    "        \"\"\"Generate embedding using Ollama API.\"\"\"\n",
    "        cached = self.cache.get_ollama_embedding(text)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "        \n",
    "        for attempt in range(self.config.max_retries):\n",
    "            try:\n",
    "                response = await client.post(\n",
    "                    self.config.ollama_url,\n",
    "                    json={\"model\": self.config.ollama_model, \"prompt\": text}\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                embedding = np.array(data.get(\"embedding\", []))\n",
    "                \n",
    "                if embedding.size == 0:\n",
    "                    raise ValueError(\"Empty embedding received from Ollama API\")\n",
    "                \n",
    "                self.cache.set_ollama_embedding(text, embedding)\n",
    "                return embedding\n",
    "            \n",
    "            except Exception as e:\n",
    "                if attempt < self.config.max_retries - 1:\n",
    "                    delay = self.config.retry_delay * (2 ** attempt)  # Exponential backoff\n",
    "                    logger.warning(f\"Ollama API error (attempt {attempt+1}/{self.config.max_retries}): {e}. Retrying in {delay:.2f}s\")\n",
    "                    await asyncio.sleep(delay)\n",
    "                else:\n",
    "                    logger.error(f\"Failed to generate Ollama embedding after {self.config.max_retries} attempts: {e}\")\n",
    "                    # Return zero vector of appropriate size on failure\n",
    "                    # Using a size of 768 as a default for many transformer-based models\n",
    "                    return np.zeros(768)\n",
    "    \n",
    "    def generate_sentence_transformer_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Generate embedding using SentenceTransformer.\"\"\"\n",
    "        cached = self.cache.get_st_embedding(text)\n",
    "        if cached is not None:\n",
    "            return cached\n",
    "        \n",
    "        try:\n",
    "            # SentenceTransformer automatically handles GPU acceleration if available\n",
    "            embedding = self.st_model.encode(text, convert_to_numpy=True)\n",
    "            self.cache.set_st_embedding(text, embedding)\n",
    "            return embedding\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate SentenceTransformer embedding: {e}\")\n",
    "            # Return zero vector of appropriate size\n",
    "            return np.zeros(self.st_model.get_sentence_embedding_dimension())\n",
    "    \n",
    "    async def generate_combined_embedding(self, text: str, client: httpx.AsyncClient) -> np.ndarray:\n",
    "        \"\"\"Generate combined embedding from both models.\"\"\"\n",
    "        ollama_embedding = await self.generate_ollama_embedding(text, client)\n",
    "        st_embedding = self.generate_sentence_transformer_embedding(text)\n",
    "        \n",
    "        # Combine embeddings (simple concatenation, can be extended with other methods)\n",
    "        return np.concatenate([ollama_embedding, st_embedding])\n",
    "    \n",
    "    async def process_batch(self, texts: List[str], client: httpx.AsyncClient) -> List[np.ndarray]:\n",
    "        \"\"\"Process a batch of texts in parallel.\"\"\"\n",
    "        tasks = []\n",
    "        for text in texts:\n",
    "            task = self.generate_combined_embedding(text, client)\n",
    "            tasks.append(task)\n",
    "        \n",
    "        return await asyncio.gather(*tasks)\n",
    "    \n",
    "    async def process_data(self, data: Dict[str, str]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Process all text data and generate embeddings.\"\"\"\n",
    "        result = {}\n",
    "        keys = list(data.keys())\n",
    "        total_batches = (len(keys) + self.config.batch_size - 1) // self.config.batch_size\n",
    "        \n",
    "        async with self.get_client() as client:\n",
    "            for i in range(0, len(keys), self.config.batch_size):\n",
    "                batch_keys = keys[i:i + self.config.batch_size]\n",
    "                batch_texts = [data[k] for k in batch_keys]\n",
    "                \n",
    "                batch_result = await self.process_batch(batch_texts, client)\n",
    "                \n",
    "                for key, embedding in zip(batch_keys, batch_result):\n",
    "                    result[key] = embedding\n",
    "                \n",
    "                # Save progress at intervals\n",
    "                if i > 0 and i % self.config.save_interval == 0:\n",
    "                    self._save_checkpoint(result)\n",
    "                    self.cache.save_cache()\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _save_checkpoint(self, current_results: Dict[str, np.ndarray]) -> None:\n",
    "        \"\"\"Save current progress to a checkpoint file.\"\"\"\n",
    "        checkpoint_path = self.config.output_file.with_suffix(\".checkpoint.pkl\")\n",
    "        try:\n",
    "            with open(checkpoint_path, \"wb\") as f:\n",
    "                pickle.dump(current_results, f)\n",
    "            logger.info(f\"Checkpoint saved: {len(current_results)} items processed\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to run the embedding generator.\"\"\"\n",
    "    # Load configuration (prioritize config file, fall back to env vars)\n",
    "    config_path = Path(\"embedding_config.yaml\")\n",
    "    if config_path.exists():\n",
    "        config = EmbeddingConfig.from_yaml(config_path)\n",
    "        logger.info(f\"Loaded configuration from {config_path}\")\n",
    "    else:\n",
    "        config = EmbeddingConfig.from_env()\n",
    "        logger.info(\"Loaded configuration from environment variables\")\n",
    "    \n",
    "    logger.info(f\"Input file: {config.input_file}\")\n",
    "    logger.info(f\"Output file: {config.output_file}\")\n",
    "    \n",
    "    # Check if input file exists\n",
    "    if not config.input_file.exists():\n",
    "        logger.error(f\"Input file not found: {config.input_file}\")\n",
    "        return 1\n",
    "    \n",
    "    # Load input data\n",
    "    try:\n",
    "        with open(config.input_file, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        logger.info(f\"Loaded {len(data)} items from {config.input_file}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load input file: {e}\")\n",
    "        return 1\n",
    "    \n",
    "    # Check for checkpoint\n",
    "    checkpoint_path = config.output_file.with_suffix(\".checkpoint.pkl\")\n",
    "    result = {}\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            with open(checkpoint_path, \"rb\") as f:\n",
    "                result = pickle.load(f)\n",
    "            logger.info(f\"Resuming from checkpoint with {len(result)} items already processed\")\n",
    "            \n",
    "            # Filter data to process only remaining items\n",
    "            data = {k: v for k, v in data.items() if k not in result}\n",
    "            logger.info(f\"{len(data)} items remaining to process\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load checkpoint, starting from scratch: {e}\")\n",
    "    \n",
    "    # Initialize and run the embedding generator\n",
    "    start_time = time.time()\n",
    "    generator = EmbeddingGenerator(config)\n",
    "    \n",
    "    if not data:\n",
    "        logger.info(\"No new data to process\")\n",
    "    else:\n",
    "        logger.info(f\"Starting embedding generation for {len(data)} items\")\n",
    "        \n",
    "        # Process data\n",
    "        new_results = await generator.process_data(data)\n",
    "        result.update(new_results)\n",
    "        \n",
    "        # Save cache for future runs\n",
    "        generator.cache.save_cache()\n",
    "    \n",
    "    # Save final results\n",
    "    try:\n",
    "        with open(config.output_file, \"wb\") as f:\n",
    "            pickle.dump(result, f)\n",
    "        \n",
    "        # Remove checkpoint file if exists\n",
    "        if checkpoint_path.exists():\n",
    "            checkpoint_path.unlink()\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Completed successfully. Processed {len(result)} items in {elapsed_time:.2f} seconds\")\n",
    "        logger.info(f\"Results saved to {config.output_file}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save results: {e}\")\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        exit_code = asyncio.run(main())\n",
    "        exit(exit_code)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Process interrupted by user\")\n",
    "        exit(130)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Unhandled exception: {e}\", exc_info=True)\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 14:27:10,643 - csv_embedding_generator - CRITICAL - Unhandled exception: asyncio.run() cannot be called from a running event loop\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/49/6ydqkbq172ngzt6p49xfm6b00000gn/T/ipykernel_2495/2335792938.py\", line 184, in <module>\n",
      "    exit_code = asyncio.run(main())\n",
      "                ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/asyncio/runners.py\", line 190, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: asyncio.run() cannot be called from a running event loop\n",
      "/var/folders/49/6ydqkbq172ngzt6p49xfm6b00000gn/T/ipykernel_2495/2335792938.py:191: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  exit(1)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "CSV Embedding Generator\n",
    "\n",
    "This script generates embeddings for text data from a CSV file using HuggingFace models,\n",
    "optimized for modern Python environments.\n",
    "\n",
    "Features:\n",
    "- Batch processing\n",
    "- GPU acceleration\n",
    "- Robust error handling and logging\n",
    "- Configurable parameters via environment variables or config files\n",
    "- Progress tracking and checkpointing\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"embedding_generator.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"csv_embedding_generator\")\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingConfig:\n",
    "    \"\"\"Configuration for embedding generation process.\"\"\"\n",
    "    input_file: Path\n",
    "    output_file: Path\n",
    "    huggingface_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    batch_size: int = 32\n",
    "    use_gpu: bool = True\n",
    "    save_interval: int = 100  # Save progress every N batches\n",
    "    text_column: str = \"articles\"\n",
    "    \n",
    "    @classmethod\n",
    "    def from_yaml(cls, config_path: Union[str, Path]) -> \"EmbeddingConfig\":\n",
    "        \"\"\"Load configuration from YAML file.\"\"\"\n",
    "        config_path = Path(config_path)\n",
    "        if not config_path.exists():\n",
    "            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "        \n",
    "        with open(config_path, \"r\") as f:\n",
    "            config_data = yaml.safe_load(f)\n",
    "        \n",
    "        # Convert string paths to Path objects\n",
    "        for path_field in [\"input_file\", \"output_file\"]:\n",
    "            if config_data.get(path_field):\n",
    "                config_data[path_field] = Path(config_data[path_field])\n",
    "        \n",
    "        return cls(**config_data)\n",
    "\n",
    "    @classmethod\n",
    "    def from_env(cls) -> \"EmbeddingConfig\":\n",
    "        \"\"\"Load configuration from environment variables.\"\"\"\n",
    "        return cls(\n",
    "            input_file=Path(os.getenv(\"EMBEDDING_INPUT_FILE\", \"articles.csv\")),\n",
    "            output_file=Path(os.getenv(\"EMBEDDING_OUTPUT_FILE\", \"embeddings.csv\")),\n",
    "            huggingface_model=os.getenv(\"EMBEDDING_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\"),\n",
    "            batch_size=int(os.getenv(\"EMBEDDING_BATCH_SIZE\", \"32\")),\n",
    "            use_gpu=os.getenv(\"EMBEDDING_USE_GPU\", \"true\").lower() in (\"true\", \"1\", \"yes\"),\n",
    "            save_interval=int(os.getenv(\"EMBEDDING_SAVE_INTERVAL\", \"100\")),\n",
    "            text_column=os.getenv(\"EMBEDDING_TEXT_COLUMN\", \"articles\"),\n",
    "        )\n",
    "\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    \"\"\"Generates embeddings using HuggingFace models.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EmbeddingConfig):\n",
    "        self.config = config\n",
    "        self.model = HuggingFaceEmbeddings(model_name=config.huggingface_model)\n",
    "        \n",
    "        if config.use_gpu:\n",
    "            logger.info(\"Using GPU acceleration\")\n",
    "            self.model.client = self.model.client.to(\"cuda\")\n",
    "\n",
    "    def generate_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for a batch of texts.\"\"\"\n",
    "        return self.model.embed_documents(texts)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main function to run the embedding generator.\"\"\"\n",
    "    # Load configuration\n",
    "    config_path = Path(\"embedding_config.yaml\")\n",
    "    if config_path.exists():\n",
    "        config = EmbeddingConfig.from_yaml(config_path)\n",
    "        logger.info(f\"Loaded configuration from {config_path}\")\n",
    "    else:\n",
    "        config = EmbeddingConfig.from_env()\n",
    "        logger.info(\"Loaded configuration from environment variables\")\n",
    "    \n",
    "    logger.info(f\"Input file: {config.input_file}\")\n",
    "    logger.info(f\"Output file: {config.output_file}\")\n",
    "    \n",
    "    # Check if input file exists\n",
    "    if not config.input_file.exists():\n",
    "        logger.error(f\"Input file not found: {config.input_file}\")\n",
    "        return 1\n",
    "\n",
    "    # Load CSV data\n",
    "    try:\n",
    "        df = pd.read_csv(\"151_ideas_updated2.csv\")\n",
    "        logger.info(f\"Loaded {len(df)} rows from CSV\")\n",
    "        \n",
    "        if config.text_column not in df.columns:\n",
    "            raise ValueError(f\"CSV file missing required column: {config.text_column}\")\n",
    "            \n",
    "        texts = df[config.text_column].tolist()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load input data: {e}\")\n",
    "        return 1\n",
    "\n",
    "    # Initialize embedding generator\n",
    "    generator = EmbeddingGenerator(config)\n",
    "    \n",
    "    # Checkpoint setup\n",
    "    checkpoint_path = config.output_file.with_suffix(\".checkpoint.csv\")\n",
    "    start_idx = 0\n",
    "    \n",
    "    # Resume from checkpoint if available\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            checkpoint_df = pd.read_csv(checkpoint_path)\n",
    "            start_idx = len(checkpoint_df)\n",
    "            df.iloc[:start_idx] = checkpoint_df\n",
    "            logger.info(f\"Resuming from checkpoint at row {start_idx}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load checkpoint: {e}\")\n",
    "\n",
    "    # Process batches with progress bar\n",
    "    try:\n",
    "        with tqdm(total=len(df), initial=start_idx, desc=\"Generating embeddings\") as pbar:\n",
    "            for batch_start in range(start_idx, len(df), config.batch_size):\n",
    "                batch_end = min(batch_start + config.batch_size, len(df))\n",
    "                batch_texts = texts[batch_start:batch_end]\n",
    "                \n",
    "                # Generate embeddings\n",
    "                embeddings = generator.generate_batch(batch_texts)\n",
    "                \n",
    "                # Update DataFrame\n",
    "                df.loc[batch_start:batch_end-1, \"embeddings\"] = embeddings\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.update(len(batch_texts))\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if (batch_end // config.batch_size) % config.save_interval == 0:\n",
    "                    df.to_csv(checkpoint_path, index=False)\n",
    "                    logger.info(f\"Saved checkpoint at row {batch_end}\")\n",
    "\n",
    "        # Save final results\n",
    "        df.to_csv(config.output_file, index=False)\n",
    "        if checkpoint_path.exists():\n",
    "            checkpoint_path.unlink()\n",
    "        logger.info(f\"Successfully processed {len(df)} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Processing failed: {e}\")\n",
    "        logger.info(f\"Saving final checkpoint to {checkpoint_path}\")\n",
    "        df.to_csv(checkpoint_path, index=False)\n",
    "        return 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        exit_code = asyncio.run(main())\n",
    "        exit(exit_code)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Process interrupted by user\")\n",
    "        exit(130)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Unhandled exception: {e}\", exc_info=True)\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b573eb6f3409472186d65951f83a1d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbb885e606e47c2adce3967a0a09aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4692ac75b7e4eac8a927a08f314a89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f99ab82376480699e0907157c4c82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad09eedab96c4ab8a358d3acbde86700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7116179a4204402a95f11388e8867bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generated and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load data from CSV\n",
    "df = pd.read_csv('151_ideas_updated2.csv')\n",
    "\n",
    "# Step 2: Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text)     # Remove extra whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "df['Cleaned_Ideas'] = df['Ideas'].apply(preprocess_text)\n",
    "\n",
    "# Step 3: Initialize RoBERTa model\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Step 4: Embedding generation function\n",
    "def get_roberta_embeddings(texts, batch_size=32):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Use mean pooling for sentence embeddings\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Step 5: Generate embeddings in batches\n",
    "text_list = df['Cleaned_Ideas'].tolist()\n",
    "embeddings = get_roberta_embeddings(text_list)\n",
    "\n",
    "# Step 6: Save embeddings back to DataFrame\n",
    "df['Embeddings'] = embeddings.tolist()\n",
    "\n",
    "# Step 7: Save to new CSV\n",
    "df.to_csv('ideas_with_embeddings.csv', index=False)\n",
    "\n",
    "print(\"Embeddings generated and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "EntryNotFoundError",
     "evalue": "404 Client Error. (Request ID: Root=1-67c4b83a-54fb0709422d4ed82cb266b8;1c2c4067-e0a0-4b69-be1d-8d677c1844cb)\n\nEntry Not Found for url: https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/llama-3.2-3b-instruct-uncensored.Q4_K_M.gguf.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/llama-3.2-3b-instruct-uncensored.Q4_K_M.gguf",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[0;32m----> 2\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbartowski/Llama-3.2-3B-Instruct-uncensored-GGUF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama-3.2-3b-instruct-uncensored.Q4_K_M.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    843\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    844\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    859\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    860\u001b[0m     )\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/huggingface_hub/file_download.py:925\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m--> 925\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/huggingface_hub/file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/huggingface_hub/file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/huggingface_hub/file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 277\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/huggingface_hub/file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 301\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:417\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntryNotFound\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    416\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(EntryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    420\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m     )\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-67c4b83a-54fb0709422d4ed82cb266b8;1c2c4067-e0a0-4b69-be1d-8d677c1844cb)\n\nEntry Not Found for url: https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/llama-3.2-3b-instruct-uncensored.Q4_K_M.gguf."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF\",\n",
    "    filename=\"llama-3.2-3b-instruct-uncensored.Q4_K_M.gguf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (2343090427.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    df['embedding'] = df['embedding'].apply(\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "# First install required packages\n",
    "!pip install langchain huggingface_hub pandas faiss-cpu numpy transformers torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# 1. Load and prepare embeddings\n",
    "def load_embeddings(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convert string embeddings to numpy arrays\n",
    "    df['embedding'] = df['embedding'].apply(\n",
    "        lambda x: np.fromstring(x.strip(\"[]\"), sep=\", \", dtype=np.float32)\n",
    "    \n",
    "    texts = df['text_column'].tolist()  # Replace with your text column name\n",
    "    embeddings = np.array(df['embedding'].tolist())\n",
    "    \n",
    "    return texts, embeddings\n",
    "\n",
    "texts, embeddings = load_embeddings(\"ideas_with_embeddings.csv\")\n",
    "\n",
    "# 2. Create FAISS vector store with your RoBERTa embeddings\n",
    "# Initialize with matching RoBERTa config\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"roberta-base\",  # Use the exact model you used for embeddings\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': False}\n",
    ")\n",
    "\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=list(zip(texts, embeddings)),\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "# 3. Set up DeepSeek-R1-1.5B\n",
    "model_name = \"deepseek-ai/deepseek-llm-1.5b-chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    device_map=\"auto\",  # Will use GPU if available\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 4. Create custom prompt template for DeepSeek\n",
    "template = \"\"\"### Instruction:\n",
    "Use the following context to answer the question. \n",
    "If you don't know the answer, say you don't know. Keep answers concise.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# 5. Build RAG chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# 6. Run the chatbot\n",
    "while True:\n",
    "    query = input(\"\\nUser: \")\n",
    "    if query.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    \n",
    "    result = qa_chain({\"query\": query})\n",
    "    print(f\"\\nAssistant: {result['result']}\")\n",
    "    print(\"\\nSources:\")\n",
    "    for doc in result['source_documents']:\n",
    "        print(f\"- {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Cell: RAG System using Existing Embeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "import numpy as np\n",
    "from ipywidgets import interact, widgets, Layout, Output, VBox, HTML\n",
    "\n",
    "# Convert DataFrame embeddings to numpy arrays\n",
    "df['Embeddings'] = df['Embeddings'].apply(eval).apply(np.array)\n",
    "\n",
    "# Create FAISS vector store directly from existing embeddings\n",
    "vector_store = FAISS.from_embeddings(\n",
    "    text_embeddings=zip(df['Cleaned_Ideas'].tolist(), df['Embeddings'].tolist()),\n",
    "    embedding=HuggingFaceEmbeddings()  # Dummy embedding, not actually used\n",
    ")\n",
    "\n",
    "# Initialize Llama-3.2-3B model (make sure you've downloaded the GGUF file)\n",
    "model_path = \"llama-3.2-3b-instruct-uncensored.Q4_K_M.gguf\"  # Update with actual path\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=0.7,\n",
    "    max_tokens=2000,\n",
    "    n_ctx=4096,\n",
    "    n_gpu_layers=40,  # Adjust based on your GPU\n",
    "    n_batch=512,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Custom prompt template for Llama-3.2\n",
    "prompt_template = \"\"\"[INST] <<SYS>>\n",
    "You are a helpful AI assistant that provides ideas based on context.\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "<</SYS>>\n",
    "\n",
    "Context: {context}\n",
    "Question: {question} \n",
    "[/INST]\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# React-style UI components\n",
    "output = Output()\n",
    "question_input = widgets.Text(\n",
    "    placeholder='Enter your question...',\n",
    "    layout=Layout(width='80%', height='40px')\n",
    ")\n",
    "submit_button = widgets.Button(description=\"Ask\", button_style='success')\n",
    "\n",
    "def on_submit(_):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        query = question_input.value\n",
    "        if query:\n",
    "            # Preprocess query same as original data\n",
    "            cleaned_query = preprocess_text(query)\n",
    "            \n",
    "            result = qa_chain({\"query\": cleaned_query})\n",
    "            \n",
    "            # Display formatted response\n",
    "            html = f\"\"\"\n",
    "            <div style='padding: 15px; border-radius: 8px; background: #f0f3f6; margin: 15px 0;'>\n",
    "                <h4 style='color: #2c3e50;'>Question:</h4>\n",
    "                <p>{query}</p>\n",
    "                <h4 style='color: #2c3e50; margin-top: 15px;'>Answer:</h4>\n",
    "                <p>{result['result']}</p>\n",
    "            </div>\n",
    "            <div style='margin-top: 20px; background: #f8f9fa; padding: 15px; border-radius: 8px;'>\n",
    "                <h4 style='color: #2c3e50;'>Relevant Ideas:</h4>\n",
    "                <ul>\n",
    "            \"\"\"\n",
    "            \n",
    "            for doc in result['source_documents']:\n",
    "                html += f\"<li style='margin: 8px 0;'>{doc.page_content}</li>\"\n",
    "            \n",
    "            html += \"</ul></div>\"\n",
    "            display(HTML(html))\n",
    "            question_input.value = ''\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Display the UI\n",
    "display(VBox([\n",
    "    HTML(\"<h1 style='color: #2c3e50; margin-bottom: 20px;'>Ideas RAG Chatbot</h1>\"),\n",
    "    question_input,\n",
    "    submit_button,\n",
    "    output\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
