{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old v - pop up \n",
    "# works responses not great - better rag needed\n",
    "# Install required packages\n",
    "!pip install langchain huggingface_hub pandas faiss-cpu numpy transformers torch accelerate bitsandbytes\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "def load_embeddings(csv_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Verify required columns exist\n",
    "        if 'Cleaned_Ideas' not in df.columns or 'Embeddings' not in df.columns:\n",
    "            raise ValueError(\"CSV must contain 'Cleaned_Ideas' and 'Embeddings' columns\")\n",
    "            \n",
    "        # Convert embeddings with proper handling\n",
    "        df['Embeddings'] = df['Embeddings'].apply(\n",
    "            lambda x: np.fromstring(\n",
    "                x.strip(\"[]\").replace(\"\\n\", \"\"),\n",
    "                sep=\", \",\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Validate embedding dimensions (RoBERTa-base has 768 dimensions)\n",
    "        expected_dim = 768\n",
    "        valid_embeddings = df['Embeddings'].apply(lambda x: len(x) == expected_dim)\n",
    "        if not valid_embeddings.all():\n",
    "            invalid_count = len(df) - valid_embeddings.sum()\n",
    "            raise ValueError(f\"{invalid_count} entries have invalid embedding dimensions\")\n",
    "            \n",
    "        return df['Cleaned_Ideas'].tolist(), np.array(df['Embeddings'].tolist())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embeddings: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# 1. Load texts and embeddings from CSV\n",
    "texts, embeddings = load_embeddings(\"ideas_with_embeddings.csv\")\n",
    "\n",
    "# 2. Create FAISS vector store using a Hugging Face embedding model (using roberta-base)\n",
    "try:\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"roberta-base\",\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        # Removed \"show_progress_bar\" to avoid duplicate parameter issues.\n",
    "        encode_kwargs={'normalize_embeddings': False}\n",
    "    )\n",
    "    \n",
    "    # Create FAISS index; each entry is a tuple (text, embedding)\n",
    "    vector_store = FAISS.from_embeddings(\n",
    "        text_embeddings=list(zip(texts, embeddings)),\n",
    "        embedding=embedding_model,\n",
    "        normalize_L2=True  # Improves cosine similarity calculations\n",
    "    )\n",
    "    print(f\"FAISS index created with {vector_store.index.ntotal} entries\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Vector store creation failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# 3. Load a smaller Hugging Face model in safetensors format (EleutherAI/gpt-neo-125M)\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "try:\n",
    "    # For a smaller model, we typically use full precision\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"GPT-Neo 125M model loaded successfully.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Model loading failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Create the text-generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    do_sample=True,\n",
    "    return_full_text=False,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# 4. Define a custom prompt template\n",
    "template = \"\"\"### Instruction:\n",
    "Analyze this philosophical concept using the provided context. \n",
    "If unsure, state \"I don't have sufficient information.\"\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template_format=\"f-string\"\n",
    ")\n",
    "\n",
    "# 5. Set up a Production-grade Retrieval QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5, \"score_threshold\": 0.4}\n",
    "    ),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": prompt,\n",
    "        \"document_prompt\": PromptTemplate(\n",
    "            input_variables=[\"page_content\"],\n",
    "            template=\"{page_content}\"\n",
    "        )\n",
    "    },\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 6. Enhanced chat interface for interactive querying\n",
    "def run_chat():\n",
    "    print(\"Chatbot initialized. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nUser: \").strip()\n",
    "            if query.lower() in [\"exit\", \"quit\"]:\n",
    "                break\n",
    "                \n",
    "            if not query:\n",
    "                print(\"Please enter a valid question\")\n",
    "                continue\n",
    "                \n",
    "            result = qa_chain({\"query\": query})\n",
    "            \n",
    "            # Process the response\n",
    "            response = result['result'].split(\"### Assistant Response:\")[-1].strip()\n",
    "            print(f\"\\nAssistant: {response}\")\n",
    "            \n",
    "            # Display top source excerpts\n",
    "            print(\"\\nTop Sources:\")\n",
    "            for i, doc in enumerate(result['source_documents'][:3], 1):\n",
    "                excerpt = doc.page_content[:150].replace(\"\\n\", \" \") + \"...\"\n",
    "                score = doc.metadata.get('score', 0)\n",
    "                print(f\"{i}. {excerpt} (Score: {score:.2f})\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting chat.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing request: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_chat()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
