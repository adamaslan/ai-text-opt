{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (3832283904.py, line 79)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 79\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.tokenizer = AutoTokenizer.from_pretrained(\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Optimized CSV Embedding Generator for Low-Resource Environments\n",
    "\n",
    "Enhancements:\n",
    "- Uses ultra-lightweight models (under 100MB)\n",
    "- Memory-efficient processing\n",
    "- CPU optimization with ONNX\n",
    "- Quantized models where available\n",
    "- Reduced precision embeddings\n",
    "- Streaming CSV processing\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import gc\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Generator\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"embedding_generator.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"csv_embedding_generator\")\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingConfig:\n",
    "    \"\"\"Configuration for embedding generation process.\"\"\"\n",
    "    input_file: Path\n",
    "    output_file: Path\n",
    "    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    batch_size: int = 16  # Reduced for low memory\n",
    "    use_float16: bool = True  # Reduce memory usage\n",
    "    text_column: str = \"articles\"\n",
    "    max_text_length: int = 256  # Prevent memory overflow\n",
    "    model_cache_dir: Optional[Path] = None\n",
    "    \n",
    "    @classmethod\n",
    "    def from_yaml(cls, config_path: Path) -> \"EmbeddingConfig\":\n",
    "        \"\"\"Load configuration from YAML file.\"\"\"\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config_data = yaml.safe_load(f)\n",
    "        return cls(**config_data)\n",
    "\n",
    "class OptimizedEmbeddingGenerator:\n",
    "    \"\"\"Memory-efficient embedding generator using ONNX/quantized models.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EmbeddingConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        \n",
    "        # Use model variants optimized for CPU\n",
    "        self.model_map = {\n",
    "            \"mini-lm\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"tiny-bert\": \"sentence-transformers/paraphrase-albert-small-v2\",\n",
    "            \"mobile-bert\": \"google/mobilebert-uncased\",\n",
    "            \"distilroberta\": \"sentence-transformers/all-distilroberta-v1\"\n",
    "        }\n",
    "        \n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load model with CPU-optimized settings.\"\"\"\n",
    "        model_name = self.model_map.get(self.config.model_name, self.config.model_name)\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            cache_dir=str(self.config.model_cache_dir) if self.config.model_cache_dir else None\n",
    "        \n",
    "        self.model = pipeline(\n",
    "            \"feature-extraction\",\n",
    "            model=model_name,\n",
    "            device=-1,  # Force CPU\n",
    "            torch_dtype=torch.float16 if self.config.use_float16 else torch.float32,\n",
    "            truncation=True,\n",
    "            framework=\"pt\",\n",
    "            max_length=self.config.max_text_length\n",
    "        )\n",
    "\n",
    "    def generate_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings with memory constraints.\"\"\"\n",
    "        try:\n",
    "            features = self.model(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.config.max_text_length\n",
    "            )\n",
    "            \n",
    "            # Convert to numpy array for memory efficiency\n",
    "            embeddings = np.mean(\n",
    "                np.array(features, dtype=np.float16 if self.config.use_float16 else np.float32),\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            return embeddings.tolist()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "def process_csv_chunks(config: EmbeddingConfig) -> Generator[pd.DataFrame, None, None]:\n",
    "    \"\"\"Stream CSV in chunks to reduce memory usage.\"\"\"\n",
    "    try:\n",
    "        for chunk in pd.read_csv(\n",
    "            config.input_file,\n",
    "            chunksize=config.batch_size,\n",
    "            usecols=[config.text_column]\n",
    "        ):\n",
    "            # Clean text and truncate\n",
    "            chunk[config.text_column] = chunk[config.text_column].str.slice(\n",
    "                0, config.max_text_length\n",
    "            )\n",
    "            yield chunk\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading CSV: {e}\")\n",
    "        raise\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Main processing function with memory optimizations.\"\"\"\n",
    "    try:\n",
    "        config = EmbeddingConfig.from_yaml(Path(\"embedding_config.yaml\"))\n",
    "    except FileNotFoundError:\n",
    "        config = EmbeddingConfig(\n",
    "            input_file=Path(\"input.csv\"),\n",
    "            output_file=Path(\"embeddings.csv\"),\n",
    "            model_cache_dir=Path(\"model_cache\")\n",
    "        )\n",
    "\n",
    "    # Create output directory if needed\n",
    "    config.output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Initialize components\n",
    "    generator = OptimizedEmbeddingGenerator(config)\n",
    "    \n",
    "    # Process in streaming fashion\n",
    "    with open(config.output_file, \"w\") as f_out:\n",
    "        for chunk_idx, chunk in enumerate(process_csv_chunks(config)):\n",
    "            try:\n",
    "                embeddings = generator.generate_batch(\n",
    "                    chunk[config.text_column].tolist()\n",
    "                )\n",
    "                \n",
    "                # Write incrementally\n",
    "                chunk[\"embeddings\"] = embeddings\n",
    "                chunk.to_csv(\n",
    "                    f_out,\n",
    "                    mode=\"a\",\n",
    "                    header=f_out.tell()==0,\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                # Clean up memory\n",
    "                del embeddings\n",
    "                gc.collect()\n",
    "                \n",
    "                logger.info(f\"Processed batch {chunk_idx + 1}\")\n",
    "                \n",
    "            except MemoryError:\n",
    "                logger.warning(\"Memory overflow! Reducing batch size...\")\n",
    "                config.batch_size = max(1, config.batch_size // 2)\n",
    "                logger.info(f\"New batch size: {config.batch_size}\")\n",
    "                continue\n",
    "\n",
    "    logger.info(\"Embedding generation complete\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(main())\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Process interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Fatal error: {e}\", exc_info=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
