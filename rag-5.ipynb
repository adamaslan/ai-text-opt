{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (3806006448.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 42\u001b[0;36m\u001b[0m\n\u001b[0;31m    def _pull_model(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    \"\"\"Structured response data with metadata\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Handles communication with Ollama API\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self._check_model()\n",
    "        \n",
    "    def _check_model(self):\n",
    "        \"\"\"Verify model exists in Ollama\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/api/tags\")\n",
    "            available_models = [model['name'] for model in response.json()['models']]\n",
    "            model_base = self.model_name.split('/')[-1].split(':')[0]\n",
    "            \n",
    "            if model_base not in available_models:\n",
    "                logger.warning(f\"Model {model_base} not found. Attempting to pull...\")\n",
    "                self._pull_model()\n",
    "                \n",
    "    def _pull_model(self):\n",
    "        \"\"\"Pull model from Hugging Face\"\"\"\n",
    "        try:\n",
    "            requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                timeout=600\n",
    "            )\n",
    "            logger.info(f\"Successfully pulled model {self.model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to pull model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate response from Ollama model\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.9,\n",
    "                        \"top_k\": 40\n",
    "                    }\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"response\"]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ollama API error: {e}\")\n",
    "            return f\"Error: Failed to generate response - {str(e)}\"\n",
    "    \n",
    "class OptimizedAgentSystem:\n",
    "    def __init__(self, \n",
    "                 model_path: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\",\n",
    "                 pickle_file: str = \"responses.pkl\",\n",
    "                 cache_size: int = 1000):\n",
    "        \"\"\"Initialize the optimized agent system with caching and parallel processing\"\"\"\n",
    "        self.pickle_file = pickle_file\n",
    "        self.responses: List[Response] = self._load_responses()\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        self.ollama_client = OllamaClient(model_path)\n",
    "        \n",
    "    def _load_responses(self) -> List[Response]:\n",
    "        \"\"\"Load responses with error handling and data validation\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.pickle_file):\n",
    "                with open(self.pickle_file, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                    if isinstance(data[0], str):\n",
    "                        return [Response(text=r, timestamp=time.time()) for r in data]\n",
    "                    return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading responses: {e}\")\n",
    "        return []\n",
    "\n",
    "    @lru_cache(maxsize=100)\n",
    "    def find_similar_insights(self, query: str, top_k: int = 3) -> List[Tuple[Response, float]]:\n",
    "        \"\"\"Optimized similarity search using caching and vector operations\"\"\"\n",
    "        if not self.responses:\n",
    "            return []\n",
    "            \n",
    "        query_words = set(query.lower().split())\n",
    "        scores = []\n",
    "        \n",
    "        for response in self.responses:\n",
    "            if response.error:\n",
    "                continue\n",
    "            response_words = set(response.text.lower().split())\n",
    "            similarity = len(query_words & response_words) / len(query_words | response_words)\n",
    "            scores.append((response, similarity))\n",
    "            \n",
    "        return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    def _process_response(self, response: str) -> Response:\n",
    "        \"\"\"Process and validate individual responses\"\"\"\n",
    "        start_time = time.time()\n",
    "        is_error = response.startswith(\"Error:\") or not response.strip()\n",
    "        \n",
    "        return Response(\n",
    "            text=response,\n",
    "            timestamp=start_time,\n",
    "            error=is_error,\n",
    "            processing_time=time.time() - start_time\n",
    "        )\n",
    "\n",
    "    def run_agent_interaction(self, prompt: str) -> Optional[Response]:\n",
    "        \"\"\"Enhanced agent interaction with error handling and metrics\"\"\"\n",
    "        try:\n",
    "            similar = self.find_similar_insights(prompt)\n",
    "            insights_text = \"\\n\".join([f\"Previous insight: {r[0].text[:100]}...\" for r in similar])\n",
    "            \n",
    "            augmented_prompt = (\n",
    "                f\"Context from similar interactions:\\n{insights_text}\\n\\n\"\n",
    "                f\"Query: {prompt}\\n\"\n",
    "                \"Provide a detailed, accurate response:\"\n",
    "            )\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response_text = self.ollama_client.generate(augmented_prompt)\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            response = Response(\n",
    "                text=response_text,\n",
    "                timestamp=time.time(),\n",
    "                error=response_text.startswith(\"Error:\"),\n",
    "                processing_time=processing_time\n",
    "            )\n",
    "            \n",
    "            self.executor.submit(self._save_response, response)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in agent interaction: {e}\")\n",
    "            return Response(text=f\"Error: {str(e)}\", timestamp=time.time(), error=True)\n",
    "\n",
    "    def _save_response(self, response: Response):\n",
    "        \"\"\"Asynchronous response saving\"\"\"\n",
    "        try:\n",
    "            self.responses.append(response)\n",
    "            with open(self.pickle_file, \"wb\") as f:\n",
    "                pickle.dump(self.responses, f)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving response: {e}\")\n",
    "\n",
    "    def generate_validation_report(self) -> dict:\n",
    "        \"\"\"Enhanced validation with detailed metrics\"\"\"\n",
    "        if not self.responses:\n",
    "            return {\"status\": \"No responses to analyze\"}\n",
    "            \n",
    "        valid_responses = [r for r in self.responses if not r.error]\n",
    "        total = len(self.responses)\n",
    "        valid_count = len(valid_responses)\n",
    "        \n",
    "        if not valid_responses:\n",
    "            return {\n",
    "                \"total_responses\": total,\n",
    "                \"valid_responses\": 0,\n",
    "                \"error_rate\": 1.0,\n",
    "                \"status\": \"No valid responses to analyze\"\n",
    "            }\n",
    "        \n",
    "        lengths = [len(r.text) for r in valid_responses]\n",
    "        word_counts = [len(r.text.split()) for r in valid_responses]\n",
    "        processing_times = [r.processing_time for r in valid_responses]\n",
    "        \n",
    "        return {\n",
    "            \"total_responses\": total,\n",
    "            \"valid_responses\": valid_count,\n",
    "            \"error_rate\": (total - valid_count) / total if total else 0,\n",
    "            \"length_analysis\": {\n",
    "                \"average\": np.mean(lengths),\n",
    "                \"min\": min(lengths),\n",
    "                \"max\": max(lengths),\n",
    "                \"distribution\": np.histogram(lengths, bins=3)[0].tolist()\n",
    "            },\n",
    "            \"vocabulary_analysis\": {\n",
    "                \"unique_words\": len(set(\" \".join(r.text for r in valid_responses).split())),\n",
    "                \"avg_words_per_response\": np.mean(word_counts)\n",
    "            },\n",
    "            \"performance\": {\n",
    "                \"avg_processing_time\": np.mean(processing_times),\n",
    "                \"max_processing_time\": max(processing_times)\n",
    "            }\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the optimized system\"\"\"\n",
    "    system = OptimizedAgentSystem(\n",
    "        model_path=\"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\"\n",
    "    )\n",
    "    \n",
    "    print(\"Initialized Agent System with Gemmasutra-Mini-2B model\")\n",
    "    print(\"Available commands:\")\n",
    "    print(\"- Type your prompt to get a response\")\n",
    "    print(\"- Type 'stats' to see system statistics\")\n",
    "    print(\"- Type 'exit' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"\\nEnter prompt: \").strip()\n",
    "            \n",
    "            if prompt.lower() == 'exit':\n",
    "                break\n",
    "            elif prompt.lower() == 'stats':\n",
    "                report = system.generate_validation_report()\n",
    "                print(\"\\n=== System Report ===\")\n",
    "                for key, value in report.items():\n",
    "                    print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
    "                    if isinstance(value, dict):\n",
    "                        for k, v in value.items():\n",
    "                            print(f\"  - {k.replace('_', ' ').title()}: {v}\")\n",
    "                    else:\n",
    "                        print(f\"  {value}\")\n",
    "            else:\n",
    "                print(\"\\nGenerating response...\")\n",
    "                response = system.run_agent_interaction(prompt)\n",
    "                if response:\n",
    "                    print(f\"\\nResponse: {response.text}\")\n",
    "                    if response.error:\n",
    "                        print(\"⚠️ Error detected in response\")\n",
    "                    print(f\"\\nProcessing time: {response.processing_time:.2f}s\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting gracefully...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
