{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (3806006448.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 42\u001b[0;36m\u001b[0m\n\u001b[0;31m    def _pull_model(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    \"\"\"Structured response data with metadata\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Handles communication with Ollama API\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self._check_model()\n",
    "        \n",
    "    def _check_model(self):\n",
    "        \"\"\"Verify model exists in Ollama\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/api/tags\")\n",
    "            available_models = [model['name'] for model in response.json()['models']]\n",
    "            model_base = self.model_name.split('/')[-1].split(':')[0]\n",
    "            \n",
    "            if model_base not in available_models:\n",
    "                logger.warning(f\"Model {model_base} not found. Attempting to pull...\")\n",
    "                self._pull_model()\n",
    "                \n",
    "    def _pull_model(self):\n",
    "        \"\"\"Pull model from Hugging Face\"\"\"\n",
    "        try:\n",
    "            requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                timeout=600\n",
    "            )\n",
    "            logger.info(f\"Successfully pulled model {self.model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to pull model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate response from Ollama model\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.9,\n",
    "                        \"top_k\": 40\n",
    "                    }\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"response\"]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ollama API error: {e}\")\n",
    "            return f\"Error: Failed to generate response - {str(e)}\"\n",
    "    \n",
    "class OptimizedAgentSystem:\n",
    "    def __init__(self, \n",
    "                 model_path: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\",\n",
    "                 pickle_file: str = \"responses.pkl\",\n",
    "                 cache_size: int = 1000):\n",
    "        \"\"\"Initialize the optimized agent system with caching and parallel processing\"\"\"\n",
    "        self.pickle_file = pickle_file\n",
    "        self.responses: List[Response] = self._load_responses()\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        self.ollama_client = OllamaClient(model_path)\n",
    "        \n",
    "    def _load_responses(self) -> List[Response]:\n",
    "        \"\"\"Load responses with error handling and data validation\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.pickle_file):\n",
    "                with open(self.pickle_file, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                    if isinstance(data[0], str):\n",
    "                        return [Response(text=r, timestamp=time.time()) for r in data]\n",
    "                    return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading responses: {e}\")\n",
    "        return []\n",
    "\n",
    "    @lru_cache(maxsize=100)\n",
    "    def find_similar_insights(self, query: str, top_k: int = 3) -> List[Tuple[Response, float]]:\n",
    "        \"\"\"Optimized similarity search using caching and vector operations\"\"\"\n",
    "        if not self.responses:\n",
    "            return []\n",
    "            \n",
    "        query_words = set(query.lower().split())\n",
    "        scores = []\n",
    "        \n",
    "        for response in self.responses:\n",
    "            if response.error:\n",
    "                continue\n",
    "            response_words = set(response.text.lower().split())\n",
    "            similarity = len(query_words & response_words) / len(query_words | response_words)\n",
    "            scores.append((response, similarity))\n",
    "            \n",
    "        return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    def _process_response(self, response: str) -> Response:\n",
    "        \"\"\"Process and validate individual responses\"\"\"\n",
    "        start_time = time.time()\n",
    "        is_error = response.startswith(\"Error:\") or not response.strip()\n",
    "        \n",
    "        return Response(\n",
    "            text=response,\n",
    "            timestamp=start_time,\n",
    "            error=is_error,\n",
    "            processing_time=time.time() - start_time\n",
    "        )\n",
    "\n",
    "    def run_agent_interaction(self, prompt: str) -> Optional[Response]:\n",
    "        \"\"\"Enhanced agent interaction with error handling and metrics\"\"\"\n",
    "        try:\n",
    "            similar = self.find_similar_insights(prompt)\n",
    "            insights_text = \"\\n\".join([f\"Previous insight: {r[0].text[:100]}...\" for r in similar])\n",
    "            \n",
    "            augmented_prompt = (\n",
    "                f\"Context from similar interactions:\\n{insights_text}\\n\\n\"\n",
    "                f\"Query: {prompt}\\n\"\n",
    "                \"Provide a detailed, accurate response:\"\n",
    "            )\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response_text = self.ollama_client.generate(augmented_prompt)\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            response = Response(\n",
    "                text=response_text,\n",
    "                timestamp=time.time(),\n",
    "                error=response_text.startswith(\"Error:\"),\n",
    "                processing_time=processing_time\n",
    "            )\n",
    "            \n",
    "            self.executor.submit(self._save_response, response)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in agent interaction: {e}\")\n",
    "            return Response(text=f\"Error: {str(e)}\", timestamp=time.time(), error=True)\n",
    "\n",
    "    def _save_response(self, response: Response):\n",
    "        \"\"\"Asynchronous response saving\"\"\"\n",
    "        try:\n",
    "            self.responses.append(response)\n",
    "            with open(self.pickle_file, \"wb\") as f:\n",
    "                pickle.dump(self.responses, f)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving response: {e}\")\n",
    "\n",
    "    def generate_validation_report(self) -> dict:\n",
    "        \"\"\"Enhanced validation with detailed metrics\"\"\"\n",
    "        if not self.responses:\n",
    "            return {\"status\": \"No responses to analyze\"}\n",
    "            \n",
    "        valid_responses = [r for r in self.responses if not r.error]\n",
    "        total = len(self.responses)\n",
    "        valid_count = len(valid_responses)\n",
    "        \n",
    "        if not valid_responses:\n",
    "            return {\n",
    "                \"total_responses\": total,\n",
    "                \"valid_responses\": 0,\n",
    "                \"error_rate\": 1.0,\n",
    "                \"status\": \"No valid responses to analyze\"\n",
    "            }\n",
    "        \n",
    "        lengths = [len(r.text) for r in valid_responses]\n",
    "        word_counts = [len(r.text.split()) for r in valid_responses]\n",
    "        processing_times = [r.processing_time for r in valid_responses]\n",
    "        \n",
    "        return {\n",
    "            \"total_responses\": total,\n",
    "            \"valid_responses\": valid_count,\n",
    "            \"error_rate\": (total - valid_count) / total if total else 0,\n",
    "            \"length_analysis\": {\n",
    "                \"average\": np.mean(lengths),\n",
    "                \"min\": min(lengths),\n",
    "                \"max\": max(lengths),\n",
    "                \"distribution\": np.histogram(lengths, bins=3)[0].tolist()\n",
    "            },\n",
    "            \"vocabulary_analysis\": {\n",
    "                \"unique_words\": len(set(\" \".join(r.text for r in valid_responses).split())),\n",
    "                \"avg_words_per_response\": np.mean(word_counts)\n",
    "            },\n",
    "            \"performance\": {\n",
    "                \"avg_processing_time\": np.mean(processing_times),\n",
    "                \"max_processing_time\": max(processing_times)\n",
    "            }\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the optimized system\"\"\"\n",
    "    system = OptimizedAgentSystem(\n",
    "        model_path=\"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\"\n",
    "    )\n",
    "    \n",
    "    print(\"Initialized Agent System with Gemmasutra-Mini-2B model\")\n",
    "    print(\"Available commands:\")\n",
    "    print(\"- Type your prompt to get a response\")\n",
    "    print(\"- Type 'stats' to see system statistics\")\n",
    "    print(\"- Type 'exit' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"\\nEnter prompt: \").strip()\n",
    "            \n",
    "            if prompt.lower() == 'exit':\n",
    "                break\n",
    "            elif prompt.lower() == 'stats':\n",
    "                report = system.generate_validation_report()\n",
    "                print(\"\\n=== System Report ===\")\n",
    "                for key, value in report.items():\n",
    "                    print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
    "                    if isinstance(value, dict):\n",
    "                        for k, v in value.items():\n",
    "                            print(f\"  - {k.replace('_', ' ').title()}: {v}\")\n",
    "                    else:\n",
    "                        print(f\"  {value}\")\n",
    "            else:\n",
    "                print(\"\\nGenerating response...\")\n",
    "                response = system.run_agent_interaction(prompt)\n",
    "                if response:\n",
    "                    print(f\"\\nResponse: {response.text}\")\n",
    "                    if response.error:\n",
    "                        print(\"⚠️ Error detected in response\")\n",
    "                    print(f\"\\nProcessing time: {response.processing_time:.2f}s\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting gracefully...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 13:52:03,112 - WARNING - Model Gemmasutra-Mini-2B-v1-GGUF not found. Attempting to pull...\n",
      "2025-02-19 13:52:03,420 - INFO - Successfully pulled model hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Agent System with Gemmasutra-Mini-2B model\n",
      "Available commands:\n",
      "- Type your prompt to get a response\n",
      "- Type 'stats' to see system statistics\n",
      "- Type 'exit' to quit\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Response: To provide an accurate and detailed response to the given query, I would need you to first provide more context or information. Without any specifics, it's not possible for me to give a detailed, accurate answer to this broad question about \"how to be good at everything.\"\n",
      "\n",
      "If you have specific areas of life that you want advice on improving - like your health, finances, relationships, career etc. - and what you're doing or not doing to improve in those areas right now, then I can provide a more detailed, personalized response tailored specifically to your situation. \n",
      "\n",
      "But if you don't have any details about what you're looking for advice on improving, or how you're currently approaching that goal, then I would need you to first tell me what specific area(s) of life you want guidance on and the current status/approach in those areas. Only then could I provide an accurate response with concrete tips and strategies to improve things.\n",
      "\n",
      "So please let me know what details you have so far about what areas of your life you'd like personalized advice on improving, and how you currently approach or don't approach that goal. From there, I can dive into providing a detailed, specific answer tailored just for you. But first, those initial key pieces of info are needed to even begin giving an accurate response. Let me know what other details you have about your current situation so far!\n",
      "\n",
      "Processing time: 23.57s\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Response: Creative Cover Letter Openings for a Frontend Developer Role\n",
      "\n",
      "1. \"Picture yourself as an explorer charting uncharted territory - we're searching for someone who can boldly go where no one has gone before and redefine the future of user experience.\" \n",
      "\n",
      "2. \"Think you have what it takes to be part of our team that is redefining how people interact with technology? We seek a visionary who sees the world through an interface lens, and crafts digital experiences that change lives.\"\n",
      "\n",
      "3. \"If your code speaks louder than words, we want to hear it! Our search for a frontend wizard begins... Let me in!\" \n",
      "\n",
      "4. \"Are you ready to build something incredible? We're looking for someone who can take our brand new flagship product and make users say 'wow!'\"\n",
      "\n",
      "5. \"Imagine the possibilities when you join a team that thinks differently, works smarter, and gets things done. Are you up for the challenge?\" \n",
      "\n",
      "6. \"We want an unconventional thinker to help us rethink how people use technology - someone who can see opportunities others miss.\"\n",
      "\n",
      "7. \"Ready to leave your mark? We're looking for pioneers willing to build something truly unique.\"\n",
      "\n",
      "8. \"Picture yourself as a storyteller with power tools, crafting experiences that move the audience and change the game.\" \n",
      "\n",
      "9. \"If you believe in dreaming big and leaving a legacy that endures, we want to hear from you!\"\n",
      "\n",
      "10. \"We're on the hunt for an idea man who can imagine things others don't see - someone with vision and guts willing to blaze new trails.\"\n",
      "\n",
      "Processing time: 15.07s\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Response: To write a compelling front-end developer cover letter, highlight your relevant skills and experience related to front-end development. Focus on past projects that showcase your abilities in HTML, CSS, JavaScript and other front-end technologies used by the company.\n",
      "\n",
      "Emphasize your strong attention to detail when it comes to web page design and optimization for both desktop and mobile viewing. Highlight how you can translate client requirements into clean, efficient code. \n",
      "\n",
      "Mention any relevant certifications or courses you've taken in HTML/CSS/JavaScript that demonstrate your mastery of front-end technology. Note how you stay up-to-date with the latest trends and best practices in web development.\n",
      "\n",
      "Conclude by clearly stating your interest in the role and why you're uniquely qualified to fill it. Conclude with a call-to-action for the hiring manager to contact you directly if they would like to discuss your qualifications further. \n",
      "\n",
      "Some sample opening lines:\n",
      "\"As an experienced front-end developer, I am confident that my skills and background make me ideally suited for this position...\"\n",
      "\"With over five years of experience as a full-stack JavaScript developer, I have honed my abilities in HTML5, CSS3, responsive design, mobile first development and more...\" \n",
      "\"My passion for clean, efficient code paired with strong attention to detail has allowed me to successfully deliver dozens of front-end web projects over the past few years.\"\n",
      "\n",
      "I hope this detailed answer provides you with a helpful framework on how to craft an effective cover letter opening as requested. Let me know if you need anything else!\n",
      "\n",
      "Processing time: 13.16s\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Response: To write a compelling front-end developer cover letter, highlight your relevant skills and experience that make you uniquely qualified for the role. Open with a strong personal anecdote or data point demonstrating how you think differently about frontend development than other candidates applying for this position. This will grab their attention right away and set you apart.\n",
      "\n",
      "In the first paragraph, give an overview of why you're reaching out - i.e. \"I recently saw your job posting for a front-end developer...\" or \"After reviewing your website, I'm convinced that my skills and experience are perfectly suited to this position.\" \n",
      "\n",
      "Next, showcase some of your most impressive frontend accomplishments from prior roles/projects. Emphasize any coding languages you worked with (e.g. HTML, CSS, JavaScript), frameworks used (e.g. React, Angular, Vue), tools mastered (e.g. Photoshop, Sketch). Explain how each project was a challenge and what creative solutions you devised.\n",
      "\n",
      "For example: \"My latest side project involved building an e-commerce site with over 100 product variants that loads faster than most sites on mobile devices using Next.js's server-side rendering and optimized image loading.\" This shows both your tech chops as well as your ability to deliver results on challenging projects.\n",
      "\n",
      "Another strong opening line would be: \"Your recent redesign caught my eye... I noticed the site was running 10+ seconds on mobile, which is unacceptable in today's fast-paced digital world. After evaluating your codebase, I have some ideas for how we can dramatically improve performance...\" \n",
      "\n",
      "Here you demonstrate technical proficiency as well as your willingness to dive into a project and provide solutions. This shows that not only are you an expert programmer, but also a problem solver and creative thinker.\n",
      "\n",
      "The rest of the cover letter should be tailored to each job posting, with relevant experience and skills highlighted, examples of difficult problems solved, and any other details that make you uniquely qualified for this role. The goal is to convince them why they need to hire YOU over all the other applicants. Let me know if you would like me to modify or expand on anything in my response!\n",
      "\n",
      "Processing time: 17.47s\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Response: I'm an adventurer at heart, always seeking new frontiers and challenges to push myself further. With my keen eye for detail and knack for problem-solving, I see the world through a lens of opportunity - spotting paths others miss and seizing chances to excel. \n",
      "\n",
      "My code is woven with passion, each line a brush stroke painting stories that move hearts and minds. I revel in crafting experiences that feel as natural as breathing, as fluid as quicksilver. When form meets function in perfect harmony, it's like unlocking the secrets of time itself - an eternal dance between the mundane and the magical.\n",
      "\n",
      "I see code not as lines on a page, but as notes in an orchestra, each one a part of something greater. I pour my heart into every line, crafting melodies that resonate deep within the human soul. With me, frontend development is more than just creating pretty webpages; it's weaving stories and shaping experiences that leave an indelible mark.\n",
      "\n",
      "When code becomes art, when problem-solving turns to adventure, I am but a humble guide to this wondrous world of possibilities. I see behind every door, taste every flavor on the menu, because I know there is always more - always something new around the corner. With me, frontend development is not just about lines of code; it's an invitation into uncharted territory, where imagination reigns supreme and the impossible becomes possible.\n",
      "\n",
      "Processing time: 11.64s\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Response: To provide an accurate and detailed response to the given query, I would need you to first provide me with the details of what exactly you are asking for in the opening lines. Are you looking for specific examples of powerful opening lines that have been used effectively in cover letters or resumes? Or do you simply want a generic list of tips on how to craft an effective opening line?\n",
      "\n",
      "If I am providing examples, I would need you to tell me what type of content is most relevant - resume samples, cover letter samples, or both. And which particular aspects of the writing are of interest. \n",
      "\n",
      "For general advice on crafting powerful openings, here are some key tips:\n",
      "- Keep it concise and attention-grabbing \n",
      "- Use a hook that piques their curiosity\n",
      "- Establish your credibility quickly\n",
      "- Show you're aligned with company values\n",
      "- Paint an enticing picture of what the role entails\n",
      "- Offer compelling reasons why they should hire you\n",
      "\n",
      "With those details, I can provide specific examples to illustrate how these opening lines are effective. But first, please let me know exactly which content area you need examples for - resume or cover letter, and if there are particular aspects you want covered. Then I'll be happy to share some effective opening line samples with you in detail.\n",
      "\n",
      "I hope this helps! Let me know what other information is needed from me.\n",
      "\n",
      "Processing time: 10.78s\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Response: 1) Oh boy, did I stumble upon a doozy of a query today! Let me see now... where to begin? Ah, let me think... how about this - \"Picture yourself as the Queen of England in 1588. You're on your way to colonize Australia when you run out of toilet paper.\" How does that open things up?\n",
      "\n",
      "2) So what do you know about me? Well, I'm an octopus with a PhD in quantum physics who loves cooking shows and binge-watching Netflix. But those are just surface level facts. Really, the important stuff is... \n",
      "\n",
      "3) You know how it is - life happens, you grow up, etc. Except maybe not exactly like that, because sometimes things change direction unexpectedly. Like when your best friend's wedding falls on the same day as a major asteroid impact which will cause earthquakes and tsunamis but also provide an opportunity to finally pay off those credit card debts...\n",
      "\n",
      "4) \"You are what you repeatedly do,\" or so they say. And if I had to choose one thing to be, that would definitely be me - a quirky, eclectic mix of interests with no two days ever quite the same as the last. So where was I? Oh right, trying to figure out how not to lose all my money on this whole \"startup\" gig...\n",
      "\n",
      "5) This might come as a surprise but for some reason, I'm an avid knitter when I'm not busy being an expert at quantum physics and binge-watching Netflix. But hey, we're allowed to dabble in things that seem totally unrelated to our field of study, right? Like knitting! Or asteroids...\n",
      "\n",
      "I hope these opening lines are funny enough without being too inappropriate or crass! Let me know if you need anything else from me - I'm happy to explore this query further.\n",
      "\n",
      "Processing time: 15.04s\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Response: To provide a detailed and accurate response to your query, I would need you to be more specific. Do you have a particular topic or question in mind? I'm happy to discuss any subject matter that interests you - but without knowing what specifically you want to talk about, it's difficult for me to generate an in-depth, thoughtful response. Let me know if there are certain topics you'd like me to elaborate on and I'll be glad to provide a detailed explanation, analysis or personal opinion on those subjects.\n",
      "\n",
      "Processing time: 4.57s\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Response: I'm sorry, but I do not feel comfortable providing explicit details about the user's intimate or sexual activities. It's important to maintain confidentiality and respect boundaries around personal matters like this. While I have general knowledge and can offer some vague guidance, discussing specific acts in detail goes beyond my ability to be helpful while keeping things private. If you are seeking support with a mental health issue related to the details you shared, I would encourage reaching out to a professional who can provide confidential counseling services tailored to your needs. Let me know if you have any other questions that don't require explicit details!\n",
      "\n",
      "Processing time: 13.11s\n",
      "\n",
      "Generating response...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    \"\"\"Structured response data with metadata\"\"\"\n",
    "    text: str\n",
    "    timestamp: float\n",
    "    error: bool = False\n",
    "    processing_time: float = 0.0\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Handles communication with Ollama API\"\"\"\n",
    "    def __init__(self, model_name: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\", base_url: str = \"http://localhost:11434\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self._check_model()\n",
    "        \n",
    "    def _check_model(self):\n",
    "        \"\"\"Verify model exists in Ollama\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/api/tags\")\n",
    "            available_models = [model['name'] for model in response.json()['models']]\n",
    "            model_base = self.model_name.split('/')[-1].split(':')[0]\n",
    "            \n",
    "            if model_base not in available_models:\n",
    "                logger.warning(f\"Model {model_base} not found. Attempting to pull...\")\n",
    "                self._pull_model()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking model: {e}\")\n",
    "            raise\n",
    "                \n",
    "    def _pull_model(self):\n",
    "        \"\"\"Pull model from Hugging Face\"\"\"\n",
    "        try:\n",
    "            requests.post(\n",
    "                f\"{self.base_url}/api/pull\",\n",
    "                json={\"name\": self.model_name},\n",
    "                timeout=600\n",
    "            )\n",
    "            logger.info(f\"Successfully pulled model {self.model_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to pull model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        \"\"\"Generate response from Ollama model\"\"\"\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.model_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"options\": {\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.9,\n",
    "                        \"top_k\": 40\n",
    "                    }\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()[\"response\"]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ollama API error: {e}\")\n",
    "            return f\"Error: Failed to generate response - {str(e)}\"\n",
    "    \n",
    "class OptimizedAgentSystem:\n",
    "    def __init__(self, \n",
    "                 model_path: str = \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\",\n",
    "                 pickle_file: str = \"responses.pkl\",\n",
    "                 cache_size: int = 1000):\n",
    "        \"\"\"Initialize the optimized agent system with caching and parallel processing\"\"\"\n",
    "        self.pickle_file = pickle_file\n",
    "        self.responses: List[Response] = self._load_responses()\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        self.ollama_client = OllamaClient(model_path)\n",
    "        \n",
    "    def _load_responses(self) -> List[Response]:\n",
    "        \"\"\"Load responses with error handling and data validation\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.pickle_file):\n",
    "                with open(self.pickle_file, \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                    if isinstance(data[0], str):\n",
    "                        return [Response(text=r, timestamp=time.time()) for r in data]\n",
    "                    return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading responses: {e}\")\n",
    "        return []\n",
    "\n",
    "    @lru_cache(maxsize=100)\n",
    "    def find_similar_insights(self, query: str, top_k: int = 3) -> List[Tuple[Response, float]]:\n",
    "        \"\"\"Optimized similarity search using caching and vector operations\"\"\"\n",
    "        if not self.responses:\n",
    "            return []\n",
    "            \n",
    "        query_words = set(query.lower().split())\n",
    "        scores = []\n",
    "        \n",
    "        for response in self.responses:\n",
    "            if response.error:\n",
    "                continue\n",
    "            response_words = set(response.text.lower().split())\n",
    "            similarity = len(query_words & response_words) / len(query_words | response_words)\n",
    "            scores.append((response, similarity))\n",
    "            \n",
    "        return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    def _process_response(self, response: str) -> Response:\n",
    "        \"\"\"Process and validate individual responses\"\"\"\n",
    "        start_time = time.time()\n",
    "        is_error = response.startswith(\"Error:\") or not response.strip()\n",
    "        \n",
    "        return Response(\n",
    "            text=response,\n",
    "            timestamp=start_time,\n",
    "            error=is_error,\n",
    "            processing_time=time.time() - start_time\n",
    "        )\n",
    "\n",
    "    def run_agent_interaction(self, prompt: str) -> Optional[Response]:\n",
    "        \"\"\"Enhanced agent interaction with error handling and metrics\"\"\"\n",
    "        try:\n",
    "            similar = self.find_similar_insights(prompt)\n",
    "            insights_text = \"\\n\".join([f\"Previous insight: {r[0].text[:100]}...\" for r in similar])\n",
    "            \n",
    "            augmented_prompt = (\n",
    "                f\"Context from similar interactions:\\n{insights_text}\\n\\n\"\n",
    "                f\"Query: {prompt}\\n\"\n",
    "                \"Provide a detailed, accurate response:\"\n",
    "            )\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response_text = self.ollama_client.generate(augmented_prompt)\n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            response = Response(\n",
    "                text=response_text,\n",
    "                timestamp=time.time(),\n",
    "                error=response_text.startswith(\"Error:\"),\n",
    "                processing_time=processing_time\n",
    "            )\n",
    "            \n",
    "            self.executor.submit(self._save_response, response)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in agent interaction: {e}\")\n",
    "            return Response(text=f\"Error: {str(e)}\", timestamp=time.time(), error=True)\n",
    "\n",
    "    def _save_response(self, response: Response):\n",
    "        \"\"\"Asynchronous response saving\"\"\"\n",
    "        try:\n",
    "            self.responses.append(response)\n",
    "            with open(self.pickle_file, \"wb\") as f:\n",
    "                pickle.dump(self.responses, f)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving response: {e}\")\n",
    "\n",
    "    def generate_validation_report(self) -> dict:\n",
    "        \"\"\"Enhanced validation with detailed metrics\"\"\"\n",
    "        if not self.responses:\n",
    "            return {\"status\": \"No responses to analyze\"}\n",
    "            \n",
    "        valid_responses = [r for r in self.responses if not r.error]\n",
    "        total = len(self.responses)\n",
    "        valid_count = len(valid_responses)\n",
    "        \n",
    "        if not valid_responses:\n",
    "            return {\n",
    "                \"total_responses\": total,\n",
    "                \"valid_responses\": 0,\n",
    "                \"error_rate\": 1.0,\n",
    "                \"status\": \"No valid responses to analyze\"\n",
    "            }\n",
    "        \n",
    "        lengths = [len(r.text) for r in valid_responses]\n",
    "        word_counts = [len(r.text.split()) for r in valid_responses]\n",
    "        processing_times = [r.processing_time for r in valid_responses]\n",
    "        \n",
    "        return {\n",
    "            \"total_responses\": total,\n",
    "            \"valid_responses\": valid_count,\n",
    "            \"error_rate\": (total - valid_count) / total if total else 0,\n",
    "            \"length_analysis\": {\n",
    "                \"average\": np.mean(lengths),\n",
    "                \"min\": min(lengths),\n",
    "                \"max\": max(lengths),\n",
    "                \"distribution\": np.histogram(lengths, bins=3)[0].tolist()\n",
    "            },\n",
    "            \"vocabulary_analysis\": {\n",
    "                \"unique_words\": len(set(\" \".join(r.text for r in valid_responses).split())),\n",
    "                \"avg_words_per_response\": np.mean(word_counts)\n",
    "            },\n",
    "            \"performance\": {\n",
    "                \"avg_processing_time\": np.mean(processing_times),\n",
    "                \"max_processing_time\": max(processing_times)\n",
    "            }\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the optimized system\"\"\"\n",
    "    system = OptimizedAgentSystem(\n",
    "        model_path=\"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\"\n",
    "    )\n",
    "    \n",
    "    print(\"Initialized Agent System with Gemmasutra-Mini-2B model\")\n",
    "    print(\"Available commands:\")\n",
    "    print(\"- Type your prompt to get a response\")\n",
    "    print(\"- Type 'stats' to see system statistics\")\n",
    "    print(\"- Type 'exit' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"\\nEnter prompt: \").strip()\n",
    "            \n",
    "            if prompt.lower() == 'exit':\n",
    "                break\n",
    "            elif prompt.lower() == 'stats':\n",
    "                report = system.generate_validation_report()\n",
    "                print(\"\\n=== System Report ===\")\n",
    "                for key, value in report.items():\n",
    "                    print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
    "                    if isinstance(value, dict):\n",
    "                        for k, v in value.items():\n",
    "                            print(f\"  - {k.replace('_', ' ').title()}: {v}\")\n",
    "                    else:\n",
    "                        print(f\"  {value}\")\n",
    "            else:\n",
    "                print(\"\\nGenerating response...\")\n",
    "                response = system.run_agent_interaction(prompt)\n",
    "                if response:\n",
    "                    print(f\"\\nResponse: {response.text}\")\n",
    "                    if response.error:\n",
    "                     print(\"⚠️ Error detected in response\")\n",
    "                print(f\"\\nProcessing time: {response.processing_time:.2f}s\")\n",
    "        except KeyboardInterrupt:\n",
    "                print(\"\\nExiting gracefully...\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "                print(f\"\\nError: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
