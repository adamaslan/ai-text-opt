{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import logging\n",
    "\n",
    "class RetrievalComponent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.documents = []\n",
    "        self.index = None\n",
    "        self.retriever = None\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def load_knowledge_base(self, file_path: str):\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                self.documents = [line.strip() for line in f if line.strip()]\n",
    "            self.logger.info(f\"Loaded {len(self.documents)} documents\")\n",
    "\n",
    "            # Create embeddings index\n",
    "            embed_model = SentenceTransformer(self.config['retriever_model'])\n",
    "            embeddings = embed_model.encode(self.documents)\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "            self.index.add(embeddings.astype('float32'))\n",
    "            self.retriever = embed_model\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Knowledge base error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
    "        try:\n",
    "            query_embed = self.retriever.encode([query])\n",
    "            distances, indices = self.index.search(query_embed.astype('float32'), k)\n",
    "            return [self.documents[i] for i in indices[0]]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Retrieval error: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import logging\n",
    "\n",
    "class RetrievalComponent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.documents = []\n",
    "        self.index = None\n",
    "        self.retriever = None\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def load_knowledge_base(self, file_path: str):\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                self.documents = [line.strip() for line in f if line.strip()]\n",
    "            self.logger.info(f\"Loaded {len(self.documents)} documents\")\n",
    "\n",
    "            # Create embeddings index\n",
    "            embed_model = SentenceTransformer(self.config['retriever_model'])\n",
    "            embeddings = embed_model.encode(self.documents)\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "            self.index.add(embeddings.astype('float32'))\n",
    "            self.retriever = embed_model\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Knowledge base error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
    "        try:\n",
    "            query_embed = self.retriever.encode([query])\n",
    "            distances, indices = self.index.search(query_embed.astype('float32'), k)\n",
    "            return [self.documents[i] for i in indices[0]]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Retrieval error: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "import logging\n",
    "from peft import LoraConfig, get_peft_model, TaskType  # Correct import for TaskType\n",
    "\n",
    "class GenerationComponent:\n",
    "    def __init__(self, config, device='cuda'):\n",
    "        self.config = config\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.device = device\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def setup_generator(self):\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config['model_name'], use_fast=True\n",
    "            )\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config['model_name'],\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map='auto'\n",
    "            )\n",
    "            peft_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                r=8,\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"q_proj\", \"v_proj\"]\n",
    "            )\n",
    "            self.model = get_peft_model(base_model, peft_config)\n",
    "            self.logger.info(\"Generator ready\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Generator setup error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def rag_generate(self, query: str, max_length: int = 500) -> str:\n",
    "        try:\n",
    "            # Retrieve relevant context\n",
    "            context = self.retrieve(query)\n",
    "            prompt = f\"Context: {' '.join(context)}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=self.config['max_length']\n",
    "            ).to(self.device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_length=max_length,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Generation error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
    "        # Placeholder for the retrieve method, should be implemented\n",
    "        # This method should return a list of relevant context strings based on the query\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (147054974.py, line 57)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 57\u001b[0;36m\u001b[0m\n\u001b[0;31m    response = run_local_ollama_model(self.config['model_name'], prompt)\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# nu part a\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import logging\n",
    "\n",
    "# Retrieval Component\n",
    "class RetrievalComponent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.documents = []\n",
    "        self.index = None\n",
    "        self.retriever = None\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def load_knowledge_base(self, file_path: str):\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                self.documents = [line.strip() for line in f if line.strip()]\n",
    "            self.logger.info(f\"Loaded {len(self.documents)} documents\")\n",
    "\n",
    "            # Create embeddings index\n",
    "            embed_model = SentenceTransformer(self.config['retriever_model'])\n",
    "            embeddings = embed_model.encode(self.documents)\n",
    "            self.index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "            self.index.add(embeddings.astype('float32'))\n",
    "            self.retriever = embed_model\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Knowledge base error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[str]:\n",
    "        try:\n",
    "            query_embed = self.retriever.encode([query])\n",
    "            distances, indices = self.index.search(query_embed.astype('float32'), k)\n",
    "            return [self.documents[i] for i in indices[0]]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Retrieval error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Generation Component with RAG Integration\n",
    "class GenerationComponent:\n",
    "    def __init__(self, config, retriever: RetrievalComponent, device='cuda'):\n",
    "        self.config = config\n",
    "        self.retriever = retriever\n",
    "        self.device = device\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def rag_generate(self, query: str, max_length: int = 500) -> str:\n",
    "        try:\n",
    "            # Retrieve relevant context\n",
    "            context = self.retriever.retrieve(query)\n",
    "            prompt = f\"Context: {' '.join(context)}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "            # Run local Ollama model\n",
    "            response = run_local_ollama_model(self.config['model_name'], prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Knowledge base error: [Errno 2] No such file or directory: 'path_to_knowledge_base.txt'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_knowledge_base.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 97\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Initialize components\u001b[39;00m\n\u001b[1;32m     96\u001b[0m retriever \u001b[38;5;241m=\u001b[39m RetrievalComponent(config)\n\u001b[0;32m---> 97\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_knowledge_base\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath_to_knowledge_base.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m generator \u001b[38;5;241m=\u001b[39m GenerationComponent(config, retriever)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Define the prompt\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m, in \u001b[0;36mRetrievalComponent.load_knowledge_base\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_knowledge_base\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nunu24/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_knowledge_base.txt'"
     ]
    }
   ],
   "source": [
    "# part 2  nu b\n",
    "import subprocess\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "HISTORY_FILE = \"ollama_history.pkl\"\n",
    "\n",
    "# History Management\n",
    "def save_to_pickle(response_data):\n",
    "    \"\"\"Save response data to pickle file with timestamp\"\"\"\n",
    "    try:\n",
    "        with open(HISTORY_FILE, \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "    except (FileNotFoundError, EOFError):\n",
    "        history = {\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'interactions': []\n",
    "        }\n",
    "\n",
    "    history['interactions'].append(response_data)\n",
    "\n",
    "    with open(HISTORY_FILE, \"wb\") as f:\n",
    "        pickle.dump(history, f)\n",
    "\n",
    "def run_local_ollama_model(model_name, prompt, timeout=60):\n",
    "    \"\"\"\n",
    "    Runs a local Ollama model and saves responses to timestamped pickle file.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    response_data = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model': model_name,\n",
    "        'prompt': prompt,\n",
    "        'response': '',\n",
    "        'execution_time': 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        process = subprocess.run(\n",
    "            [\"ollama\", \"run\", model_name],\n",
    "            input=prompt,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout\n",
    "        )\n",
    "\n",
    "        response = process.stdout.strip()\n",
    "        error = process.stderr.strip()\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        if error:\n",
    "            response_data['response'] = f\"Error: {error}\"\n",
    "        else:\n",
    "            response_data.update({\n",
    "                'response': response,\n",
    "                'execution_time': round(elapsed_time, 2)\n",
    "            })\n",
    "\n",
    "        save_to_pickle(response_data)\n",
    "\n",
    "        print(f\"Execution time: {elapsed_time:.2f} seconds\")\n",
    "        return response\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        response_data['response'] = \"Error: Response timed out.\"\n",
    "        save_to_pickle(response_data)\n",
    "        return response_data['response']\n",
    "\n",
    "def load_history():\n",
    "    \"\"\"Load and print interaction history\"\"\"\n",
    "    try:\n",
    "        with open(HISTORY_FILE, \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "            print(f\"History created at: {history['created_at']}\")\n",
    "            for idx, interaction in enumerate(history['interactions'], 1):\n",
    "                print(f\"\\nInteraction {idx} ({interaction['timestamp']}):\")\n",
    "                print(f\"Model: {interaction['model']}\")\n",
    "                print(f\"Prompt: {interaction['prompt']}\")\n",
    "                print(f\"Response: {interaction['response']}\")\n",
    "                print(f\"Execution time: {interaction['execution_time']}s\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No history found\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'model_name': \"hf.co/TheDrummer/Gemmasutra-Mini-2B-v1-GGUF:Q3_K_L\",\n",
    "        'retriever_model': \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        'max_length': 512\n",
    "    }\n",
    "\n",
    "    # Initialize components\n",
    "    retriever = RetrievalComponent(config)\n",
    "    retriever.load_knowledge_base(\"path_to_knowledge_base.txt\")\n",
    "\n",
    "    generator = GenerationComponent(config, retriever)\n",
    "\n",
    "    # Define the prompt\n",
    "    prompt_text = \"Explain the benefits of running models locally.\"\n",
    "\n",
    "    # Get response\n",
    "    response = generator.rag_generate(prompt_text)\n",
    "    print(\"AI Response:\", response)\n",
    "\n",
    "    # View history\n",
    "    print(\"\\n=== Interaction History ===\")\n",
    "    load_history()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nunu24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
